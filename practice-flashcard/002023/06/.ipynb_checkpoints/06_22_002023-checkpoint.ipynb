{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bedcdb2-8e7e-4dce-b576-cc191110cc9d",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd05c3b-1c6a-4d31-a3ae-204e3ddccc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1e826-7e7c-4d45-ad0c-585588c8be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    int numbers[] = {1, 2, 3, 4, 5};\n",
    "    \n",
    "    for (int i = 0; i < 3; i++) {\n",
    "        std::cout << numbers[i];\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e1977-07df-4106-b647-61bbc18dfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "send sync, receiv sync\n",
    "send sync, receiv async\n",
    "send async, receiv sync\n",
    "send async, recei async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d988a4-2448-4fdf-ba32-88d8c418123e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae83e9a-00aa-476a-ae01-0650dd075f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_grad_enabled(input):\n",
    "    return torch.is_grad_enabled and input.require_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f840cb-2bbc-448a-8306-781fe343befe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def broadcast(input):\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057ea6f-fa0f-4f93-b758-3d43ab28c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(input):\n",
    "    world_size = torch.distributed.get_world_size()\n",
    "    if world_size == 1:\n",
    "        return input\n",
    "    \n",
    "    torch.distributed.all_reduce(input)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f2f30-b1a3-4d98-885c-9737c8ddc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return broadcast(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, input):\n",
    "        return reduce(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488f904-b6b6-4f4e-a82d-b3bea7b29081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward(input):\n",
    "    if is_grad_enabled(input):\n",
    "        output = Broadcast.apply(input)\n",
    "    else:\n",
    "        output = broadcast(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0377f509-11c4-42f9-9e8b-0e2532724669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            # GPU waits GPU\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            # CPU waits GPU\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c45e675-730d-4e3b-aaea-5ef0e72cf52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        wait_stream(\n",
    "            source_stream=next_stream,\n",
    "            target_stream=prev_stream\n",
    "        )\n",
    "        \n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        wait_stream(\n",
    "            source_stream=prev_stream,\n",
    "            target_stream=next_stream\n",
    "        )\n",
    "        \n",
    "        grad_streams = [None, None]\n",
    "        \n",
    "        return grad_streams + grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f2d8a3-4dbc-4837-9927-69059f7926d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StartDependency(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        phony = torch.empty(0, requires_grad=False)\n",
    "        return input, phony\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_input, grad_phony):\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2c0ef-2099-408d-9750-e563343dfad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndDependency(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, phony):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21024b9e-9816-4c16-bddb-3e5bba6287e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dependency(start_batch, end_batch):\n",
    "    start_batch, phony = StartDependency.apply(start_batch)\n",
    "    end_batch = EndDependency.apply(end_batch, phony)\n",
    "    return start_batch, end_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe7b5c9-d153-419b-a35d-9462cd11608a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from contextlib import contextmanager\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0cda11-3d3d-4ae6-bf4c-b058d5d62e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_and_execute(device, in_queue, out_queue):\n",
    "    while True:\n",
    "        task = inqueue.get()\n",
    "        \n",
    "        try:\n",
    "            output = task()\n",
    "        except Exception:\n",
    "            out_queue.put((None, False))\n",
    "            continue\n",
    "        \n",
    "        out_queue.put((output, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d1d493-b4c8-4e8c-b703-d32a614e0008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def spawn_workers(devices):\n",
    "    in_queues = []\n",
    "    out_queues = []\n",
    "    workers = {}\n",
    "    \n",
    "    for device in devices:\n",
    "        in_queue = Queue()\n",
    "        out_queue = Queue()\n",
    "        workers[device].append((in_queue, out_queue))\n",
    "        \n",
    "        thread = Thread(\n",
    "            target=wait_and_execute,\n",
    "            args=(device, in_queue, out_queue)\n",
    "        )\n",
    "        thread.start()\n",
    "        \n",
    "        in_queues.append(in_queue)\n",
    "        out_queues.append(out_queue)\n",
    "        \n",
    "    yield (in_queues, out_queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e978a4-f0af-4403-9091-1db256c0b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = torch.distributed.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa9689-53a1-425c-b53e-16c9096b1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 69:\n",
    "    torch.distributed.broadcast(x, src=69)\n",
    "elif rank == 42:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49436d40-ff4d-45f4-aa6c-908ee2608eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd5f6eb-cccb-4f57-a767-dc1ddb073946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_paralell_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eef41c31-5a3d-42df-90c6-285a453effdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_model_parallel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6327b1f-9299-4abb-a8ba-3e79e1d68171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c16aea-3f8c-4231-ba25-4078bc38707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "&x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212339cc-8dc3-4fdb-9d9e-639ee6b31c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: scale the loss using scaling factor\n",
    "step 2: calculate the gradient of the scaled loss\n",
    "step 3: unscale the gradient using the scaling factor\n",
    "step 4: update the gradient of the model using the unscaled gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7798875-3c35-4762-8ecc-56d227999148",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global memory > thread block > register memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade22505-b86a-428c-9408-473b2a1d69f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "474c1687-9624-46d1-8d5a-8137ffa5befc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, output):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        wait_stream(prev_stream, next_stream)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        wait_stream(next_stream, prev_stream)\n",
    "        grad_stream = [None, None]\n",
    "        return grad_stream + grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c5cca-4009-4e02-8da3-f8d061ad0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = torch.distributed.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ee494-d463-4cfd-908f-d4c19696be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 69:\n",
    "    torch.distributed.isend(x, dest=42)\n",
    "elif rank == 42:\n",
    "    torch.distributed.irecv(x, src=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a582b2e-2113-4f6d-8e3b-a24befc15bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4df3658-bf8e-42d7-b9aa-8fffb5efe011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b115b2ee-9655-4c92-b9ea-710e9aa6bd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_model_parallel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a2f480a-b741-4352-ac69-57fdd41b03d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f86757e-3e4c-44e8-9433-0239797cdade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_parallel_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c78c935f-26ad-402d-b75c-071790733837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_rank = i*num_pipeline_model_parallel_groups\n",
    "    end_rank = (i+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_paralell_size):\n",
    "        ranks = list(range(\n",
    "            start_rank+j,\n",
    "            end_rank,\n",
    "            tensor_model_paralell_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47007dc4-3125-4acd-8e93-cac146e5aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "int x{y};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae36dcc-eefe-4c47-8241-56f4e5431088",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: cpu memory\n",
    "step 2: allocate gpu memory\n",
    "step 3: move cpu to gpu\n",
    "step 4: do the computation\n",
    "step 5: move to cpu\n",
    "step 6: gpu memory dellocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ee72c-5968-4d04-b520-300146e5cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "int p_number;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd75b8a-6fd3-465e-8138-29d8ff410c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "int* p_number{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e561ad-2a89-48ea-b193-a9c4fc4b1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "*p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110f88e-bf9b-48ee-81e6-31fef2dd0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "foward(x) > forward(x) > backward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c37eea-7044-41b4-9c6f-e6e507d6c936",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e650d-a55e-4a6e-b91f-d47cea1dd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_head, d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1da04-3e36-41e4-aafe-48f42e5f339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [model.ln_final.register_forward_pre_hook(hook) in hooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c3bf0-baee-4142-ba84-ee33b69531af",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles[1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa4cc3-d669-41ab-a959-253776bbecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(prompt, return_tensor=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba708649-1ff5-428f-9eb1-9f95ffed0a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_shape(module, input):\n",
    "    print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b4dc9-8bbb-403b-9eab-c449e76b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[1].register_forward_pre_hook(print_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b3996c2-35a4-48dc-9541-6264d3623951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e09b6-12a3-4278-a201-9ffe126b5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def use_hooks(model, hooks):\n",
    "    try:\n",
    "        module = model.transformer.h[1]\n",
    "        handles = [module.register_forward_pre_hook(hook) for hook in hooks]\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd060da-c890-4577-ac9c-e9f72685b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79e27a-043c-497c-bae4-f7128479018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a9d8a-4bd4-4940-8557-4fd11a389acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a512f3-d6e9-48c5-af10-f740c4417aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokens = tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c8948-ef97-406a-9d72-eccda5207342",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_log_probs = -log_probs[target_tokens].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330a651-9838-4867-9741-a8101cd56ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributed.broadcast(x, async_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c560f37a-3de2-4e5b-a735-31a41cde1029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403403c-eadc-436a-9aa9-8b6a7c88ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1(ti):\n",
    "    ti.xcomm.push(\"x\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac2ed8e2-61ad-494d-be3c-7f57a3a5978a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def task_2(ti):\n",
    "    x = ti.xcomm.get(\"x\", task=[\"task_1\"])\n",
    "    y = x + 1\n",
    "    ti.xcomm.push(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84444e9c-a963-408f-9c98-fbd0defa3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dag(dag_id=dag_id, start_date=start_date)\n",
    "def workflow():\n",
    "    task_1 = PythonOperator(python_callable=task_1, task_id=\"task_1\")\n",
    "    task_2 = PythonOperator(python_callable=task_2, task_id=\"task_2\")\n",
    "    \n",
    "    task_1 >> task_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fa69f2d-6b6d-4420-a70b-e4d4189fdecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa6909-f978-4b55-a367-4c6248319a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
