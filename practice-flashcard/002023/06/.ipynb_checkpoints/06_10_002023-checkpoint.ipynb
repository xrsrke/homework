{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8964437d-8512-4d1a-b8b0-b409dbf06019",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f5940-af49-4937-872d-8492491941fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: calculate the act of part 1\n",
    "step 2: calculate the act of part 2 using part 1\n",
    "step 3: delete the act of part 1\n",
    "step 4: calculate the gradient of part 2\n",
    "step 5: recompute the activation of part 1\n",
    "step 6: calculate the gradient of part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9a8d5-06ca-40d7-85ae-00fc2010ad9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bee2f24-df60-4e4d-9fec-417dd895cbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c192081-75c0-4701-bfcb-141d103a45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu-gpu\n",
    "gpu-cpu\n",
    "cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a25816-ac45-4b53-8ae7-b4ba29f10990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            source_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e35753-4153-4293-8712-8aa97de00f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, source_stream, next_stream, input):\n",
    "        ctx.source_stream = source_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        wait_stream(next_stream, source_stream)\n",
    "        \n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        source_stream = ctx.source_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        grad_stream = (None, None)\n",
    "        \n",
    "        wait_stream(source_stream, next_stream)\n",
    "        \n",
    "        return grad_stream + grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072015c-9e0d-4777-87bd-71d2b9572517",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubelet, kube-proxy, container runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c64130-474f-4619-b3ac-b438e5f8c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "api server, control manager, ectd, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383143a8-886f-4b1c-825c-40a02a4d39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e075027-dbbd-450b-8d6c-c0d09ddb0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    int file_size = 100;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dab00f-5eca-44c7-af71-e99b64d5c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "int zero() {\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a938f8a-3719-4cba-9380-c438f688ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf0a7e-3560-4333-8f9e-79b670711884",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    std::cout << \"hello world\";\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00980372-b7ed-421d-926f-2914297295fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    const float pi = 3.14;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5121f3-28d8-49be-9a21-99034b5d6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "register >sram > dram >hard drive > external harddrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f4e520-e973-40a5-9d48-46e5021cc592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9688a104-252e-415e-aad8-9dc9b723ac2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2\n",
    "pipeline_model_parallel_Size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796f0931-7e35-4a17-ab87-9d45a70978fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a113a4-0987-46d0-ae34-f2f684bbabd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(pipeline_model_parallel_Size):\n",
    "    start_rank = i*num_pipeline_model_parallel_groups\n",
    "    end_rank = (i+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(\n",
    "            start_rank+j,\n",
    "            end_rank,\n",
    "            tensor_model_parallel_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7518c706-30a1-4155-8cda-39a0425004f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577f0d4-bec1-405e-ab45-077431e33b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(\n",
    "    config_path=\"./\",\n",
    "    config_name=\"config.yaml\"\n",
    ")\n",
    "def hello(conf):\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab1b288-b686-48bc-9c01-ba892559e20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastapi import HTTPException, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912c5a6-2edf-4a94-82fe-3cc705fa1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/tasks\")\n",
    "def get_tasks(id):\n",
    "    if id == 0:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a01f5-b3eb-443d-a84e-cd004d239a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: dvc add\n",
    "step 2: git add \n",
    "step 3: git ignore\n",
    "step 4: dvc push\n",
    "step 5: git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd5e10-9672-4d4d-9a8a-ad21d5174c5f",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942fa367-a64b-4908-b8ea-b6ec12757e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32b25d-c940-4f5b-afa0-8c06902da4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_communication(name, rank, world_size):\n",
    "    torch.distributed.rpc.init_rpc(name, rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d098a-383e-437a-b9b0-9a1eb2ac86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.input = input\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return ctx.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e62e3-788c-4a41-a39d-a062b81119c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.roll(x, shifts=1, dims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08427fa-1080-4c23-8d02-f879a4f6b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: choose an component\n",
    "step 2: two prompts\n",
    "step 3: record the activations of two prompts\n",
    "step 4: patch\n",
    "step 5: compute the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f59f9b-51f7-4b6d-8d75-66fd77236ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: choose a component\n",
    "step 2: a clean prompt and a corrupted prompt\n",
    "step 3: record the activations of the clean prompt and the corrupted prompt\n",
    "step 4: extract the activation of the target component in the corrupted prompt\n",
    "step 5: patch the extract to clean prompt, and extract the act of recevier\n",
    "step 6: patch the receiver to the clean prompt\n",
    "step 7: calculate the logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf568c4-d9fa-4fd8-8618-61b78ec6f026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "components = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0be727-e5e2-4634-a75b-f4474182f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = cache[\"embed\"]\n",
    "positional_embedding = cache[\"pos_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec599b7-f642-47a6-9d7c-2bb533c8cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.append(text_embedding)\n",
    "components.append(positional_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b6bab7-df31-4ed4-8708-1b71394a8fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d66a0-e25b-4720-8a6d-ca2e0773e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(3):\n",
    "    attn_out_name = utils.get_act_name(\"attn_out\", layer_idx)\n",
    "    mlp_out_name = utils.get_act_name(\"mlp_out\", layer_idx)\n",
    "    components.append(cache[attn_out_name])\n",
    "    components.append(cache[mlp_out_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56ca95-e30b-46b8-bddf-284ef42cbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model.blocks:\n",
    "    residual = block(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ea95a-8b38-4c4f-bcc7-8c1800c82272",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = model.ln_final(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af901fec-7bb0-4d2b-b661-b412d1ecde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = model.unembed(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f56c3b-4557-4b88-9437-2def5041b121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fa197-93ba-43a3-804d-31c207b2d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            # GPU wait CUDA\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            # GPU wait CPU\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1dcfff-0835-4c2d-8bff-fbd04bee67db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        wait_stream(source_stream=next_stream, next_stream=prev_stream)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        wait_stream(source_stream=prev_stream, next_stream=next_stream)\n",
    "        \n",
    "        grad_stream = (None, None)\n",
    "        \n",
    "        return grad_stream + grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf885bd-b7cd-485c-9cb1-5e79ecd7debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_prompt = \"A told B: 'Persistence is all you need.' C replied back to \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd02bc-6e9b-4978-beda-81e43d6ac42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(prompt)\n",
    "corrupted_tokens = model.to_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80da1d-0c0a-423c-9a4c-8836d783f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_activations = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_activatiosn = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42b30317-33b1-435b-a715-2c311968ea88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b0595f-7561-425f-aeaa-d92ac76f657f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_idx, layer_idx = 6, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c16e83-5a52-4bdc-b2d0-a41160a981e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = utils.get_act_name(\"attn\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0135c-288c-4c37-b02e-d12b1fcede80",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_head_activations = corrupted_head_activations[hook_name][0, head_idx, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b65036-c57e-444b-8f4c-b6e0b9bcacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_activations(activations, hook):\n",
    "    activations[0, head_idx, :, :] = corrupted_head_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fede78-9b02-451a-abf6-49291377da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, patched_activations = model.run_with_hooks(\n",
    "    clean_activations,\n",
    "    fwd_hook=[(hook_name, patch_head_activations)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32ec0a-b787-4032-95b5-23751f84ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_receiver_activations = patched_activations[receiver_hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a59f56-b100-471e-b268-34806d012c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_receiver_activations(activations, hook):\n",
    "    activations = corrupted_receiver_activations\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edf63a-e957-48f5-a3bc-c9c6155a5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_hook(receiver_hook_name, patch_receiver_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80577c84-14b7-4317-9f8c-880ca7322176",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_logits = model(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c5af0-857d-41e1-8f4c-018c5494269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hook()\n",
    "clean_logits = model(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0289881-8102-4d3b-b6d3-05b3195d0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_difference(clean_logit, corrupted_logit, target_token):\n",
    "    clean_logit_token = clean_logit[0, -1, target_token]\n",
    "    corrupted_logit_token = corrupted_logit[0, -1, target_token]\n",
    "    return corrupted_logit_token - clean_logit_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cb907-24da-4488-90e0-238a1b10bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: clean prompt and corrupted prompt\n",
    "step 2: record the activations of the two prompts\n",
    "step 3: patch the activations of each component from the corrupted prompt to the clean prompt\n",
    "step 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649afae2-2d35-4d38-a692-c100baf63015",
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_1, enter_2, hello, exit_2, exit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c13c89c6-76b7-456c-83cd-6216b249d605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model)\n",
    "        self.to_k = nn.Linear(d_model, d_model)\n",
    "        self.to_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, self.n_heads, seq_len, self.d_head)\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        batch_size, n_heads, seq_len, d_head = x.size()\n",
    "        return x.view(batch_size, seq_len, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        q = self.to_q(q)\n",
    "        k = self.to_k(k)\n",
    "        v = self.to_v(v)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        attn_out, attn_weights = self.attention(\n",
    "            q, k, v\n",
    "        )\n",
    "        \n",
    "        output = self.concat_heads(attn_out)\n",
    "        output = self.proj(output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d0482d9-8d0b-41e2-8b10-3a70e2766658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5ac83-8b11-4f43-a03b-84fb8902d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0]),\n",
    "            high=np.array([100])\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.shower_length = 60\n",
    "        self.temperature = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7edd3b-c570-438e-bf3e-2a13a56c88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(logits, targets):\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    return -log_probs[torch.arange(targets.shape[-1]), targets].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37ac260b-1b25-40d2-9a9a-2e039130e185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kl = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c53d44-917d-4e9b-952a-a002a0d6d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl(\n",
    "    input=F.log_softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21ff4b-b77f-4463-964d-28f5e04d971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_the_loss(states, actions, action_probs):\n",
    "    entropy_of_policy = action_probs.log()\n",
    "    \n",
    "    q_expectation = torch.mean(q_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc24a77a-cb0a-42d3-ab26-bb84fa00d2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e05257-34a4-47d4-a47a-96c613b95adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(\n",
    "    input=F.log_softmax(student_logits),\n",
    "    target=F.softmax()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
