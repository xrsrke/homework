{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5fe6e2-8b16-4b52-967f-9fdc080fa568",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9b15d-6142-4f99-a0f5-524a1b3b829d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59700824-fabd-4f28-87f6-ddaf5cb96ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd98abf-f43b-4e0f-ad41-a1b5bc2c6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool watcher, worker thread, job selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d1ecc-abdc-4d0e-b672-fb43f70e3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"worker1\": {\"cuda:0\": \"cuda:1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9c28d9-a3f7-4f34-8a91-f2106f5c9b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effbe17-fcde-4126-b447-73f1c7a061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_ln_name = get_act_name(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ce70c-4a88-4519-b624-825a5c817902",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "W_U = model.W_U\n",
    "\n",
    "full_OV_circuit = W_E @ OV_circuit @ W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90975d5b-6d7e-46f1-8aeb-29820b7b1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79837e64-0c8b-449e-ad4a-45f24f844eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"pattern\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637c746-867a-4bfc-8e4e-548acae49287",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[hook_name][:, head_idx, target_query_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a66e16-4107-4b95-9664-832d61698232",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"worker1\": {\"cuda:0\": \"cuda:1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c7777-38bd-42b5-ac65-9e5e32a5b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: partition weight\n",
    "step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0a2b87-3bba-4b89-9f3d-deff4420e129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e5709-3704-43e2-834d-157e46b8d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(x, \"h w -> h w n\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775cbc-8559-4c39-a546-5d27ec021464",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_ln_name = get_act_name(\"resid_post\", 2)\n",
    "pre_head20_ln_name = get_act_name(\"resid_pre\", 2)\n",
    "post_head_20_ln = get_act_name(\"normalized\", 2, \"ln1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f5106-7366-4963-aa69-039ff0bfe78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e50280-6079-426b-969a-55d0faade31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_ln = cache[pre_final_ln_name]\n",
    "post_final_ln = cache[post_final_ln_name]\n",
    "\n",
    "pre_head20_ln_name = cache[pre_head20_ln_name]\n",
    "post_head20_ln_name = cache[post_head20_ln_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5ff80-0085-4889-ba13-2286551f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_ln = pre_final_ln[:, 0]\n",
    "post_final_ln = post_final_ln[:, 0]\n",
    "\n",
    "pre_head20_ln_name = pre_head20_ln_name[:, 1]\n",
    "post_head20_ln_name = post_head20_ln_name[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c81f2-4c7e-47e9-bdf0-fd53dc605e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ln_coefs = fit_ln(pre_final_ln, post_final_ln)\n",
    "head20_ln_coefs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bda6b4-740d-41e0-9f73-0c3fc2fbc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "clean_logitts = model.run_with_hooks(clean_tokens)\n",
    "corrupted_logits = model.run_with_hooks(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf121fc-ff1b-4641-93f0-06ae64ed3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = clean_tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ef522f-db63-4dd6-85f6-e89c03f1b2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_resid(resid, hook, corrupted_cache, position_idx):\n",
    "    resid[:, position_idx] = corrupted_cache[hook.name][:, position_idx]\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6572d008-dbd0-46e8-8f71-8b67141b571f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94d232-e6a3-47ff-8408-587ab1417c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5139468-6eff-417d-8647-4675688bb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    for position_idx in range(seq_len):\n",
    "        hook_name = get_act_name(\"resid_post\", layer_idx)\n",
    "        hook_func = partial(\n",
    "            patch_resid,\n",
    "            corrupted_cache=corrupted_cache,\n",
    "            position_idx=position_idx\n",
    "        )\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            clean_tokens,\n",
    "            fwd_hooks=[(hook_name, hook_func)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a77e8f-5e72-49a0-b2d8-9cb5d196b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(logits, targets):\n",
    "    log_probs = F.log_probs(logits, dim=-1)\n",
    "    return -log_probs[range(targets.shape[0]), targets].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc895f-ecb3-4340-baf9-ab2adb34c161",
   "metadata": {},
   "source": [
    "Okay, let's break this down step-by-step:\n",
    "\n",
    "$12 \\times n l \\times h d^2$\n",
    "\n",
    "This is calculating the total number of parameters in a Transformer-based model, where:\n",
    "\n",
    "$n l$ = number of layers\n",
    "$h d$ = hidden dimension size\n",
    "In a Transformer layer, there are 4 main linear layers with the following parameter sizes:\n",
    "\n",
    "Input to self-attention: $h d \\times 3h d$ parameters\n",
    "Self-attention to hidden: $3h d \\times h d$ parameters\n",
    "Hidden to MLP: $h d \\times 4h d$ parameters\n",
    "MLP to hidden: $4h d \\times h d$ parameters\n",
    "Adding up the sizes of these 4 layers:\n",
    "\n",
    "$h d \\times 3h d = 3h d^2$\n",
    "$3h d \\times h d = 3h d^2$\n",
    "$h d \\times 4h d = 4h d^2$\n",
    "$4h d \\times h d = 4h d^2$\n",
    "Total per layer = $3h d^2 + 3h d^2 + 4h d^2 + 4h d^2 = 12h d^2$\n",
    "\n",
    "Since there are $n l$ layers in the model, the total parameters is:\n",
    "\n",
    "$12h d^2 \\times n l = 12 \\times n l \\times h d^2$\n",
    "\n",
    "So this formula is calculating the total number of parameters in a Transformer model by summing up the parameters in each of the 4 linear layers per layer, multiplied by the number of layers.$2 \\times b s z \\times s e q \\times h d \\times n l / c i$\n",
    "\n",
    "This is calculating the memory required to store activation checkpoints in a Transformer model using activation checkpointing, where:\n",
    "\n",
    "$bsz$ = batch size\n",
    "$seq$ = sequence length\n",
    "$hd$ = hidden dimension size\n",
    "$nl$ = number of layers\n",
    "$ci$ = number of layers between activation checkpoints\n",
    "During forward propagation, the activations are stored at set intervals based on $ci$.\n",
    "\n",
    "The activation size per layer is approximately:\n",
    "\n",
    "$bsz \\times seq \\times hd$\n",
    "\n",
    "This is because the input to each Transformer layer is of shape (batch size, sequence length, hidden dimension).\n",
    "\n",
    "So for $ci$ layers, the total activation size is:\n",
    "\n",
    "$ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "These activations need to be stored during both the forward and backward pass, so we multiply by 2:\n",
    "\n",
    "$2 \\times ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "There are $nl$ layers total, with checkpoints stored every $ci$ layers, so the total number of activation checkpoints stored is $\\frac{nl}{ci}$.\n",
    "\n",
    "Therefore, the total memory required for activation checkpoints is:\n",
    "\n",
    "$2 \\times \\frac{nl}{ci} \\times ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$2 \\times bsz \\times seq \\times hd \\times nl / ci$\n",
    "\n",
    "So this formula calculates the total memory required to store the activation checkpoints used for activation checkpointing in a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179c768-b749-49bb-8e46-41d822f0cfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e92f29-bba6-4398-a7c9-5ee8435661ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b3fde8-a907-4db4-9f99-4e49f467b9ac",
   "metadata": {},
   "source": [
    "$2 \\times b s z \\times s e q \\times h d \\times n l / c i$\n",
    "\n",
    "This is calculating the memory required to store activation checkpoints in a Transformer model using activation checkpointing, where:\n",
    "\n",
    "$bsz$ = batch size\n",
    "$seq$ = sequence length\n",
    "$hd$ = hidden dimension size\n",
    "$nl$ = number of layers\n",
    "$ci$ = number of layers between activation checkpoints\n",
    "During forward propagation, the activations are stored at set intervals based on $ci$.\n",
    "\n",
    "The activation size per layer is approximately:\n",
    "\n",
    "$bsz \\times seq \\times hd$\n",
    "\n",
    "This is because the input to each Transformer layer is of shape (batch size, sequence length, hidden dimension).\n",
    "\n",
    "So for $ci$ layers, the total activation size is:\n",
    "\n",
    "$ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "These activations need to be stored during both the forward and backward pass, so we multiply by 2:\n",
    "\n",
    "$2 \\times ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "There are $nl$ layers total, with checkpoints stored every $ci$ layers, so the total number of activation checkpoints stored is $\\frac{nl}{ci}$.\n",
    "\n",
    "Therefore, the total memory required for activation checkpoints is:\n",
    "\n",
    "$2 \\times \\frac{nl}{ci} \\times ci \\times bsz \\times seq \\times hd$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$2 \\times bsz \\times seq \\times hd \\times nl / ci$\n",
    "\n",
    "So this formula calculates the total memory required to store the activation checkpoints used for activation checkpointing in a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e937ff9-a71e-4efc-a798-9c4f5d509cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fa558e-3b31-4c82-8fd4-10822963f967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f108adc-fd96-4074-a2bf-03aac37da40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5c7074-c3b1-4044-82ae-c8b58129d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_to_str(base, *args, **kwargs):\n",
    "    \"\"\"Construct a string representation of a call.\n",
    "\n",
    "    Args:\n",
    "        base (str): name of the call\n",
    "        args (tuple, optional): args to ``base``\n",
    "        kwargs (dict, optional): kwargs supplied to ``base``\n",
    "\n",
    "    Returns:\n",
    "        str: A string representation of base(*args, **kwargs)\n",
    "    \"\"\"\n",
    "    name = f'{base}('\n",
    "    if args:\n",
    "        name += ', '.join(repr(arg) for arg in args)\n",
    "        if kwargs:\n",
    "            name += ', '\n",
    "    if kwargs:\n",
    "        name += ', '.join(f'{key}={repr(arg)}' for key, arg in kwargs.items())\n",
    "    name += ')'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b831bb-e5fb-42fa-bf6e-44bc25e3f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class PipeSchedule(ABC):\n",
    "    \"\"\"Directs the execution of a pipeline engine by generating sequences of\n",
    "    :class:`PipeInstruction`.\n",
    "\n",
    "    Schedules are generators that yield sequences of\n",
    "    :class:`PipeInstruction` to process the micro-batches in one batch.\n",
    "    Each yielded step is atomic in the sense that a barrier\n",
    "    synchronization can be placed between successive steps without\n",
    "    deadlock.\n",
    "\n",
    "    Below is an example schedule that implements data parallelism with gradient accumulation:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        class DataParallelSchedule(PipeSchedule):\n",
    "            def steps(self):\n",
    "                for step_id in range(self.micro_batches):\n",
    "                    cmds = [\n",
    "                        LoadMicroBatch(buffer_id=0),\n",
    "                        ForwardPass(buffer_id=0),\n",
    "                        BackwardPass(buffer_id=0),\n",
    "                    ]\n",
    "                    if step_id == self.micro_batches - 1:\n",
    "                        cmds.extend([\n",
    "                            ReduceGrads(),\n",
    "                            OptimizerStep(),\n",
    "                        ])\n",
    "                    yield cmds\n",
    "\n",
    "            def num_pipe_buffers(self):\n",
    "                return 1\n",
    "\n",
    "    Args:\n",
    "        micro_batches (int): The number of micro-batches that comprise a batch.\n",
    "        stages (int): The number of pipeline stages.\n",
    "        stage_id (int): The pipe stage that will execute the generated schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, micro_batches, stages, stage_id):\n",
    "        super().__init__()\n",
    "        self.micro_batches = micro_batches\n",
    "        self.stages = stages\n",
    "        self.stage_id = stage_id\n",
    "        self.prev_stage = self.stage_id - 1\n",
    "        self.next_stage = self.stage_id + 1\n",
    "\n",
    "    @abstractmethod\n",
    "    def steps(self):\n",
    "        \"\"\"Yield a list of :class:`PipeInstruction` for each step in the schedule.\n",
    "\n",
    "        .. note::\n",
    "            Schedules must implement ``steps()`` to define the schedule.\n",
    "\n",
    "        Returns:\n",
    "            Instructions to be executed as one step of the pipeline\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def num_pipe_buffers(self):\n",
    "        \"\"\"The number of pipeline buffers that will be used by this stage.\n",
    "\n",
    "        .. note::\n",
    "            Schedules should specialize ``num_pipe_buffers()`` for memory savings at scale.\n",
    "\n",
    "        Returns:\n",
    "            The number of buffers for the engine to allocate.\n",
    "        \"\"\"\n",
    "        return self.micro_batches\n",
    "\n",
    "    def _valid_micro_batch(self, micro_batch_id):\n",
    "        return 0 <= micro_batch_id < self.micro_batches\n",
    "\n",
    "    def _valid_stage(self, stage_id):\n",
    "        return 0 <= stage_id < self.stages\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        \"\"\"Stage index used to configure this schedule.\"\"\"\n",
    "        return self.stage_id\n",
    "\n",
    "    @property\n",
    "    def num_stages(self):\n",
    "        \"\"\"The number of total pipeline stages used to configure this schedule.\"\"\"\n",
    "        return self.stages\n",
    "\n",
    "    @property\n",
    "    def num_micro_batches(self):\n",
    "        \"\"\"The number of total micro_batches used to configure this schedule.\"\"\"\n",
    "        return self.micro_batches\n",
    "\n",
    "    @property\n",
    "    def is_first_stage(self):\n",
    "        \"\"\"True if the configured ``stage_id`` is the first stage in the pipeline.\"\"\"\n",
    "        return self.stage_id == 0\n",
    "\n",
    "    @property\n",
    "    def is_last_stage(self):\n",
    "        \"\"\"True if the configured ``stage_id`` is the last stage in the pipeline.\"\"\"\n",
    "        return self.stage_id == self.stages - 1\n",
    "\n",
    "    def _buffer_idx(self, micro_batch_id):\n",
    "        \"\"\"Map a micro-batch index to a pipeline buffer index.\n",
    "\n",
    "        This method uses a cyclic allocation strategy.\n",
    "\n",
    "        Args:\n",
    "            micro_batch_id (int): The micro-batch index relative to the beginning of the schedule.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the buffer that should store data.\n",
    "        \"\"\"\n",
    "        assert self._valid_micro_batch(micro_batch_id)\n",
    "        return micro_batch_id % self.num_pipe_buffers()\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.it = None\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.it is None:\n",
    "            self.it = self.steps()\n",
    "        return next(self.it)\n",
    "\n",
    "\n",
    "class InferenceSchedule(PipeSchedule):\n",
    "    \"\"\"A schedule for inferencing batches using pipeline parallelism.\n",
    "    \"\"\"\n",
    "\n",
    "    def steps(self):\n",
    "        \"\"\"\"\"\"\n",
    "        prev_micro_batch_id = -1\n",
    "        total_steps = self.micro_batches + self.stages - 1\n",
    "        for step_id in range(total_steps):\n",
    "            cmds = []\n",
    "            micro_batch_id = step_id - self.stage_id\n",
    "\n",
    "            # Alternate send/recv buffers\n",
    "            if _is_even(self.stage_id):\n",
    "                recv_buf = step_id % 2\n",
    "                send_buf = (step_id + 1) % 2\n",
    "            else:\n",
    "                recv_buf = (step_id + 1) % 2\n",
    "                send_buf = step_id % 2\n",
    "\n",
    "            if self.is_first_stage or self.is_last_stage:\n",
    "                if self._valid_micro_batch(micro_batch_id):\n",
    "                    cmds.append(LoadMicroBatch(recv_buf))\n",
    "\n",
    "            if _is_even(self.stage_id):\n",
    "                if self._valid_stage(self.next_stage):\n",
    "                    if self._valid_micro_batch(micro_batch_id - 1):\n",
    "                        cmds.append(SendActivation(send_buf))\n",
    "                if self._valid_stage(self.prev_stage):\n",
    "                    if self._valid_micro_batch(micro_batch_id):\n",
    "                        cmds.append(RecvActivation(recv_buf))\n",
    "            else:\n",
    "                if self._valid_stage(self.prev_stage):\n",
    "                    if self._valid_micro_batch(micro_batch_id):\n",
    "                        cmds.append(RecvActivation(recv_buf))\n",
    "\n",
    "                if self._valid_stage(self.next_stage):\n",
    "                    if self._valid_micro_batch(micro_batch_id - 1):\n",
    "                        cmds.append(SendActivation(send_buf))\n",
    "\n",
    "            if self._valid_micro_batch(micro_batch_id):\n",
    "                cmds.append(ForwardPass(recv_buf))\n",
    "\n",
    "            yield cmds\n",
    "\n",
    "    def num_pipe_buffers(self):\n",
    "        \"\"\"Only two pipeline buffers are required for inferencing.\n",
    "\n",
    "        Returns:\n",
    "            ``2``\n",
    "        \"\"\"\n",
    "        return 2\n",
    "\n",
    "\n",
    "class TrainSchedule(PipeSchedule):\n",
    "    \"\"\"A schedule for training a batch using hybrid parallelism.\n",
    "\n",
    "    Pipeline parallelism is extracted through gradient accumulation and thus\n",
    "    convergence follows that of a data parallel approach with the same batch\n",
    "    size.\n",
    "    \"\"\"\n",
    "\n",
    "    def steps(self):\n",
    "        \"\"\"\"\"\"\n",
    "        prev_micro_batch_id = -1\n",
    "        total_steps = 2 * (self.micro_batches + self.stages - 1)\n",
    "        for step_id in range(total_steps):\n",
    "            # Map the step of the pipeline to the micro-batch id and also whether it is a\n",
    "            # forward or backward pass step.\n",
    "            micro_batch_id, is_forward = self._step_to_micro_batch(step_id)\n",
    "\n",
    "            if self._valid_micro_batch(prev_micro_batch_id):\n",
    "                prev_buffer = self._buffer_idx(prev_micro_batch_id)\n",
    "            if self._valid_micro_batch(micro_batch_id):\n",
    "                curr_buffer = self._buffer_idx(micro_batch_id)\n",
    "\n",
    "            cmds = []\n",
    "\n",
    "            # Exchange activations\n",
    "            if is_forward:\n",
    "                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                    cmds.append(SendGrad(prev_buffer))\n",
    "                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                    cmds.append(RecvActivation(curr_buffer))\n",
    "            else:\n",
    "                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                    cmds.append(RecvGrad(curr_buffer))\n",
    "                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                    cmds.append(SendActivation(prev_buffer))\n",
    "\n",
    "            # First/last stage loads\n",
    "            if self.stage_id == 0 or self.stage_id == self.stages - 1:\n",
    "                if is_forward and self._valid_micro_batch(micro_batch_id):\n",
    "                    cmds.append(LoadMicroBatch(curr_buffer))\n",
    "\n",
    "            # Computation\n",
    "            if self._valid_micro_batch(micro_batch_id):\n",
    "                if is_forward:\n",
    "                    cmds.append(ForwardPass(curr_buffer))\n",
    "                else:\n",
    "                    cmds.append(BackwardPass(curr_buffer))\n",
    "\n",
    "            # Model step at the end of the batch\n",
    "            if step_id == total_steps - 1:\n",
    "                cmds.append(ReduceTiedGrads())\n",
    "                cmds.append(ReduceGrads())\n",
    "                cmds.append(OptimizerStep())\n",
    "\n",
    "            # Prepare state for next time\n",
    "            prev_micro_batch_id = micro_batch_id\n",
    "            yield cmds\n",
    "\n",
    "    def num_pipe_buffers(self):\n",
    "        \"\"\"Return the number of pipeline buffers required for this stage.\n",
    "\n",
    "        This is equivalent to the maximum number of in-flight forward passes,\n",
    "        since we need to remember the activations of forward passes in order\n",
    "        to run backpropagation. For synchronous 1F1B, this is equivalent to\n",
    "        the index difference between this stage and the last stage.\n",
    "        \"\"\"\n",
    "        buffers = min(self.stages - self.stage_id, self.micro_batches)\n",
    "        return max(2, buffers)\n",
    "\n",
    "    def _step_to_micro_batch(self, step_id):\n",
    "        if _is_even(step_id) and _is_even(self.stage_id):\n",
    "            micro_batch_id = self._even_step_forward_id(step_id)\n",
    "            is_forward = True\n",
    "\n",
    "        elif _is_odd(step_id) and _is_odd(self.stage_id):\n",
    "            micro_batch_id = self._odd_step_forward_id(step_id)\n",
    "            is_forward = True\n",
    "\n",
    "        elif _is_even(step_id) and _is_odd(self.stage_id):\n",
    "            micro_batch_id = self._even_step_backward_id(step_id)\n",
    "            is_forward = False\n",
    "\n",
    "        elif _is_odd(step_id) and _is_even(self.stage_id):\n",
    "            micro_batch_id = self._odd_step_backward_id(step_id)\n",
    "            is_forward = False\n",
    "\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return micro_batch_id, is_forward\n",
    "\n",
    "    def _even_step_forward_id(self, step_id):\n",
    "        base = step_id // 2\n",
    "        micro_batch_id = int(base - self.stage_id // 2)\n",
    "        return micro_batch_id\n",
    "\n",
    "    def _odd_step_forward_id(self, step_id):\n",
    "        base = (step_id - 1) // 2\n",
    "        micro_batch_id = int(base - self.stage_id // 2)\n",
    "        return micro_batch_id\n",
    "\n",
    "    def _even_step_backward_id(self, step_id):\n",
    "        base = step_id // 2\n",
    "        micro_batch_id = int(base - self.stages + (self.stage_id + 1) // 2)\n",
    "        return micro_batch_id\n",
    "\n",
    "    def _odd_step_backward_id(self, step_id):\n",
    "        base = ((step_id - 1) // 2) - self.stages + 1\n",
    "        micro_batch_id = int(base + self.stage_id // 2)\n",
    "        return micro_batch_id\n",
    "\n",
    "\n",
    "class DataParallelSchedule(PipeSchedule):\n",
    "    \"\"\"An example schedule that trains using traditional data parallelism with gradient\n",
    "    accumulation.\n",
    "    \"\"\"\n",
    "\n",
    "    def steps(self):\n",
    "        \"\"\"\"\"\"\n",
    "        for step_id in range(self.micro_batches):\n",
    "            cmds = [\n",
    "                LoadMicroBatch(buffer_id=0),\n",
    "                ForwardPass(buffer_id=0),\n",
    "                BackwardPass(buffer_id=0),\n",
    "            ]\n",
    "            if step_id == self.micro_batches - 1:\n",
    "                cmds.extend([\n",
    "                    ReduceGrads(),\n",
    "                    OptimizerStep(),\n",
    "                ])\n",
    "            yield cmds\n",
    "\n",
    "    def num_pipe_buffers(self):\n",
    "        \"\"\"Only one pipeline buffer needed.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "\n",
    "class PipeInstruction:\n",
    "    \"\"\"Base class for all instructions to be executed by the pipeline engine.\n",
    "\n",
    "    All keyword arguments are stored as members similar to a ``namedtuple``. These are\n",
    "    then accessible to the :class:`PipeEngine` during execution.\n",
    "\n",
    "    Args:\n",
    "        kwargs (optional): keyword arguments to store as members\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.name = self.__class__.__name__\n",
    "        self.kwargs = kwargs\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return call_to_str(self.name, **self.kwargs)\n",
    "\n",
    "\n",
    "class OptimizerStep(PipeInstruction):\n",
    "    \"\"\"Performs one step with the optimizer and zeros gradients.\n",
    "\n",
    "    .. note:: Should be issued after :class:`ReduceGrads` and :class:`ReduceTiedGrads`.\n",
    "\n",
    "    .. note:: Can be a synchronization point among data-parallel ranks.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ReduceGrads(PipeInstruction):\n",
    "    \"\"\"Reduce the computed gradients among data-parallel processes within the stage.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ReduceTiedGrads(PipeInstruction):\n",
    "    \"\"\"Reduce the computed gradients of tied modules within a pipeline-parallel group.\n",
    "\n",
    "    .. warning::\n",
    "        The stages included in this synchronization point are not known until\n",
    "        the model is partitioned among pipeline stages. In the worst case, it\n",
    "        includes all pipeline stages. This instruction should be scheduled\n",
    "        carefully to avoid deadlocks.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BufferOpInstruction(PipeInstruction):\n",
    "    \"\"\"A pipeline instruction that operates on pipeline buffer(s).\n",
    "\n",
    "    Args:\n",
    "        buffer_id (int): the index of the pipeline buffer() to modify.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_id, **kwargs):\n",
    "        super().__init__(buffer_id=buffer_id, **kwargs)\n",
    "\n",
    "\n",
    "# IO\n",
    "class LoadMicroBatch(BufferOpInstruction):\n",
    "    \"\"\"Load a micro-batch into a buffer.\n",
    "\n",
    "    Roughly:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        buffers['inputs'][buffer_id] = next(data_iter)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Compute\n",
    "class ForwardPass(BufferOpInstruction):\n",
    "    \"\"\"Compute a forward pass.\n",
    "\n",
    "    Roughly:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        buffers['outputs'][buffer_id] = forward(buffers['inputs'][buffer_id])\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BackwardPass(BufferOpInstruction):\n",
    "    \"\"\"Compute a backward pass and accumulate gradients.\n",
    "\n",
    "    Roughly:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        outputs = buffers['outputs'][buffer_id]\n",
    "        gradients = buffers['gradients'][buffer_id]\n",
    "        torch.autograd.backward(tensors=outputs,\n",
    "                                grad_tensors=gradients)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Communication\n",
    "class SendActivation(BufferOpInstruction):\n",
    "    \"\"\"Send activations to the next stage in the pipeline.\n",
    "\n",
    "    Roughly:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        send(buffers['outputs'][buffer_id])\n",
    "\n",
    "    .. note::\n",
    "        The communication is blocking and must be paired with a :class:`RecvActivation`\n",
    "        on the next pipeline stage to avoid deadlock.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class RecvActivation(BufferOpInstruction):\n",
    "    \"\"\"Receive activations from the previous stage in the pipeline.\n",
    "\n",
    "    Roughly:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        buffers['inputs'][buffer_id] = recv()\n",
    "\n",
    "    .. note::\n",
    "        The communication is blocking and must be paired with a :class:`SendActivation`\n",
    "        on the previous pipeline stage to avoid deadlock.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class SendGrad(BufferOpInstruction):\n",
    "    \"\"\"Send computed gradients to the previous pipeline stage.\n",
    "    with respect to the received activations\n",
    "\n",
    "    .. note::\n",
    "        Only received tensors with ``requires_grad==True`` will produce gradients.\n",
    "        Missing gradients will be replaced with ``None`` on the receiving stage.\n",
    "\n",
    "    .. note::\n",
    "        The communication is blocking and must be paired with a :class:`RecvGrad`\n",
    "        on the previous pipeline stage to avoid deadlock.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class RecvGrad(BufferOpInstruction):\n",
    "    \"\"\"Receive computed gradients the next pipeline stage.\n",
    "\n",
    "    .. note::\n",
    "        Only activations with ``requires_grad==True`` will produce gradients.\n",
    "        Missing gradients will be replaced with ``None``.\n",
    "\n",
    "    .. note::\n",
    "        The communication is blocking and must be paired with a :class:`SendGrad`\n",
    "        on the next pipeline stage to avoid deadlock.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def _is_even(x):\n",
    "    return x % 2 == 0\n",
    "\n",
    "\n",
    "def _is_odd(x):\n",
    "    return x % 2 != 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa92834f-2bb9-42d9-b552-076609ba2990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[RecvActivation(buffer_id=0), LoadMicroBatch(buffer_id=0), ForwardPass(buffer_id=0)]\n",
      "[BackwardPass(buffer_id=0)]\n",
      "[SendGrad(buffer_id=0), RecvActivation(buffer_id=1), LoadMicroBatch(buffer_id=1), ForwardPass(buffer_id=1)]\n",
      "[BackwardPass(buffer_id=1)]\n",
      "[SendGrad(buffer_id=1), RecvActivation(buffer_id=0), LoadMicroBatch(buffer_id=0), ForwardPass(buffer_id=0)]\n",
      "[BackwardPass(buffer_id=0)]\n",
      "[SendGrad(buffer_id=0), RecvActivation(buffer_id=1), LoadMicroBatch(buffer_id=1), ForwardPass(buffer_id=1)]\n",
      "[BackwardPass(buffer_id=1)]\n",
      "[SendGrad(buffer_id=1), RecvActivation(buffer_id=0), LoadMicroBatch(buffer_id=0), ForwardPass(buffer_id=0)]\n",
      "[BackwardPass(buffer_id=0)]\n",
      "[SendGrad(buffer_id=0)]\n",
      "[]\n",
      "[ReduceTiedGrads(), ReduceGrads(), OptimizerStep()]\n"
     ]
    }
   ],
   "source": [
    "for tasks in TrainSchedule(micro_batches=5, stages=4, stage_id=3):\n",
    "    print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361d8289-5a1c-46c1-b25e-144f11cfda80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.clock_idx = 0\n",
    "        self.data = {\n",
    "            0: ['a', 'b', 'c'],\n",
    "            1: ['d', 'e', 'f'],\n",
    "            2: ['g', 'h', 'i'],\n",
    "            # etc...\n",
    "        }\n",
    "        self.condition = threading.Condition()\n",
    "\n",
    "    @property\n",
    "    def current_data(self):\n",
    "        with self.condition:\n",
    "            while True:\n",
    "                for item in self.data.get(self.clock_idx, []):\n",
    "                    yield item\n",
    "                # Wait for a notification to proceed\n",
    "                self.condition.wait()\n",
    "\n",
    "    def increase_clock(self):\n",
    "        with self.condition:\n",
    "            self.clock_idx += 1\n",
    "            if self.clock_idx not in self.data:\n",
    "                self.clock_idx = 0\n",
    "            # Notify waiting threads that they can proceed\n",
    "            self.condition.notify_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28529c2-2b20-4b61-9f83-6a6c61f3c853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_partitions = 1\n",
    "n_microbatches = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7678dc24-bf17-4cca-850c-be67b65998ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4+3-1)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717d6aa-8d6e-4fd9-9575-f1510ab65bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def increase_clock_every_second(my_instance):\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        my_instance.increase_clock()\n",
    "\n",
    "my_instance = MyClass()\n",
    "\n",
    "clock_thread = threading.Thread(target=increase_clock_every_second, args=(my_instance,))\n",
    "clock_thread.start()\n",
    "\n",
    "for item in my_instance.current_data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9373c1b-9773-4d9e-b9eb-5b262d39d6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
