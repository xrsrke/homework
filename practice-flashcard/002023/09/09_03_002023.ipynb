{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffba4572-8ace-4571-85f6-58f965b55fd7",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f4c5033-d10a-4096-8039-3be795d4bbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bfd814-6d9e-43a7-ae2d-043ac0999734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4074219-ba6e-456b-8785-9a75c3cc9b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = None\n",
    "    \n",
    "    def prefetch(self):\n",
    "        self.data = torch.load(self.filename)\n",
    "        total_elements = sum([self.data[i].numel() for i in idxs])\n",
    "        self.cache = torch.zeros(total_elements, dtype=self.data.dtype)\n",
    "        \n",
    "        offset = 0\n",
    "        for i in idxs:\n",
    "            n_elements = self.data[i].numel()\n",
    "            \n",
    "            self.cache[offset:offset+n_elements] = n_elements.view(dim=-1)\n",
    "            offset += n_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3e9ea-2932-49d7-8515-088db6840642",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cache[offset:offset+n_elements] = data[i].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6193263e-4d1d-4b60-94f5-5f8562a2c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelContext:\n",
    "    def init_rpc_workers(self):\n",
    "        if self.pipeline_parallel_size > 1:\n",
    "            init_method = f\"tcp://{host}:\"\n",
    "            ranks = self.get_ranks_in_group(ParallelMode.PIPELINE)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                rpc_worker_map = {\n",
    "                    rank: WORKER_NAME.format(rank)\n",
    "                    for rank in ranks\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec3b3c-c12d-43a2-9737-643c166e9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear, attention, layernorm, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17923bfc-9afb-4600-a9a6-6238e62f00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine global rank\n",
    "step 2: resize embedding size\n",
    "step 3: resize unembedding size\n",
    "step 4: parallelize linear layers, embeddings, attention, layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10f8e1-41a6-4596-a45b-b4a9e552ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: package\n",
    "step 2: invoke rpc\n",
    "step 3: receive \n",
    "step 4: execute based on the rpc call's logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1361c-9081-4e52-b4f6-20ed99edae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool watcher, worker thread, job selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18a7bc7-dab5-48d7-b9ee-7b1eda2a15fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3dc37bf-ed3b-4361-bd27-4cde4ae9902a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _P2P:\n",
    "    def recv(self, src_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        \n",
    "        dtype, requires_grad, shape = self._recv_metadata(src_rank, parallel_context, parallel_mode)\n",
    "        \n",
    "        data = torch.zeros(shape, requires_grad=requires_grad, dtype=dtype)\n",
    "        dist.recv(data, src=src_rank, group=group)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _recv_metadata(self, src_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        \n",
    "        dtype = torch.zeros(1)\n",
    "        dist.recv(dtype, src=src_rank, group=group)\n",
    "        dtype = ID_TO_DTYPE[dtype]\n",
    "        \n",
    "        requires_grad = torch.zeros(1)\n",
    "        dist.recv(requires_grad, src=src_rank, group=group)\n",
    "        requires_grad = True if requires_grad == 1 else False\n",
    "        \n",
    "        shape = torch.zeros(1)\n",
    "        dist.recv(shape, src=src_rank, group=group)\n",
    "        \n",
    "        return dtype, requires_grad, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7aa0f8-2546-4145-9527-76f5f2ff58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recv(src_rank, dst_rank, parallel_context, parallel_mode: ParallelMode.PIPELINE):\n",
    "    rank = parallel_context.get_local_rank(ParallelMode.PIPELINE)\n",
    "    if rank == dst_rank:\n",
    "        return _P2P().recv(dst_rank, parallel_context, parallel_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a439aba-1820-4ab1-a713-f99c299c06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximize storage\n",
    "minimize communication\n",
    "minimize flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f515a2b-39f9-463a-b2b1-3b1b8e396418",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: mask targets\n",
    "step 2: local_predicted_logits\n",
    "step 3: global_predicted_logits\n",
    "step 4: log(...)\n",
    "step 5: loss = log(...) - global_predicted_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5e397-385c-4bd0-a06e-713b1fb3833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "- monitor node changes\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9108b-34b5-467f-b4b3-91bacb221fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank*partition_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025196f-8677-4fca-b101-2f64aedb45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx+partition_sze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f25c7f-5648-4a98-8202-e10541156c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        dist.all_reduce(grad_input)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4633ae68-7188-4aff-9ff7-e02b1ff68409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Gather(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = dist.get_world_size()\n",
    "        inputs = [torch.zeros_like(input) for _ in range(world_size)]\n",
    "        dist.all_gather(inputs, input)\n",
    "        inputs = torch.cat(inputs, dim=-1)\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        \n",
    "        per_partition = grad_input.shape[-1] // world_size\n",
    "        chunks = torch.split(\n",
    "            grad_input,\n",
    "            split_size_or_sections=per_partition,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        return chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da91df5c-976f-48bb-a3ac-1095292cc3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        per_partition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            per_partition, input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.randn(per_partition))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        parallel_input = Broadcast.apply(input)\n",
    "        parallel_output = F.linear(parallel_input, self.weight, self.bias)\n",
    "        output = Gather.apply(parallel_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31aac97e-e5fa-4b2a-8a42-aeb426357aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        dist.all_reduce(input, group=group)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ea328a-9481-4f9f-92e0-1cf6955db99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, parallel_context):\n",
    "        super().__init__()\n",
    "        world_size = parallel_context.get_world_size(ParallelMode.TENSOR)\n",
    "        num_embeddings_per_partition = num_embeddings // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            num_embeddings_per_partition,\n",
    "            embedding_dim\n",
    "        ))\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self._get_vocab_range(\n",
    "            num_embeddings_per_partition,\n",
    "            parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "        )\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def _get_vocab_range(self, partition_size, rank):\n",
    "        start_idx = partition_size*rank\n",
    "        end_idx = start_idx+partition_size\n",
    "        return start_idx, end_idx\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask_input = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input.clone() - self.vocab_start_idx\n",
    "        masked_input[mask_input] = 0\n",
    "        \n",
    "        parallel_embeddings = F.embedding(masked_input, self.weight)\n",
    "        parallel_embeddings[masked_input] = 0.\n",
    "        \n",
    "        embeddings = Reduce.apply(\n",
    "            parallel_embeddings,\n",
    "            self.parallel_context,\n",
    "            parallel_mode=ParallelMode.TENSOR\n",
    "        )\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0332cd-9a27-4b73-b7c0-59c6c3e77746",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: sharding\n",
    "step 2: mask targets\n",
    "step 3: local_embeddings\n",
    "step 4: global_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b94569-0cd8-4c20-b58e-e6195a0385d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddd240-721c-4790-99ae-3de76e739005",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "open_embeddings = W_E[:, open_idx]\n",
    "close_embeddings = W_E[:, close_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "861a83d7-720a-40ff-98f0-bbfc61e5a14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db884ea-53be-496d-be16-89411777bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_OV = model.W_V[layer_idx, head_idx] @ model.W_O[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ae564-f175-4c84-ab66-1964e383a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "open_embedding = W_E[:, open_idx]\n",
    "close_embedding = W_E[:, close_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39428aac-2779-4d9a-96eb-4c13714fed68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce5d53-ae4f-4377-90d9-8ba111e383f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_OV = model.W_V[layer_idx, head_idx] @ model.W_O[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe3aaff-1d5d-46df-83ae-8f91b4a52e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_embedding = open_embedding @ layer0_ln_coefs.T @ W_OV\n",
    "close_embedding = close_embedding @ layer0_ln_coefs.T @ W_OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93981f8-e1f6-44a7-95c4-541c163d9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = torch.cosine_similarity(open_embedding, close_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb7a051-aeb1-4796-b6f4-90765260e4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_sender_head_output(\n",
    "    acts, hook,\n",
    "    clean_cache, corrupted_cache,\n",
    "    target_head\n",
    "):\n",
    "    trg_layer_idx, trg_head_idx = target_head\n",
    "    if hook.layer() == trg_layer_idx:\n",
    "        corrupted_acts = corrupted_cache[hook.name]\n",
    "        acts[:, :, trg_head_idx] = corrupted_acts[:, :, trg_head_idx]\n",
    "    else:\n",
    "        acts = clean_cache[hook.name]\n",
    "    \n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68cc06bb-8533-40b8-b8ed-00336668517a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import product\n",
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d66f8-a94b-48f7-89d6-045d05a2598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a61ff-c640-4a94-9ab5-4b3d3e01ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_heads = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "receiver_layer_idxs = [7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd32ce0-9eef-4543-aa86-c73ca3e7aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_heads = list(product(\n",
    "    range(max(receiver_layer_idxs)),\n",
    "    range(model.cfg.n_heads)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e22063b-1765-4186-ac6c-6c0039abfc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receiver_names = [get_act_name(\"v\", layer_idx) for layer_idx in [7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c869a5c-0d7c-492d-aef3-9f68874223cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blocks.7.attn.hook_v', 'blocks.8.attn.hook_v']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receiver_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e08d26-220d-434d-bb4c-abe5ce94b9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3544222-3c77-4c73-8044-09622d143ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_receiver_head_input(acts, hook, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67e1e5-333e-4278-b3ca-7b9285308fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, head_idx in sender_heads:\n",
    "    model.reset_hooks()\n",
    "    filter_head_output = lambda name: name.endswith(\"z\")\n",
    "    hook_func = partial(\n",
    "        patch_sender_head_output,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache=corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    \n",
    "    model.add_hook(filter_head_output, hook_func)\n",
    "    _, patched_cache = model.run_with_cache(clean_tokens)\n",
    "    \n",
    "    \n",
    "    hook_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "189d8bf8-a6fa-44f7-ad95-72c4a0ae4b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff89df9-70a0-4974-ad5d-2e7dbdf40abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelContext:\n",
    "    def init_rpc_worker(self, host, port):\n",
    "        if self.pipeline_parallel_size > 1:\n",
    "            init_method = f\"rpc://{host}:{port}\"\n",
    "            rank = self.get_global_rank()\n",
    "            world_size = self.get_world_size(ParallelMode.GLOBAL)\n",
    "            \n",
    "            options = rpc.RpcBackendOptions(\n",
    "                init_method=init_method\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                ranks = self.get_ranks_in_group(ParallelMode.PIPELINE)\n",
    "                \n",
    "                rpc_worker_map = {\n",
    "                    rank: WORKER_NAME.format(rank)\n",
    "                    for rank in ranks\n",
    "                }\n",
    "                \n",
    "                for other in ranks:\n",
    "                    if other == rank:\n",
    "                        continue\n",
    "                    \n",
    "                    options.set_device_map()\n",
    "            \n",
    "            rpc.init_rpc(\n",
    "                name=WORKER_NAME.format(name),\n",
    "                rank=rank,\n",
    "                world_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454651a-8471-40d1-a5b6-b41db4e68dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: qkv\n",
    "step 2: split\n",
    "step 3: self\n",
    "atep 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881c291-5a3a-4cf2-94b1-c305c1a09e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: size\n",
    "step 2: split\n",
    "step 3: x\n",
    "step 4: gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa51c3f6-b983-4fba-93ba-989632653c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from queue import Queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2a9b0e4-f2fe-4bcc-917a-1896eb09870a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_worker(in_queue, out_queue):\n",
    "    while True:\n",
    "        task = in_queue.get()\n",
    "        output = task()\n",
    "        out_queue.put(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6726cb8-c175-47e2-885c-20a490701ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def spawn_workers(devices):\n",
    "    in_queues = []\n",
    "    out_queues = []\n",
    "    \n",
    "    for device in devices:\n",
    "        in_queue = Queue()\n",
    "        out_queue = Queue()\n",
    "        \n",
    "        thread = threading.Thread(target=run_worker, daemon=True)\n",
    "        thread.start()\n",
    "        \n",
    "        in_queues.append(in_queue)\n",
    "        out_queues.append(out_queue)\n",
    "    \n",
    "    yield in_queues, out_queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa5384-2930-4baa-a8f6-3dcf5d15fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1c170-6943-4a7f-83fd-b2c8e2ba8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = attn_weights.diagonal(dim1=-2, dim2=-1, offset=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc1a82-c349-4f89-9255-ca351399c428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4583678e-1763-43da-bb9e-20d7283c5ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_shape(input, _):\n",
    "    print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a2c90-c06a-4034-b3ca-90917a6355e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[1].register_forward_pre_hook(print_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ab1e0-8e5e-40da-9b56-21c6806d28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563b1b3-0813-4e59-a551-213601231ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3e2e7-2726-40ab-91bc-46c320006da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e1335-2059-4b56-b68a-1a9a50f71eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = cache[hook_name][:, neuron_idx].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d02ed7-acf0-4d16-bdc5-15b47db6be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_OV = model.W_V[0, 1] @ model.W_O[0, 1]\n",
    "W_QK = model.W_Q[1, 2] @ model.W_K[1, 2].T\n",
    "\n",
    "virtual_weight = W_OV @ W_QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6590144-9f93-4181-8bdf-b185ccb720b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(x@W_Q@W_K.T@x.T) @ x @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125d542-627f-4ee4-8b39-15ffc925c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98584585-298c-4288-9869-4882bf88f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391438b7-9b35-43aa-afc2-d1ece70f4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits[:, -1, :], dim=-1)\n",
    "predicted_log_probs = -log_probs[:, tokens[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772a3ea-03bf-416f-a209-267921512a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_stripe = attn_weights.diagonal(\n",
    "    dim1=-2,\n",
    "    dim2=-1,\n",
    "    offset=4-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688c370-30f2-4ce8-a133-29bf49848e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3356c1b2-a6ba-46bd-9deb-14ccefb8d5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receiver_heads = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "receiver_layer_idxs = [7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bad255d-42ca-4116-a0c4-9191b3f59af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa778fc7-2ace-4cf5-9f24-61ea4930729f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09ecf261-3f18-40f2-8979-8c4507e1313f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sender_heads = list(product(range(max(receiver_layer_idxs)), range(n_heads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be58ea95-d704-44e7-b453-ee0b9fbdd2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_sender_head_output(acts, hook, clean_cache, corrupted_cache, target_head):\n",
    "    trg_layer_idx, trg_head_idx = target_head\n",
    "    \n",
    "    if hook.layer() == trg_layer_idx:\n",
    "        acts[:, :, head_idx] = corrupted_cache[hook.name][:, :, head_idx]\n",
    "    else:\n",
    "        acts = clean_cache[hook.name]\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0aab5b63-38db-4fcb-ab2d-a67d1f80effb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pach_receiver_head_input(acts, hook, new_value):\n",
    "    acts[:, :, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bb56138-8d34-4bb2-8c37-d776355e8c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_sender_name = lambda x: x.endswith(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "215cf97a-e223-45cf-aae4-d9cfe797fb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receiver_input_names = [get_act_name(\"v\", layer_idx) for layer_idx in [7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e753120-7eca-4829-b7a9-6b19ba96e151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blocks.7.attn.hook_v', 'blocks.8.attn.hook_v']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receiver_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8026d308-5914-4740-a2f0-dd8fbb0330e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_receiver_name = lambda x: x in receiver_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791a4d6-b86e-4ba1-89f8-b17b74257fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, head_idx in sender_heads:\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    hook_func = partial(\n",
    "        patch_sender_head_output,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache=corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    \n",
    "    model.add_hook(filter_sender_name, hook_func)\n",
    "    _, patched_cache = model.run_with_cache(clean_tokens)\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    hook_func = \n",
    "    patched_logits = model.run_with_hooks(\n",
    "        clean_tokens,\n",
    "        fwd_hooks=[(filter_receiver_name, hook_func)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af368ad6-53f7-4cae-9fc3-c00f27b619f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87e6d7f0-b6b5-4ea4-9e21-206644114f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, output):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        wait_stream(\n",
    "            souce_stream=next_stream,\n",
    "            target_stream=prev_stream\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        wait_stream(\n",
    "            source_stream=prev_stream,\n",
    "            target_stream=next_stream\n",
    "        )\n",
    "        \n",
    "        return tuple([None, None, grad_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fba9d1-65ca-4f46-93bd-ef7b7d1ab2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(clock_idx+1, n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06fd92-a644-4fda-a648-3ccfbc04ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c7436-e630-43ab-89a0-5b0e4d8f007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.stream(stream1):\n",
    "    x_mean = x.mean(dim=-1)\n",
    "    \n",
    "with torch.cuda.stream(stream2):\n",
    "    y_mean = y.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0e329-543f-49a9-b203-58b32e822bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = dist.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    dist.send(x, dst=1)\n",
    "elif rank == 1:\n",
    "    dist.recv(tensor_will_be_received_data, src=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed1a8660-11b2-4b78-a4aa-eb14fe87b7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CachedDataset:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def prefetch(self, idxs):\n",
    "        self.data = torch.load(self.filename)\n",
    "        total_elements = sum([self.data[i] for i in idxs])\n",
    "        self.cache = torch.zeros(\n",
    "            total_elements,\n",
    "            dtype=self.data.dtype\n",
    "        )\n",
    "        \n",
    "        offset = 0\n",
    "        for i in idxs:\n",
    "            num_elements = self.data[i].numel()\n",
    "            self.cache[offset:offset+num_elements] = self.data[i].view(dim=-1)\n",
    "            offset += num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc18f1-560d-470c-97bb-16ef244f0376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ac628d2-9508-43f9-b105-e6756256a335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c152395e-4ffb-49fa-8887-f9c691ad93ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86706924-cc74-4d7d-9cda-fb82e814bcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_model_parallel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cae2a413-4329-471d-a865-e5bc5b0541c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_parallel_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22d924ba-dbd6-4e91-9b31-000c7af38da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b2de61c-3b1e-4822-bde5-0baa3da12a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "\n",
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_idx = i*num_pipeline_model_parallel_groups\n",
    "    end_idx = (i+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(\n",
    "            start_idx+j,\n",
    "            end_idx,\n",
    "            tensor_model_parallel_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f77d98-2413-4835-b4e0-283de056ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653584c-85e1-4150-b8e3-890eb0abbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.embed\n",
    "mlp = model.blocks[0].mlp\n",
    "ln2 = model.blocks[0].ln2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5bd52-9304-4292-aedb-59e4ce603fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = embed(tokens)\n",
    "resid_after_mlp0 = text_embeddings + mlp(ln2(text_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deed715d-5a06-4c11-9643-ab80dd400b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9bda844-9b3a-4c5d-b0de-6bbe504a5a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_k(input_tokens, layer_idx, head_idx):\n",
    "    hook_name = get_act_name(\"k\", layer_idx)\n",
    "    _, cache = model.run_with_cache(input_tokens)\n",
    "    return cache[hook_name][:, :, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd955f-1b5b-4a30-ac59-ed4d5f7f6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_open = get_k(all_open_tokens, layer_idx=0, head_idx=0)\n",
    "k_close = get_k(all_close_tokens, layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5125b0-fe86-4cac-96c9-dd9192f228a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_avg = (k_open+k_close) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5072b066-4c1c-437b-8b72-603887c45284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_k(acts, hook, new_k, head_idx):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040f184-7fc4-4306-881b-e8acc372e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"k\", layer_idx)\n",
    "hook_func = partial(\n",
    "    patch_k,\n",
    "    new_k=k_avg,\n",
    "    layer_idx=0,\n",
    "    head_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d2e17-ffa2-4f0e-bc05-e54cb4763fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265edbe-0256-47c4-bc29-633a9290c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute, forward, backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8baec8fa-3771-481c-a18e-a623b3e53456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_idx = i*num_pipeline_model_parallel_groups\n",
    "    end_idx = (i+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(\n",
    "            start_idx+j,\n",
    "            end_idx,\n",
    "            tensor_model_parallel_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ed3cb8d-2e11-4530-9428-3549619a9d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0565035c-60a3-4f54-9f33-39c7af699e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foo(func: Callable[[int, int], str]) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87759036-8b08-4dec-8c42-d6909043f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df947c8-70ed-46f8-9026-61da19150560",
   "metadata": {},
   "outputs": [],
   "source": [
    "ionic, covalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7483bb-9ff2-4be5-97c1-ba44fdf11fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "two uncertainty principles\n",
    "quantization of angular momentum\n",
    "quantization of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78f0df-a087-471e-baf9-7f6c3db4dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24146d3c-32e2-4384-8185-4194a965e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "- two uncertain principles\n",
    "- quantization of angular momentum\n",
    "- quantization of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebaef5-a41e-43c5-a372-5e6a8edf6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong, weak nuclear force, gravity, electromagitc force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2852781-ed14-4389-8bc0-5aaf813f87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "biocompatible, reliable recordings, neural plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b1416-05fb-47b4-8ae6-c4b43c59cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording, send, memory, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a62a9a-45c5-468d-834d-fa2a945479d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
