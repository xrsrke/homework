{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c906f75c-47f5-4331-8972-1c952207fa95",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4db791-e4f1-4be0-8d4c-5be1b899a593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb016de-2573-4983-ba7d-55937e4e053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter > all reduce > identity > gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996995d1-9f96-43bb-9a80-7064471b29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, linear, layer norm, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109579f-2b6c-401b-9548-abe9c06d54ae",
   "metadata": {},
   "source": [
    "start, begin forward, begin backward, finished backward, finished batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a468e-def5-4820-ba97-cef1e291175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"worker1\": {\"cuda:0\": \"cuda:1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3516698d-18c0-47e7-91c7-c8535b27ed09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe672d8-243a-4d26-af05-74735641181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelContext:\n",
    "    def init_rpc_workers(self, host, port):\n",
    "        if self.pipeline_parallel_size > 1:\n",
    "            rank = self.get_local_rank(ParallelMode.GLOBAL)\n",
    "            world_size = self.get_world_size(ParallelMode.GLOBAL)\n",
    "            \n",
    "            init_method = f\"tcp://{host}:{port}\"\n",
    "            options = rpc.RpcBackendOptions(\n",
    "                init_method=init_method\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                ranks = self.get_ranks_in_group(ParallelMode.GLOBAL)\n",
    "                worker_map = {\n",
    "                    rank: WORKER_NAME.format(rank)\n",
    "                    for rank in ranks\n",
    "                }\n",
    "                \n",
    "                for other in ranks:\n",
    "                    if other == rank:\n",
    "                        continue\n",
    "                    options.set_device_map(\n",
    "                        WORKER_NAME.format(other),\n",
    "                        {rank: other}\n",
    "                    )\n",
    "            \n",
    "            rpc.init_rpc(\n",
    "                name=WORKER_NAME.format(rank),\n",
    "                rank=rank,\n",
    "                world_size=world_size,\n",
    "                rpc_backend_options=options\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c04af3-b8c6-4c92-996e-4ce05b2cd31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _P2P:\n",
    "    def _send_metadata(self, data, dst_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        \n",
    "        dtype = torch.tensor(DTYPE_TO_ID[data.dtype])\n",
    "        dist.send(dtype, dst=dst_rank, group=group)\n",
    "        \n",
    "        requires_grad = torch.tensor(1 if data.requires_grad else 0)\n",
    "        dist.send(requires_grad, dst=dst_rank, group=group)\n",
    "        \n",
    "        shape = torch.tensor(data.shape)\n",
    "        dist.send(shape, dst=dst_rank, group=group)\n",
    "    \n",
    "    def send(self, data, dst_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        self._send_metadata(data, dst_rank, parallel_context, parallel_mode)\n",
    "        dist.send(data, dst=dst_rank, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603fe76f-95fa-4934-82db-5bb42a1de602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send(data, src_rank, dst_rank, parallel_context, parallel_mode):\n",
    "    rank = parallel_context.get_local_rank(parallel_mode)\n",
    "    \n",
    "    if rank == src_rank:\n",
    "        _P2P().send(data, dst_rank, parallel_context, parallel_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef48125-4d53-46c5-b0c2-8308f50f8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr = (int*)malloc(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32001712-c508-4d64-962a-4f07cb343dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "int zero() {\n",
    "    return 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337bdae-d3d1-4539-9264-4ce708be203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node > pod > container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd647092-bba7-4722-9fbd-4cc85f66a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73839a-4d08-46b0-9cf5-c20437dc8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = corrupted_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505e067-e6a4-4ed7-b734-9d157530491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.zeros(n_layers, batch_size, seq_len, n_heads, d_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d360b9e-074b-45d6-97b7-461cb71f0629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb43bb2a-bc04-4cbe-925d-01d6302523ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fc520-d5a4-48bc-83c7-db83a5918306",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    z = cache[hook_name]\n",
    "    \n",
    "    for sample_idxs in corrupted_groups:\n",
    "        template_z = cache[hook_name]\n",
    "        mean_z = reduce(template_z, \"batch_size seq_len n_heads d_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ab9ede-0274-4623-b7be-8f624c596768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receiver_heads = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "receiver_layers = [7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c4ff80-4068-400b-8b06-fba829328021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_sender_head_output(\n",
    "    acts, hook, clean_cache, corrupted_cache, target_head\n",
    "):\n",
    "    trg_layer_idx, trg_head_idx = target_head\n",
    "    \n",
    "    if trg_layer_idx == hook.layer():\n",
    "        acts[:, :, trg_head_idx] = corrupted_cache[hook.name][:, :, trg_head_idx]\n",
    "    else:\n",
    "        acts = clean_cache[hook.name]\n",
    "    \n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b46c7fe-9d0f-49ef-8262-d88c5fa22203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_receiver_head_input(acts, hook, patched_cache, receiver_heads):\n",
    "    head_idxs = [head_idx for layer_idx, head_idx in receiver_heads if layer_idx == hook.layer()]\n",
    "    acts[:, :, head_idxs] = patched_cache[hook.name][:, :, head_idxs]\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c389c6d5-28e2-4dbb-839e-8b28a177b906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1497c5a1-f028-45cb-b1bf-11d8f0fe1cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf18342-330c-418b-be1d-decf1266b4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sender_heads = list(product(range(max(receiver_layers)), range(n_heads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c324fc5-04fb-464c-9522-d3f628e1e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56983b-e8d0-489d-9059-e29552a4f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer_idx, head_idx in sender_heads:\n",
    "    model.reset_hooks()\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_sender_head_output,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache=corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    model.add_hook(hook_name, hook_func)\n",
    "    _, patched_cache = model.run_with_cache(clean_tokens)\n",
    "    \n",
    "    hook_name = get_act_name(\"v\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_sender_head_output,\n",
    "        patched_cache=patched_cache,\n",
    "        receiver_heads=receiver_heads\n",
    "    )\n",
    "    patched_logits = model.run_with_hook(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1ae3f-7af9-41e7-a275-262a785b9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "int zero() {\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fbbc67-0824-4adb-8439-1a79b5853496",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.roll(x, shifts=1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cec5c4-2bf2-49fb-b5f5-4ac89d4ddfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: get global rank\n",
    "step 2: resize vocab size\n",
    "step 3: resize lm_head\n",
    "step 4: parallelize embedding, linear, attn, layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343bb9b-e7ae-493b-b958-05dbb253c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: wait input\n",
    "step 2: get input\n",
    "step 3: construct task\n",
    "step 4: put\n",
    "step 5: wait for in\n",
    "step 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19409d2d-8c68-4a0c-b76f-a53a9442f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecab6d79-10f8-4dfb-befc-a87d1176f0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea335e8a-5aca-4c18-ae84-7c549157f3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7f1ec3-347d-4c5c-bb58-11f94a5e811e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    with lock:\n",
    "        print_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfdeb7-8153-4663-949d-958e04841572",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = threading.Thread(target=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ede17f-0799-46e3-9f19-4a91b5745252",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(x) -> output = forward(x) -> backward(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "632c4617-94cd-471b-a0cd-510d0dfa0a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d7738e9-5420-4cb1-b40c-014b048b12ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foo(func: Callable[[int, int], str]) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c562ea-8feb-487e-ac8b-8a97787e103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x**2 for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f2f04-1fe1-4bf9-8f84-985fb1489327",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = corrupted_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476d188-11aa-4b82-8b29-729cbae6ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.zeros(n_layers, batch_size, seq_len, n_heads, d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7e682-dda8-486a-a06f-b8f4b00af9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6ec7291-c3ce-4764-b521-dc18e79d7ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2eaa8-d015-4223-b2bf-38910e2f336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    for sample_idxs in corrupted_groups:\n",
    "        mean_z = reduce(\n",
    "            cache[hook_name][:, sample_idxs],\n",
    "            \"batch_size seq_len n_heads d_head -> batch_size n_heads d_head\"\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdedadf-4fda-45b2-8fc8-2c8da2c3bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(x@W_Q@W_K.T@x.T) @ x @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1579642-1aca-4224-bda2-51c35a70fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name mover heads, s-inhibition heads, duplication token head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208f5d4-bf05-4668-a694-719c1b4ca6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "821803ab-8733-4b71-9bcc-091f5aea65d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_layers = 3\n",
    "n_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27881703-80b9-4c67-aa22-2a32268e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_names = [get_act_name(\"mlp_out\", layer_idx) in range(n_layers)]\n",
    "attn_names = [get_act_name(\"result\", layer_idx), range(n_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11053ea-f6e7-46e0-a83e-1eaa0eefd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_names = [\"embed\", \"pos_embed\"] + mlp_names + attn_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "797ee2d7-add3-4a8f-b5d6-aa652f13c304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2272e-6677-4b3c-a623-2c5b2a24306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "decomposed_q = einsum(\n",
    "    input_components, W_Q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd3fe5-a944-4278-a7e9-43ce9514f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = model.W_K[layer_idx, head_idx]\n",
    "decomposed_k = einsum(\n",
    "    input_components, W_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53016b47-171c-4d85-8983-0b7f6a416de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = einsum(\n",
    "    decomposed_q, decomposed_k, \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7750c9f-0dea-48a4-8563-88f441ec4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp, attn, layer norm, embedding, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e3d2c-d421-483e-8077-85ee65dd44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, value, ,q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadc6af-25d8-4fa3-9e40-4832be67acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, discount_factor):\n",
    "b    factors = torch.pow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9aa3e1e-78a0-4854-baf1-5233ea43c2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hello():\n",
    "    print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e56120-c76a-4f29-a298-3bdb3ce89984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "xs = {lambda x: isinstance(x, int): hello()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68611764-55e8-4993-8be2-0153f59431f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "on run slice_weight if module don't have the attribution parallel_info, and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
