{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d628cb-04e1-4103-8d46-e97a9fc80c49",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df844b3-d821-4bfb-8117-34ed44e60ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d2ac99-49c0-404b-8483-e6dd49b22994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b81071-da0a-4598-8300-4d8672d4060c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, microbatches, partitions, devices):\n",
    "        self.batches = batches\n",
    "        self.partitions = partitions\n",
    "        self.devices = devices\n",
    "    \n",
    "    def fit(self):\n",
    "        n_batches = len(self.batches)\n",
    "        n_partitions = len(self.partitions)\n",
    "        \n",
    "        with spawn_worker(self.devices) as (in_queues, out_queues):\n",
    "            for schedule in DetermisticScheduler().generate(n_batches, n_partitions):\n",
    "                self.compute(schedule, in_queues, out_queues)\n",
    "    \n",
    "    def compute(self, schedule, in_queues, out_queues):\n",
    "        for microbatch_idx, partition_idx in schedule:\n",
    "            batch = self.batches[microbatch_idx]\n",
    "            partition = self.partitions[partition_idx]\n",
    "            \n",
    "            def wrapper_func():\n",
    "                def func():\n",
    "                    return partition[partition_idx]\n",
    "            \n",
    "            in_queues[partition_idx].put(wrapper_func)\n",
    "        \n",
    "        for microbatch_idx, partition_idx in schedule:\n",
    "            output = out_queues[partition_idx].get()\n",
    "            self.batches[microbatch_idx] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdd768-cd85-46bb-88d9-5fad7df776bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "int zero() {\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771debc-68fb-4062-97cf-d46fae27f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool watcher, job selector, worker threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2193b-9a7a-4421-bfef-b51dbebdb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared memory, file system, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc661c-c58e-4270-a798-7ca2553b95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.repeat((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df701f-34be-43ca-9bab-c016d835469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: fp32 and fp16 of the weights\n",
    "step 2: do forward and backward pass using fp16\n",
    "step 3: cast fp16 to fp32\n",
    "step 4: update us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5ae03-bc93-4047-9a15-aec121ab7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast, scatter, reduce, gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3f397b-8c7e-4c70-9da4-6be961ead66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e2b0a2-a4c0-445a-abe6-fc9965d9abe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794bbcf7-99ba-4a4a-875b-4c7078a4b9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    with lock:\n",
    "        increment_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c992f2-a825-447b-b875-2642ab1fe8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    t = threading.Thread(target=run)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536d69e-dff8-41a9-9aa2-752bda64a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "{x: v**2 for x, v in simple_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8b5a22-b2ee-496a-a663-025afb9f68ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3984a63-8e73-4b3a-93ae-7cc8c9667f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rearrange(images, \"b c h w -> b c w h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e53a56-778c-4369-b69a-37757467ec5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef384d-0d3c-4ad5-aed7-5389bb683fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1530ec4-92bf-4376-b4eb-b366ea49cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = corrupted_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89f624-22f3-46e8-a57f-1f8f98ab4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(n_layers, batch_size, seq_len, n_heads, d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fde0d575-86f1-4594-8b1d-69e70c2f6c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1d75a1-60cb-4208-b4ea-c5dbad2dca44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac7caa-415a-4d13-b0c1-9a7609b8f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    for head_idx in range(n_heads):\n",
    "        for sample_idxs in corrupted_groups:\n",
    "            template_output = get_act_name(\"z\", layer_idx)[:, sample_idxs]\n",
    "            mean = reduce(\n",
    "                template_output,\n",
    "                \"bs seq_len n_heads d_head -> bs n_heads d_head\",\n",
    "                reduction=\"mean \"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f501c98-ad80-4bbb-a945-814ef04a3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7481a25e-4e1e-45e0-90bf-dd7f599dba53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 9, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b46b3-5dc5-495c-b61b-ddc98cf40889",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"z\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86025716-fb0d-4619-b13d-f50bcd67c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_output = cache[hook_name][:, head_idx] @ model.W_O[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953574da-bb83-4507-80f3-b0efa9d76519",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_dir = model.W_U[:, io_tokens]\n",
    "s_dir = model.W_U[:, s_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935b740-555e-41c9-a4fc-6668e98298e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_in_io_dir = (head_output @ io_dir).sum()\n",
    "projection_in_s_dir = (head_output @ io_dir).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090caddf-b491-49f7-9bfe-13a250749460",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = cache[get_act_name(\"pattern\", layer_idx)][:, :, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b92024-f910-4326-8c25-b161696af9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_from_end_to_io = pattern[:, end_idxs, io_idxs]\n",
    "attn_from_end_to_s = pattern[:, end_idxs, s_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b6611-d8b5-40ea-9eda-07068d17afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E @ W_OV^{0, 7} @ W_QK^{T} @ W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6815b3c-d371-4918-84d4-a1000f59235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f9cd1d-2cf7-4a06-bc5f-76c5250732a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c824327-a40f-42e5-8b21-e967f56b2b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"post\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853e02-3074-4e4d-8c97-d9bf970a7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in_acts = cache[hook_name]\n",
    "output = W_in_acts @ model.W_out[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a610c785-873c-4bb5-bc1e-d32c95ac90bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "receiver_heads = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "receiver_layers = [7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f57a85e2-5c65-436a-8b14-923943fdde17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_sender_head_output(acts, hook, clean_cache, corrupted_cache, target_head):\n",
    "    target_layer_idx, target_head_idx = target_head\n",
    "    if hook.layer() == target_layer_idx:\n",
    "        acts[:, head_idx] = corrupted_cache[hook.name][:, head_idx]\n",
    "    else:\n",
    "        acts = clean_cache[hook.name]\n",
    "    \n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1231ea4-090b-4c63-9a28-70842a36ba3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_receiver_head_input(acts, hook, patched_cache, receiver_heads):\n",
    "    head_idxs = [head_idx for layer_idx, head_idx in receiver_heads if layer_idx == hook.layer()]\n",
    "    acts[:, :, head_idxs] = patched_cache[hook.name][:, :, head_idxs]\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0000249-8db8-4d24-b4f1-25ee877dc965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dde6c6-7567-4619-afbf-73d9099e7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_heads = list(product(\n",
    "    range(max(receiver_layers)),\n",
    "    range(n_heads)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5f102-8347-45af-9c27-e28c2626fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "827f63b4-0d9e-489c-8208-d53ef0c07338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac9628-4eec-4c4f-ab15-2c8d86153531",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.zeros(max(receiver_layers), n_heads)\n",
    "\n",
    "for layer_idx, head_idx in sender_heads:\n",
    "    model.reset_hooks()\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_sender_head_output,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache=corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    model.add_hook(hook_name, hook_func)\n",
    "    _, patched_cache = model.run_with_cache(clean_tokens)\n",
    "    \n",
    "    hook_name = get_act_name(\"v\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_receiver_head_input,\n",
    "        patched_cache=patched_cache,\n",
    "        receiver_heads=receiver_heads\n",
    "    )\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        clean_tokens,\n",
    "        fwd_hooks=[(hook_name, hook_func)]\n",
    "    )\n",
    "    results[layer_idx, head_idx] = compute_ioi_metric(patched_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c7df9-8e8b-465b-97c2-da9a60cefb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42244826-2b59-4736-8782-ab509af01fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 0, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd80d01c-823d-4562-9931-06e6eb741bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"z\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc1171-e0a6-4e6a-9a00-e9da81ff628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_output = cache[hook_name][:, :, head_idx] @ model.W_O[layer_idx, head_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10338cd8-02b2-477e-a2f8-6d8fec042055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit0 = final_resid @ W_U[0]\n",
    "logit1 = final_resid @ W_U[1]\n",
    "\n",
    "\n",
    "logit0 - logit1 = final_resid @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17368f7c-1af1-41ae-a461-b15332d4a980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0bf3b-c0c8-4af9-ba1d-e1478ade9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "space, time, matter, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837264e-4e46-4321-9a62-deb9804d0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "record, send, memory, process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd360ad-71fa-41ec-8a47-b6ad4c65c418",
   "metadata": {},
   "source": [
    "Here is the mathematical proof written in LaTeX:\n",
    "\n",
    "Let's say we have a dataset $\\mathcal{X}$ split into $m$ subsets ${\\mathcal{X}_1, \\mathcal{X}_2, ..., \\mathcal{X}_m}$, with each subset going to one model replica.\n",
    "\n",
    "The loss function is $L(w; \\mathcal{X}, y)$ where $w$ are the model parameters.\n",
    "\n",
    "The gradient on the full dataset is:\n",
    "\n",
    "$\\nabla_w L(w; \\mathcal{X}, y) $\n",
    "\n",
    "In distributed data parallel training, each replica $r$ computes:\n",
    "\n",
    "$\\nabla_w L_r(w; \\mathcal{X}_r, y_r)$\n",
    "\n",
    "Then gradients are averaged across replicas:\n",
    "\n",
    "$\\nabla_w \\bar{L}(w) = \\frac{1}{m} \\sum_{r=1}^m \\nabla_w L_r(w; \\mathcal{X}_r, y_r)$\n",
    "\n",
    "Plugging in $L(w; \\mathcal{X}, y) = \\sum_{r=1}^m L_r(w; \\mathcal{X}_r, y_r)$ since $\\mathcal{X}$ is split into $m$ subsets:\n",
    "\n",
    "$\\nabla_w \\bar{L}(w) = \\frac{1}{m} \\sum_{r=1}^m \\nabla_w L_r(w; \\mathcal{X}r, y_r)$\n",
    "$= \\frac{1}{m} \\nabla_w \\sum{r=1}^m L_r(w; \\mathcal{X}_r, y_r)$\n",
    "$= \\nabla_w L(w; \\mathcal{X}, y)$\n",
    "\n",
    "Therefore, the averaged gradient $\\nabla_w \\bar{L}(w)$ equals the gradient on the full dataset $\\nabla_w L(w; \\mathcal{X}, y)$.\n",
    "\n",
    "This shows that communicating and averaging gradients makes each replica see the same gradient as training on the full data, proving the mathematical equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c376a-09d6-4456-a389-3888d667330e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
