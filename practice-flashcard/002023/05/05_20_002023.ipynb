{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98bff60-1422-4ddc-b097-dd624313da58",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5355f-2227-4130-87f8-871892171c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e553fb5-56c4-4f06-bce7-e3051296e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class f(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        torch.distributed.all_reduce(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcb95f-3514-43d8-9985-e1b10e01c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        input_list = [torch.empty_like(input) for _ in range(world_size)]\n",
    "        torch.all_gather(input_list, input)\n",
    "        input_list = torch.cat(input_list, dim=-1)\n",
    "        return input_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        rank = torch.distributed.get_rank()\n",
    "        \n",
    "        dim_size = grad_output.shape[-1]\n",
    "        dim_size_per_partrition = dim_size // world_size\n",
    "        grad_chunks = torch.split(\n",
    "            grad_output,\n",
    "            split_size_or_sections=dim_size_per_partrition,\n",
    "            dim=-1\n",
    "        )\n",
    "        return grad_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e4e92-e945-4557-925c-db8f1a389626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnLinearParallel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        self.input_size = input_size\n",
    "        self.output_size_per_partrition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition,\n",
    "            self.input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = f.apply(input)\n",
    "        output_parallel = F.linear(\n",
    "            input_parallel,\n",
    "            self.weight,\n",
    "            self.bias\n",
    "        )\n",
    "        outputs = g.apply(input_parallel)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a8e49-47b2-4136-90c7-f289bad267a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dab92-031c-43ab-ab18-f529af602869",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tensor_model_parallel_groups = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f07e4d-919f-4f87-96cb-7bdbb27ca9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[4, 5]\n",
      "[6, 7]\n",
      "[8, 9]\n",
      "[10, 11]\n",
      "[12, 13]\n",
      "[14, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_tensor_model_parallel_groups):\n",
    "    ranks = list(range(\n",
    "        i*tensor_model_parallel_size,\n",
    "        (i+1)*tensor_model_parallel_size\n",
    "    ))\n",
    "    \n",
    "    print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be126c66-27c8-4c28-9528-bc102910be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ea276-dc5e-44c7-ae94-a4c5786c0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tensor_model_parallel_size,\n",
    "        master_addr,\n",
    "        master_port,\n",
    "        backend\n",
    "    ):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            self._initialize_distributed(\n",
    "                master_addr=master_addr,\n",
    "                master_port=master_port,\n",
    "                backend=backend\n",
    "            )\n",
    "        \n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        tensor_model_parallel_groups = world_size // tensor_model_parallel_size\n",
    "        \n",
    "    \n",
    "    def _initialize_distributed(self):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            \n",
    "            torch.distributed.new_process_group(\n",
    "                backend=backend\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32932bd7-4544-4249-bc05-ab85822e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grad_enabled(inputs):\n",
    "    return torch.is_grad_enabled() and inputs.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d41cb-e685-403e-849f-fd8337b0ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _broadcast(inputs):\n",
    "    return inputs.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75508fa-5064-404b-9b7a-6cc9e5bd78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        return _broadcast(inputs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        return _reduce(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa447cd-eac0-4442-a493-81096b271963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward(inputs):\n",
    "    if is_grad_enabled(inputs):\n",
    "        outputs = Broadcast.apply(inputs)\n",
    "    else:\n",
    "        outputs = _broadcast(inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b7053-9134-4bec-8e0d-b28edce970c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_memory(memory_size):\n",
    "    FP16_SIZE = 4\n",
    "    n_numbers = memory_size // FP16_SIZE\n",
    "    return torch.empty(FP16_SIZE, type=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad15da-fa22-4e25-be4e-1dfc04659b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro-batch 1 > 2 > ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f790656-c0d7-4e00-8681-0064859e2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2412f44-7cc3-49b4-af55-47d022c197c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPU:\n",
    "    def __init__(self, master_addr, master_port, backend):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            self._initialize_distributed(\n",
    "                master_addr,\n",
    "                master_port,\n",
    "                backend\n",
    "            )\n",
    "            \n",
    "    def process_to_gpu(self, rank):\n",
    "        n_devices = torch.cuda.device_count()\n",
    "        \n",
    "        if n_devices > 0:\n",
    "            torch.cuda.set_device(rank % n_devices)\n",
    "    \n",
    "    def _initialize_distributed(self, master_addr, master_port, backend):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            RANK = int(os.getenv(\"RANK\"))\n",
    "            WORLD_SIZE = os.getenv(\"WORLD_SIZE\")\n",
    "            os.environ[\"MASTER_ADDR\"] = str(master_addr)\n",
    "            os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "            self.process_to_gpu(rank)\n",
    "            \n",
    "            torch.distributed.new_process_group(\n",
    "                rank=rank,\n",
    "                world_size=world_size,\n",
    "                backend=backend\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f2f0b-3873-4de3-a4dd-909323e0a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward pass, backward pass, recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90199166-253c-4cd4-ae1e-af993db8c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0269f9-575e-4146-9a4b-517c8b581799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_times(model, batch):\n",
    "    records = [[] for _ in range(model)]\n",
    "    \n",
    "    for i, layer in enumerate(model):\n",
    "        start_time = time.time()\n",
    "        outputs = [layer(x) for x in batch]\n",
    "        outputs_with_grad = [x for x in outputs if x.requires_grad]\n",
    "        \n",
    "        if outputs_with_grad:\n",
    "            torch.autograd.backward(outputs_with_grad, outputs_with_grad)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        records[i].append(end_time - start_time)\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b271c2-6530-4bef-adcd-83f8c9e8cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class f(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        torch.distributed.all_reduce(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f01125-cf18-4229-a871-9ab7ee9304b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        inputs = [torch.empty_like(input) for _ in range(world_size)]\n",
    "        torch.distributed.all_gather(inputs, input)\n",
    "        inputs = torch.cat(inputs, dim=-1)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        rank = torch.distributed.get_rank()\n",
    "        \n",
    "        dim_size = grad_output.shape[-1]\n",
    "        dim_size_per_partrition = dim_size // world_size\n",
    "        grad_chunks = torch.split(grad_output, dim_size_per_partrition, dim=-1)\n",
    "        return grad_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a23a7-be2f-4acb-a6c7-65cd2eb0c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size_per_partrition = output_size_per_partrition // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition,\n",
    "            self.input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = f.apply(input)\n",
    "        output_parallel = F.linear(input_parallel, self.weight, self.bias)\n",
    "        output = g.apply(output_parallel)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5d2bc-e8b7-4670-8080-0c5217b7db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "partrition j on j device\n",
    "F_{m, n} must be completed before F_{m+1, n}\n",
    "B_{m, n} must be completed before B_{m-1, n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af5924-8eff-4c96-8be4-8fdb6fd8b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grad_enabled(inputs):\n",
    "    return torch.is_grad_enabled() and inputs.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55eb4a-8857-4719-b232-f6b9bd719a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _broadcast(inputs):\n",
    "    return inputs.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cbe87-67f3-445d-b4c0-0991ea34111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce(inputs):\n",
    "    torch.distributed.all_reduce(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404784f-cffe-4292-9c72-72d42f5eb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        return _broadcast(inputs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        return _reduce(grad_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3364ca-14ae-40fa-a4af-289afebbcbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward(inputs):\n",
    "    if is_grad_enabled(inputs):\n",
    "        outputs = Broadcast.apply(inputs)\n",
    "    else:\n",
    "        outputs = _broadcast(inputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1dd86-a504-4042-b3f0-5b9421a08f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new > ready >running > blocked > terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e86962-646e-4448-8e51-25cc40642d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = torch.distributed.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce39e74-1601-4e2f-aa1a-b5aadde13a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 69:\n",
    "    torch.distributed.isend(x, dst=42)\n",
    "elif rank == 42:\n",
    "    torch.distributed.irecv(tensor_will_be_received_data, src=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5d8b4-8486-4aeb-b817-6da2980dc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tensor_model_parallel_groups):\n",
    "    ranks = list(range(\n",
    "        i*tensor_model_parallel_size,\n",
    "        (i+1)*tensor_model_parallel_size\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc92038-fb42-4859-9bce-445c51e9a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for in in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e19f8e-ba1a-403e-abd5-6038f9790d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: local variable\n",
    "step 2: communication\n",
    "step 3: set device\n",
    "step 4: initialize parallel groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0609f-81af-4bf7-90a1-d95aaa195d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "- clock cycle 1: F_{1, 1}\n",
    "- clock cycle 2: F_{1, 2}, F_{2, 1}\n",
    "- clock cycle 3: F_{1, 3}, F_{2, 2}, F_{3, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660459b-e1d7-45cd-a3a3-9b770c0cc1f8",
   "metadata": {},
   "source": [
    "### ML Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86954f17-2364-4d37-b39e-e658148d15f7",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f03e3-0db4-473c-b98b-254b6a4b52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63808a4d-1f01-4ca9-892a-6f53bfb381f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b00ba1-b251-4f83-8db6-8b56506f7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3b0df-f665-405c-8041-a09905c893ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0383f2-0916-414f-922d-d7ca624f43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neuron_activation(activations, hook):\n",
    "    data[neuron_idx] = activations[batch_idx, :, neuron_idx]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebee7d-5647-48cc-80ef-0ae4e01fd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429311ce-a6c4-4234-ae5f-5d40c75519c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1b425-54d3-4d6d-9854-217b9d4ebee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_with_hooks(\n",
    "    tokens,\n",
    "    fwd_hooks=[(hook_name, extract_neuron_activation)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db305732-7350-454c-9ad5-eb26825e47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_with_highest_activation = torch.argmax(data[neuron_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0c9f4-d388-41b7-86cd-0983eb52deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1983f-fdc6-4680-90de-dc01a810ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c8f3e-857d-4f06-982a-4452a27d9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pattern = torch.zeros(seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f1182-4a08-4a3a-b9f3-a9dbce0dba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pattern[torch.arange(seq_len), torch.arange(seq_len)] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4ec1d-a169-4863-a2e1-d55b88ef10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pattern[0] = torch.zeros_like(target_pattern[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a125-3f8c-410a-a3bb-9f2ea40036f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(attention_pattern):\n",
    "    score = attention_pattern * target_pattern\n",
    "    return score.sum() / attention_pattern.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a10de-b19a-4ecf-a70a-32f6dce674dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727cfd8-fc71-4304-92b9-7cda15b3fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab4916-291a-4a30-82e2-defb2323adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(n_layers, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a63dd9-3959-4688-b3ee-9d3c9f8cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    hook_name = f\"blocks.{layer_idx}.attn.hook_pattern\"\n",
    "    layer_attention_pattern = cache[hook_name]\n",
    "    for head_idx in range(n_heads):\n",
    "        attention_pattern = layer_attention_pattern[0, head_idx, :]\n",
    "        score = compute_score(attention_pattern)\n",
    "        data[layer_idx][head_idx] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d2a45-c876-4557-8410-2931aaff7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_token = model.to_single_token(\" John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf42b4-d84a-4ef8-aa87-aed4184d0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_token = model.to_single_token(\" Mary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5ae45-b1d5-4851-a4fe-11601c4ac71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd4020-3710-4427-8823-a66e2eb49fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de0bf9-af2c-4c47-986d-53d8ca28aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_activations = model.run_with_cache(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b0e9b-ce8c-4e4a-ba88-d61421bbc008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_residual(\n",
    "    corrupted_activations,\n",
    "    hook,\n",
    "    position,\n",
    "    clean_activations\n",
    "):\n",
    "    corrupted_activations[:, position, :] = clean_activations[hook.name][:, position, :]\n",
    "    return corrupted_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e06a41-c8a4-44e3-b3a0-b2a31e5ed592",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02da70-a316-4fe7-8f6d-f79811e55d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba3a49-d8c5-4aea-bba7-dc601b3a676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d07990-0a15-49d8-a1d1-53d3f7a992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_diff(logits, correct_token, incorrect_token):\n",
    "    correct_logits = logits[:, correct_logit, :]\n",
    "    incorrect_logits = logits[:, incorrect_token, :]\n",
    "    return correct_logits - incorrect_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2e2c3-6b10-4705-93cb-7e479b1a402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(n_layers):\n",
    "    for position in range(n_tokens):\n",
    "        hook_func = partial(patch_residual, position=position, clean_activations)\n",
    "        hook_name = utils.get_act_name(\"resid_pre\", layer)\n",
    "        \n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(hook_name, hook_func)]\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b93a-ce13-4b1b-8021-a190664a315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: tokenize the pormpt\n",
    "step 2: tokenize observation, append to the prompt\n",
    "step 3: predict\n",
    "step 4: exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72bef0-b8d0-42d4-bf54-f138b3884caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: tokenize the pormpt\n",
    "step 2: tokenize observation, append to the prompt\n",
    "step 3: predict\n",
    "step 4: exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f297d-78e9-44cd-a83f-8bf4677bbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3e68c-4748-4c28-98bc-460980cec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79102465-9d5f-4d28-9177-4d4f44d10618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        k = k.transpose(3, 2)\n",
    "        qk = torch.matmul(q, k)\n",
    "        scores = qk / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask != None:\n",
    "            scores.fill_mask(mask == 0, 1e-9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19c923-d4b8-444f-b82c-cc0110b0808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.repeat((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4d6da-5e67-4fbc-9811-2d49428f32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.roll(x, shifts=1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29ade1-5658-41ba-aa3b-e5c3d8517a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e60f3-65f6-45ce-b5ee-3489ca225fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc.remote(\"worker_1\", create_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24688c-def8-48e1-90ff-fbd4b6b83dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f23b0e-a8bd-4370-9f79-02127b3bbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d2ed-6f38-4a02-91ef-bf8e51947e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(tokens)\n",
    "corrupted_tokens = model.to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab37b0-cf26-413a-b051-9fb0330d4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_token = model.to_single_token(\" John\")\n",
    "incorrect_token = model.to_single_token(\" Mary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb61489-bf4e-4eb1-b670-6c6072cfa2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_activations = model.run_with_cache(\n",
    "    clean_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aaeb25-16b8-4c70-9870-da363c025efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c0a82-9e7d-4d21-b64d-ab87b9692825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(n_layers, n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae014d4-e1b1-46b4-89a6-983db619aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ecc1c-a7f5-4ded-9157-361c732da406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_component(\n",
    "    corrupted_activations,\n",
    "    hook,\n",
    "    position,\n",
    "    clean_activations\n",
    "):\n",
    "    corrupted_activations[:, position, :] = clean_activations[hook.name][:, position, :]\n",
    "    return corrupted_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd6582-43a9-46d1-bd8b-77292a697ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff(logits, correct_token, incorrect_token):\n",
    "    correct_logits = logits[:, correct_token, :]\n",
    "    incorrect_logits = logits[:, incorrect_token, :]\n",
    "    return correct_logits - incorrect_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221462f4-b93a-4669-881c-f5a85571b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(layer_idx):\n",
    "    for position in range(n_tokens):\n",
    "        hook_func = partial(\n",
    "            patch_component,\n",
    "            position=position,\n",
    "            clean_activations=clean_activations\n",
    "        )\n",
    "        hook_name = get_act_name(\"resid_pre\", layer_idx)\n",
    "        patched_logits = model.run_with_hook(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(hook_name, hook_func)]\n",
    "        )\n",
    "        logit_diff = logit_diff(patched_logits, correct_token, incorrect_token)\n",
    "        data[layer_idx][position] = logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a15c24-ad38-41d3-9984-15a02ffa60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_preds = sm_pred[torch.arange(y_train.shape[0]), y_train]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
