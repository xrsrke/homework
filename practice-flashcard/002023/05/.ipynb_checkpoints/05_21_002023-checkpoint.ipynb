{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8247c8-d5a9-469b-adc3-79900b3da85b",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af4a27-d7b5-46ed-9a0d-8834b65f54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9c2ec-f192-47e4-ba3c-98e2ead2395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ab5c5-dd00-4a44-8b13-da782b263c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d140f-b83e-486b-be7e-9aabc430986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_time = time.time()\n",
    "\n",
    "with torch.cuda.stream(stream1):\n",
    "    operation_a()\n",
    "\n",
    "with torch.cuda.stream(stream2):\n",
    "    operation_b()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be4ea7-5091-4747-871f-b3c1d33e49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 16\n",
    "num_gpus = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af5262-e222-4f10-ae9e-65ca0ed9f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 0 -> gpu: 0\n",
      "rank: 1 -> gpu: 1\n",
      "rank: 2 -> gpu: 2\n",
      "rank: 3 -> gpu: 3\n",
      "rank: 4 -> gpu: 0\n",
      "rank: 5 -> gpu: 1\n",
      "rank: 6 -> gpu: 2\n",
      "rank: 7 -> gpu: 3\n",
      "rank: 8 -> gpu: 0\n",
      "rank: 9 -> gpu: 1\n",
      "rank: 10 -> gpu: 2\n",
      "rank: 11 -> gpu: 3\n",
      "rank: 12 -> gpu: 0\n",
      "rank: 13 -> gpu: 1\n",
      "rank: 14 -> gpu: 2\n",
      "rank: 15 -> gpu: 3\n"
     ]
    }
   ],
   "source": [
    "for rank in range(world_size):\n",
    "    print(f\"rank: {rank} -> gpu: {rank % num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc5afa-38a6-4c58-aaa1-32c300288637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class f(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        torch.distributed.all_reduce(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5da447-f3b5-40b6-93f3-581aa5829010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        rank = torch.distributed.get_rank()\n",
    "        \n",
    "        inputs = [torch.zeros_like(input) for _ in range(world_size)]\n",
    "        torch.distributed.all_gather(inputs)\n",
    "        inputs = torch.cat(inputs, dim=-1)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        rank = torch.distributed.get_rank()\n",
    "        \n",
    "        dim_size = grad_output.shape[-1]\n",
    "        dim_size_per_partrition = dim_size // world_size\n",
    "        grad_chunks = torch.split(grad_output, dim_size_per_partrition, dim=-1)\n",
    "        return grad_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95224952-4fab-4737-b3b9-f751b747cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(torch.autograd.Function):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size_per_partrition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition,\n",
    "            self.input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = f.apply(input)\n",
    "        output_parallel = F.linear(input_parallel, self.weight, self.bias)\n",
    "        outputs = g.apply(output_parallel)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9454-27aa-4dd5-835f-85be20978fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine the location of activation checkpoints\n",
    "step 2: compute the activations, save the activation checkpoints\n",
    "step 3: compute the gradient of the last part\n",
    "step 4: recompute the forward if need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef89122-387d-4ee4-9798-20b88b00d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer-related variables, parameters, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34040f8d-f96d-40c8-8a18-81f37bea9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890dcba-bf73-4823-909d-6be5079267c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPU:\n",
    "    def __init__(self, master_addr, master_port, backend):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            self.initialize_distributed(master_addr, master_port, backend)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def process_to_gpu(self, rank):\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        if num_devices > 0:\n",
    "            device = rank % num_devices\n",
    "            torch.cuda.set_device(device)\n",
    "    \n",
    "    def initialize_distributed(self, master_addr, master_port, backend):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            rank = int(os.getenv[\"RANK\"])\n",
    "            world_size = int(os.getenv[\"WORLD_SIZE\"])\n",
    "            os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "            os.environ[\"MASTER_PORT\"] = master_port\n",
    "            \n",
    "            self.process_to_gpu(rank)\n",
    "            \n",
    "            torch.distributed.new_process_group(\n",
    "                rank=rank,\n",
    "                world_size=world_size,\n",
    "                backend\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265f8f0-a80e-473c-85ad-4f8613d6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _broadcast(input):\n",
    "    return input.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c235081-7de0-446d-aa96-081e3ad8b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scatter(input):\n",
    "    world_size = torch.distributed.get_world_size(group=parallel_group)\n",
    "    rank = torch.distributed.get_rank(group=parallel_group)\n",
    "    \n",
    "    dim_size = input.shape[-1]\n",
    "    dim_size_per_partrition = dim_size // world_size\n",
    "    \n",
    "    input_chunks = torch.split(input, dim_size_per_partrition, dim=-1)\n",
    "    \n",
    "    return input_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ea07a-5f26-42f5-86d2-3b798ed613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return _broadcast(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return _scatter(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295cf5c-59fe-4e28-95ff-9baa9d3a3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward(inputs):\n",
    "    if is_grad_enabled(inputs):\n",
    "        outputs = Broadcast.apply(inputs)\n",
    "    else:\n",
    "        outputs = _broadcast.apply(inputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39798c8-6e16-429d-b45c-5a4876ab9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: sender sends\n",
    "step 2: sender continue its execution\n",
    "step 3: receiver stop and wait\n",
    "step 4: once the receiver received, it continues its execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d201a32-85bb-4cb2-86e6-8c60cf0e8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.daa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb25c2-8e90-4445-a85b-2648039092a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: load the training data to ram\n",
    "step 2: measure the size of the next\n",
    "step 3: allocate a portion in memory\n",
    "step 4: load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f13c91-2a5e-495e-8eca-03c7dfda5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tensor_model_parallel_groups = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d727dd9-4b9b-4da9-84da-bf60e7a86137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0274a-fd8a-4f7c-8e24-4cd4957dbdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[4, 5]\n",
      "[6, 7]\n",
      "[8, 9]\n",
      "[10, 11]\n",
      "[12, 13]\n",
      "[14, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_tensor_model_parallel_groups):\n",
    "    ranks = list(range(\n",
    "        i*tensor_model_parallel_size,\n",
    "        (i+1)*tensor_model_parallel_size\n",
    "    ))\n",
    "    \n",
    "    print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627a424-0b88-48ca-9c97-c7e35b55ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, forward, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa78997-e86d-4bf4-94c4-a824842e0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "send sync, receiv sync\n",
    "send sync, receive async\n",
    "send async, recev sync\n",
    "send async, recev async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cafd6eb-ca94-4221-b6cd-6683bafac3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _broadcast(input):\n",
    "    return input.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6406bb-396a-4b67-9be3-3b2f5356d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce(grad_output):\n",
    "    world_size = torch.distributed.get_world_size(group=parallel_group)\n",
    "    \n",
    "    if world_size == 1:\n",
    "        return grad_output\n",
    "    \n",
    "    torch.distributed.all_reduce(grad_output, group=parallel_group)\n",
    "    \n",
    "    return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2f464-4b0f-40e2-a1df-b3ccf8204970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return _broadcast(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return _reduce(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b61d61-1d2c-481c-99bb-91750d785944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grad_enable(inputs):\n",
    "    return torch.is_grad_enabled() and inputs.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa7ec8-64de-4a0a-8489-5d2afcda4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_with_forward_and_backward(inputs):\n",
    "    if is_grad_enable(inputs):\n",
    "        outputs = Broadcast.apply(inputs)\n",
    "    else:\n",
    "        outputs = _broadcast(inputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b3bb5-8c56-497f-83e2-a64424e57927",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy loading\n",
    "data prefetching\n",
    "memory mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a61d15-510d-44c1-a943-71cdd64853b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_memory(memory_size):\n",
    "    FP32_SIZE = 4\n",
    "    n_numbers = memory_size // 4\n",
    "    return torch.empty(n_numbers, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c8668-dfb7-46c6-9023-1982d6409981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc7666-0e73-4502-a2d4-f9a421ac7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.cache_index = {}\n",
    "        self.data = None\n",
    "    \n",
    "    def prefetch(self, idxs):\n",
    "        if all([i in self.cache_index for i in idxs]):\n",
    "            return\n",
    "        \n",
    "        if not self.data:\n",
    "            self.data = torch.load(self.filename)\n",
    "        \n",
    "        n_elements = [x.numel() for x in self.data]\n",
    "        self.cache = torch.empty(n_elements, dtype=self.data.dtype)\n",
    "        \n",
    "        self.cache_index = {}\n",
    "        offset = 0\n",
    "        \n",
    "        for i in idxs:\n",
    "            n = self.data[i].numel()\n",
    "            self.cache[offset:offset+n] = self.data[i]\n",
    "            self.cache_index[i] = offset\n",
    "            offset += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dd3f7-b8c7-4082-b13d-850d4222d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file system, shared memory, message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f380368-4dfc-4b00-86bf-8b0057de31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 16\n",
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83dbb1-d8ad-4606-8b73-d571a9fd37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_paralell_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af697a77-49ef-4a45-a501-ec86e661e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = world_size // tensor_model_parallel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8c613-c1e3-425d-89cd-51c5cb957412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tensor_model_parallel_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19570a2e-1290-440a-a889-9730e0318590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6]\n",
      "[1, 3, 5, 7]\n",
      "[8, 10, 12, 14]\n",
      "[9, 11, 13, 15]\n",
      "[16, 18, 20, 22]\n",
      "[17, 19, 21, 23]\n",
      "[24, 26, 28, 30]\n",
      "[25, 27, 29, 31]\n"
     ]
    }
   ],
   "source": [
    "for stage_idx in range(pipeline_model_paralell_size):\n",
    "    start_rank = stage_idx*num_pipeline_model_parallel_groups\n",
    "    end_rank = (stage_idx+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for i in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(\n",
    "            start_rank+i,\n",
    "            end_rank,\n",
    "            tensor_model_parallel_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4b89c-5948-43d0-b87e-0d0785a1d785",
   "metadata": {},
   "source": [
    "### ML Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c11e1-4bfa-4ff7-ab90-1dace29785e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import FlowSpec, step, schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b90115-42ff-4377-9108-82b35af6b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schedule(daily=True)\n",
    "class HelloFlow(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.fuck)\n",
    "    \n",
    "    @step\n",
    "    def fuck(self):\n",
    "        print(69)\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46b461-ba8d-4c2d-b593-4879ccd66ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee820482-12cb-4da7-902f-0252e6591682",
   "metadata": {},
   "outputs": [],
   "source": [
    "@project(name=\"project_69\")\n",
    "class TrainFlow(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.train)\n",
    "    \n",
    "    @step\n",
    "    def train(self):\n",
    "        train()\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b72176-1a96-4459-bc8d-539c53d51316",
   "metadata": {},
   "outputs": [],
   "source": [
    "@project(name=\"project_69\")\n",
    "class EvaluateFlow(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.train)\n",
    "    \n",
    "    @step\n",
    "    def train(self):\n",
    "        evaluate()\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b50a74-ff79-45b2-bdb6-249209998b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingFlow(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.train, self.eval)\n",
    "    \n",
    "    @step\n",
    "    def train(self):\n",
    "        print(\"training...\")\n",
    "        self.next(self.join)\n",
    "    \n",
    "    @step\n",
    "    def eval(self):\n",
    "        print(\"evaluating...\")\n",
    "        self.next(self.join)\n",
    "    \n",
    "    @step\n",
    "    def join(self, inputs):\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336cb98-a6ac-4ab5-94c2-56394b90f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --net mongo-network mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c03b2b-70ac-47ff-a528-f512a9cba90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker pull redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3803c-eaa4-4f49-af5d-920aeb9229ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: nat gate way to vpc\n",
    "step 2, 3, 4: create, route table to nat gate way, attach\n",
    "step 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18a3a5-0cbe-4967-a212-20f7b6f460ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "public ip, private ip, elastic ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50550feb-beae-4ce7-94b4-9af3c87e1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab792ef6-1e63-4b43-98e1-1014d7bdcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "in-server\n",
    "as-a-service\n",
    "edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4aeec-9d99-4580-90ae-6bbaea607530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import overload, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d0f661-3489-493c-b0a1-7c69c5c74e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload\n",
    "def getitem(x: str) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c06b3-bb11-4bf4-adaa-067769a1c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload\n",
    "def getitem(x: List[int]) -> int:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5873359-04d5-4bb3-a00f-46c74f046a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93412d77-793c-48ed-ba36-6e3d30ccc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo() -> Callable[[int, int], int]:\n",
    "    def add(x: int, y: int) -> int:\n",
    "        return x+y\n",
    "    return add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0745d7-55fc-4d3a-83aa-9974a262cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "region > vpc > availability zone > subnet > resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30c48a-16cb-40c2-ba57-9faf1cdc715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker rmi redis:22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96838b53-e194-497b-a31d-a4e995833486",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker network create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0b616-39ba-46d3-9399-e8e3b9981059",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker rm 23a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a24f8-ff2a-4f97-9f41-c134b23ce52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c1dd1-bafe-4d28-a884-e21f452fce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run postgres:4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443deff-0a57-417d-8e6a-c6d825ae79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker stop 12b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa111b-7a45-4287-95d8-145ec3c0a943",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c5265-5fb0-4bd1-8fc6-81e5818adb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d69d5e-3388-433e-b8f6-0e00e99bcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c765e4f-71a5-432e-84b5-23a505bc4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.stream(stream1):\n",
    "    x_mean = x.mean(dim=-1)\n",
    "\n",
    "with torch.cuda.stream(stream1):\n",
    "    y_mean = y.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319421b1-4689-4aad-8082-7597e941a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db390e9-d35f-4233-b8a6-5c762746344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOptimizer(Optimizer):\n",
    "    def __init__(self, params):\n",
    "        defaults = dict()\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    def step(self):\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group[\"params\"]:\n",
    "                if param.requires_grad is False:\n",
    "                    continue\n",
    "                \n",
    "                param.data = param.data * 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f8b11-bd99-4063-b5a5-8aff76e91b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752e9d1-745f-4c0c-b025-8c98573cb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439eb0d8-3026-48fd-96d9-61f50f5241df",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_tokens = model.to_tokens(clean_prompt)\n",
    "incorrect_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca341b9-8344-4ef0-b08d-d45bc4ed895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_token = model.to_single_token(\" John\")\n",
    "incorrect_token = model.to_single_token(\" Mary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c4302-5fb9-49e2-8727-28a962578531",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_logits = model.run_with_cache(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54054f-2264-4bdf-82bc-3a945944c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = clean_tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8a996-8113-4688-a211-0d671ef6976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(n_layers, n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b32177-213e-4841-8a04-39721ef35553",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b310676-06fd-44aa-9497-8ebb1a1c18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_component(\n",
    "    corrupted_activations,\n",
    "    hook,\n",
    "    position,\n",
    "    clean_activations\n",
    "):\n",
    "    corrupted_activations[batch_idx, position, :] = clean_activations[hook.name][batch_idx, position, :]\n",
    "    return corrupted_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19348d3-9161-4dfb-a62e-a1e422e0122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2adc4-f5b9-494f-bd0b-c82bd425f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312a6ed-f307-45a8-9753-e48cbb1788c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_diff(logits, correct_token, incorrect_token):\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    correct_logit = last_token_logits[:, correct_token]\n",
    "    incorrect_logit = last_token_logits[:, incorrect_token]\n",
    "    return correct_logit - incorrect_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c6bdf-2b69-416e-8958-35400ad1fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    \n",
    "    for position in range(n_tokens):\n",
    "        hook_name = get_act_name(\"resid_pre\", layer_idx)\n",
    "        hook_func = partial(patch_component, position=position, clean_activations=clean_activations)\n",
    "        corrupted_logits = model.run_with_hook(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(hook_name, hook_func)]\n",
    "        )\n",
    "        logit_diff = compute_logit_diff(corrupted_logits, correct_token, incorrect_token)\n",
    "        data[layer_idx][position] = logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63925d-a9c0-47b3-b847-de19e509a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a90083-5b2a-4b61-bbd0-9ab26d4c2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05554308-ec59-4303-afae-1663797144fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba754f1-fdf8-4735-8ba4-aa22d9887d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9a94d-71d6-48e2-b7ec-41c9148f8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_log_prob = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22302549-8b67-4f8b-a117-c9084c39d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_heads = [(6, 9), (4, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5b9e3-4a13-4748-b363-367f7cacfca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(repeated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d092f-23b9-4142-9ddd-030e85d96622",
   "metadata": {},
   "outputs": [],
   "source": [
    ", cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbf0b7-f09a-47bf-8051-35d8347e3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for head_idx, layer_idx in induction_heads:\n",
    "    hook_name = get_act_name(\"attn\", layer_idx)\n",
    "    attention_pattern = cache[hook_name][:, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff8d3b-0327-4d63-beac-cf4d038c7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1159b-ec72-4fd5-aff9-ecaf9ad847ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker network ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb1380-5cae-4e51-bd45-8d4fff9b3bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b9f50-04f1-47ee-9300-d61aacf322dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263e14c-1bbf-402a-a23e-7e147d2ed93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad78ae4-9556-4473-8ed3-7d74fa96c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_probs = torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc39a4-f2a5-4df7-b0e1-7dfca366b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: choose the component\n",
    "step 2: prompt x1 and x2\n",
    "step 3: record the output and activation at C in prompt 1\n",
    "step 4: activation patching\n",
    "step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e291e-0d3e-45ac-8f35-aea82babb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(activations, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    \n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(activations)\n",
    "    \n",
    "    mask = (torch.randn_like(activations) > dropout)\n",
    "    \n",
    "    return activations[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f14a0-7905-4618-bb89-a3170b7f458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    for param in param_groups[\"params\"]:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0b1df-1f14-471f-a576-71c65723894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2c116-8579-4312-970a-a5520594f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ec48c-170f-4b9b-b9be-6a7c58430a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: partrition parameter\n",
    "step 2: assign parameter to a specific ranks\n",
    "step 3: allocate the parameter to device\n",
    "step 4: local optimizer\n",
    "step 5: do local update\n",
    "step 6: broadcast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524d9b0-5d31-4f45-9db1-5b4b60a1231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c4f83-c6e9-4609-9081-79572d1f14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "einops_output = einsum(x, y, \"batch dim, batch dim ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb708e-a8c5-4e9b-910f-c07c6fdf0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac42e17-8a31-4f7c-9796-041b9b13fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1a42e-aae4-4d74-a4a9-160e5900e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_parallel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae22d0-04ba-4607-a72a-de3c4fab4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_parallel_groups = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf1de4-1ee1-45d8-aba7-7c67530ca713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_rank = i*pipeline_model_parallel_groups\n",
    "    end_rank = (i+1)*pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(start_rank+j, end_rank, tensor_model_parallel_size))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e259fdd-ce48-4fbd-99e9-255a8e7f90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: tokenize the prompt\n",
    "step 2: tokenize the observation and append to the prompt\n",
    "step 3: take action\n",
    "step 4: execute the action\n",
    "step 5: repeat step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6662a-02ae-4c8f-b98a-a4449eef6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent representation of the current observation, recurrent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b1056-10c5-4504-8f96-4472df7195c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "impulse\n",
    "\n",
    "momentum: fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab22bc-cdf1-41d9-ac11-551c255573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye > optic ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5336a-5c13-4230-b7dd-ce1f4f26fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c6bfc-32ab-4623-8736-f0739fe6f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf85c30-0390-4ce4-bf9d-cd3191218857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target():\n",
    "    with lock:\n",
    "        increment_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a9265-f665-4282-a277-2c5b8752ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba176395-84cb-40f1-8167-fcd63da0ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea861882-10e0-4dd5-b1ab-b789498f535f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
