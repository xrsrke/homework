{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6295b97-e497-4396-be80-8e0792fee9d8",
   "metadata": {},
   "source": [
    "### ML Engingeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ba810-7840-4be3-a221-f093285adcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34a7bf-82e6-442d-aabe-9b25c52ed31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(\n",
    "    config_path=\"./\",\n",
    "    config_name=\"config.yaml\"\n",
    ")\n",
    "def hello(cfg):\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738c25-52b5-4fc1-8b07-91f90d7d4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d9f61-c609-4119-8f11-40861b0d15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.load(\"./config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2cd04-459f-43a5-9aab-a932eeb4166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import root_validator, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e69907-d599-4a33-9ca6-76a69487a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(BaseModel):\n",
    "    password1: str\n",
    "    password2: str\n",
    "    \n",
    "    @root_validator(pre=True)\n",
    "    def check_password(cls, params):\n",
    "        pd1 = params.get(\"password1\")\n",
    "        pd2 = params.get(\"password2\")\n",
    "        \n",
    "        if pd1 == pd2:\n",
    "            return params\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2311ef-ba8a-44d0-bc1d-b79631c7b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92479ff-5bd1-4076-a076-d07f80946c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c208d0c-5f2d-4e0d-9487-60b737a469b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eec06f-e69b-422a-8995-7bc30a89067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: add to dvc\n",
    "step 2: git ignore\n",
    "step 3: push dvc\n",
    "step 4: add metadata to git\n",
    "step 5: git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4455427-c3f4-4125-8de8-a4a6f0005861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f422d6-5a1e-4e97-969d-e5f2efde318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/g3nr86yn4j71vzzmv6knwdhr0000gp/T/ipykernel_1398/584366940.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(\n",
    "    config_path=\"./\",\n",
    "    config_name=\"config.yaml\"\n",
    ")\n",
    "def hello(cfg):\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c48bd-9469-4e1b-afc2-2622dbcc15aa",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6e547-8847-4e1f-8d19-1c062ea426f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: create value on cpu\n",
    "step 2: memory reservation on gpu\n",
    "step 3: move to gpu\n",
    "step 4: determine\n",
    "step 5: launch kernel\n",
    "step 6: do\n",
    "step 7: copy results back to cpu\n",
    "step 8: memory dellocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b98ea6-d451-4287-926a-e64fb766e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0699df2-2e91-464f-b212-e1558a5fd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class f(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        torch.distributed.all_reduce(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123d1f4-bd0b-4702-9b93-355b6173ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        inputs = [torch.zeros_like(input) for _ in range(world_size)]\n",
    "        torch.distributed.all_gather(inputs, dim=-1)\n",
    "        inputs = torch.cat(inputs, dim=-1)\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        rank = torch.distributed.get_rank()\n",
    "        \n",
    "        dim_size = input.shape[-1]\n",
    "        dim_size_per_partrition = dim_size // world_size\n",
    "        \n",
    "        grad_chunks = torch.split(grad_output, dim_size_per_partrition)\n",
    "        return grad_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49564fc-2090-42c6-809e-e6c7118aef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size_per_partrition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition,\n",
    "            self.input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.empty(\n",
    "            self.output_size_per_partrition\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = f.apply(input)\n",
    "        output_parallel = F.linear(input_parallel, self.weight, self.bias)\n",
    "        outputs = g.apply(output_parallel)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f706031-5109-4d65-87ed-e132ed9af2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce33b5-dbf2-4f8c-889c-285831ac786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_times(model, batch):\n",
    "    records = [[] for _ in model]\n",
    "    \n",
    "    for idx, layer in enumerate(model):\n",
    "        start_time = time.time()\n",
    "        outputs = [layer(x) for x in batch]\n",
    "        outputs_with_grad = [x for x in outputs if x.requires_grad]\n",
    "        \n",
    "        if outputs_with_grad:\n",
    "            torch.autograd.backward(outputs_with_grad, outputs_with_grad)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        records[idx].append(end_time-start_time)\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a7652-26fb-4093-9236-30292a0ed6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: partrition\n",
    "step 2: move to rank\n",
    "step 3: move to device\n",
    "step 4: local optimizer\n",
    "step 5: step\n",
    "step 6: broadcast\n",
    "step 7: sync "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ad396-2167-45c9-9553-e21257e1c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "global memory > shared memory > register memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e229cd-1672-4213-b569-3f2785f1607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid > block > threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b33a96-34d3-4f2b-b410-d023f9858e38",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206cc231-727a-4096-8c45-d8ae90b8b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32721c9-6cb9-4a58-9f47-72d111b91055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254f3f3-1bd2-4448-ba88-6214b9535a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOptimizer(Optimizer):\n",
    "    def __init__(self, params):\n",
    "        defaults = dict()\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    def step(self):\n",
    "        for param_group in self.param_groups:\n",
    "            for params in param_group[\"params\"]:\n",
    "                if params.requires_grad:\n",
    "                    params.data = params.data * 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a5e00-48de-4eb6-adbe-803b2f1f1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdd069-7f29-417a-b88d-eb082cb22aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064e319-3338-4b6b-be70-b4350ea4e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd812c6-d57f-4884-b816-27b593280df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0]),\n",
    "            high=np.array([100])\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.temperature -= action\n",
    "        self.shower_length -= 1\n",
    "        \n",
    "        reward = 1 if 37 <= self.temperature <= 39 else 0\n",
    "        done = True if self.shower_length == 0 else False\n",
    "        info = {}\n",
    "        return self.temperature, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.temperature = 20\n",
    "        self.shower_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f06c67-1cd0-48b1-ae3d-8a357caeff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc7346-1cf6-4a64-bcaf-8f8934590378",
   "metadata": {},
   "outputs": [],
   "source": [
    "interference = einsum(\n",
    "    W, W,\n",
    "    \"b f1 d, b f2 d -> b f1 f2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db0aaa-320a-4807-b6eb-e52fc0fc943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transitions(model, env):\n",
    "    transitions = []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while done:\n",
    "        action = model(state)\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        transitions.append(state, action, reward, done)\n",
    "        \n",
    "        new_state = state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
