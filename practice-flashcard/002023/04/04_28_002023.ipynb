{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b1d290-fac6-4c6d-9067-1432500b4a20",
   "metadata": {},
   "source": [
    "### Sci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a4af9-f7ea-48ad-a53d-15b74bc40961",
   "metadata": {},
   "source": [
    "### ML Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3809a-8725-4d20-9be0-9d1285ab25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15275364-39c5-4545-9b5e-4f0d0b0a9af1",
   "metadata": {},
   "source": [
    "select users.name, likes.like FROM users LEFT JOIN likes ON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabaf07-c7c4-4bea-bee1-4838bae1bb1b",
   "metadata": {},
   "source": [
    "SELECT users.name, likes.like FROM users LEFT JOIN likes ON users.id == likes.user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d4bd2-2498-4c96-abf1-dfb811f1627e",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd81c8-7cdc-48c1-b7aa-15c7a39449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e75374-a3e5-438e-96dc-5dc508f290c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.parallel.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8f710-4775-47e1-b258-162ae68eabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f686503-f6d6-4566-9406-a1d718da9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.data.from_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d942e-5fa9-4842-a278-7f6cb57c9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fbca4-a2c8-4d8c-82be-36e45151cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_pass_using_data_parallelism(\n",
    "    model, input, device_ids, output_id\n",
    "):\n",
    "    models = nn.parallel.replicate(model, device_ids)\n",
    "    inputs = nn.parallel.scatter(input, device_ids)\n",
    "    \n",
    "    logit = nn.parallel.parallel_apply(models, inputs)\n",
    "    \n",
    "    logits = nn.parallel.gather(logit, output_ids)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbbc33-a644-49b8-98d3-c84e744ceb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74517e35-3007-4397-ab3b-cab18b2a74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_times(model, batch):\n",
    "    records = [[] for _ in model]\n",
    "    \n",
    "    for i, layer in enumerate(model):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        outputs = layer(batch)\n",
    "        \n",
    "        outputs_grad = [x for x in outputs_grad if x.requires_grad]\n",
    "        \n",
    "        if outputs_grad:\n",
    "            grad = torch.autograd.backward(outputs_grad, outputs_grad)\n",
    "        \n",
    "        end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ee201-4057-44a2-a729-b698b220f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0c9e8-67d2-492d-aa1c-7a880cdc78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e146058-c875-4845-a448-ecf6c1d6af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_memory(model):\n",
    "    total_memory = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        total_memory += param.storage.size() * param.element_size()\n",
    "    \n",
    "    return total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e112457-2c64-4819-9e91-03bee2f8e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ef245-1e5b-458d-9633-1aabdd8fed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist.get_rank() == 0:\n",
    "    dist.send(x, dest=1)\n",
    "elif dist.get_rank() == 1:\n",
    "    dist.recv(tensor_will_be_received_data, src=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d41d03-3a69-49ea-8724-056bed4c950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure the time take\n",
    "determine number of layers for each patrition abased on the elapsed time for each layer\n",
    "split\n",
    "move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3f470-2eb3-4e24-bdb9-11d9264e0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist.get_rank() == 69:\n",
    "    dist.isend(x, dist=42)\n",
    "elif dist.get_rank() == 42:\n",
    "    dist.irecv(tensor_will_be_received_data, src=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e275b-81cd-4849-bf39-6f69a7c04820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033ec8a-1842-43da-9fe7-c68020878126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_module(model, balances, devices):\n",
    "    j = 1\n",
    "    patritions = []\n",
    "    layers = OrderedDict()\n",
    "    \n",
    "    for name, layer in model.named_children():\n",
    "        layers[name] = layer\n",
    "        \n",
    "        if len(layers) == balance[j]:\n",
    "            device = devices[j]\n",
    "            patrition = nn.Sequential(layers)\n",
    "            patrition.to(device)\n",
    "            patrition.append(patritions)\n",
    "            layers.clear()\n",
    "            j += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213babcc-2f01-438a-a130-812123c37913",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p, collective, message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5849e3f-66bd-4d07-9c24-4e63e70f770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast, scatter, reduce, gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585002bc-1bbd-4fc2-9a32-5e76ea5ae7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed013962-68d0-40a2-8e31-070e84a01a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_model(model, balances, devices):\n",
    "    patrition_idx = 0\n",
    "    layers = OrderedDict()\n",
    "    patritions = []\n",
    "    \n",
    "    for name, layer in model.named_children():\n",
    "        layers[name] = layer\n",
    "        \n",
    "        if len(layers) == balances[patrition_idx]:\n",
    "            patrition = nn.Sequential(layers)\n",
    "            device = devices[patrition_idx]\n",
    "            patrition.to(device)\n",
    "            patritions.append(patrition)\n",
    "            layers.clear()\n",
    "        \n",
    "    \n",
    "    return patritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3a0f8-9477-4981-a1a7-6dc30d11f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, i, n, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f043450-09ed-4c81-a7f9-a1564bf764bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.hook_points import HookedRootModule, HookPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a92709-e60d-4f8a-8579-a862b6f3935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(HookedRootModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(42, 69), HookPoint(),\n",
    "            nn.ReLU(), HookPoint(),\n",
    "            nn.Linear(), HookPoint()\n",
    "        )\n",
    "        super().setup()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7abe6-b84a-47b8-b8d1-e8932e159495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ad044-fae9-420d-8b89-042011308e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c094e-2f7c-4ed7-b104-8de1f0beaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(\"pp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d0035-b002-4211-8033-421228be6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu usage in forward pass, model parameters, optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
