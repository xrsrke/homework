{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7c4061-4e0e-40ea-a04c-05170b6c3c16",
   "metadata": {},
   "source": [
    "### Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e3e8a-484f-4591-b346-e8c5e4476fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models import MultitaskClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61510a-b8ce-4950-82e9-a8f8ce6c5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskClassifier(\n",
    "    n_tasks=12,\n",
    "    n_features=1024,\n",
    "    layer_sizes=[1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76b027-1d71-4a0e-9677-b38fd8cd6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio-compatible\n",
    "reliable recording\n",
    "adapt new changes in neural patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320e936-8600-42ad-bb19-36220e5d3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned response, stimulus-evoked, population activity, motor imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7e2b7-2d13-4bc6-8d77-770f14bc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "- step 1: measure neural activity of the subject do different actions\n",
    "- step 2: observe which specific group of neurons are activated during the desire action\n",
    "- step 3: reward the animal when it performs the desired action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99dd7bb-411d-40ed-854c-404b85cad255",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron 1 > axon > synapse > dendrite > neuron 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de035d46-6f59-4419-83e4-665bf5e154d9",
   "metadata": {},
   "source": [
    "step 1: record neural activity while the subject performs different actions\n",
    "step 2: identify the unique pattern of neural activity when the subject performs the target action\n",
    "step 3: reward the subject when the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98f676-cc84-471b-aef0-51bd6a40a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell body, axon, dendrite, synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a83ba2-de07-4e9b-8f7f-1fe7aa69543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: record neural activity of the monkey while it performs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fbfb3-6090-423a-9ecd-6c4f7e056fc1",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa046f4b-24cf-42c4-ae9a-1e2207fd340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c7987-b1e5-444b-b6ce-ecd756971cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Predict the capital of a country\n",
    "Country: {country}\n",
    "Capital:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309af493-3109-466b-a3a8-d8f21068a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"country\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12b58b-0f19-43bf-b73e-94526e35541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d06fd-7548-48db-addf-f5bff7075e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_chain = LLMChain(prompt=prompt, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59aae3-22f2-4c05-be4b-db184436bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82b88b-6bb6-4dcc-9330-e57b9fe80442",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad(input=batch, pad=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0a127-a90d-4923-a751-92f2693b3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model)\n",
    "        self.to_k = nn.Linear(d_model, d_model)\n",
    "        self.to_v = nn.Linear(d_model, d_model)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, self.n_heads, seq_len, self.d_head)\n",
    "    \n",
    "    def concat(self, x):\n",
    "        batch_size, n_heads, seq_len, d_head = x.size()\n",
    "        return x.view(batch_size, seq_len, n_heads*d_head)\n",
    "    \n",
    "    def forward(self, pre_q, pre_k, pre_v):\n",
    "        \n",
    "        q, k, v = self.to_q(pre_q), self.to_k(pre_k), self.to_v(pre_v)\n",
    "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        \n",
    "        attn_output, attn_weights = self.attention(q, k, v)\n",
    "        \n",
    "        output = self.concat(attn_output)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4105a48-1723-41a6-93b8-575857ec6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP, Autoencoder, UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fe7c4-daab-450e-8789-4a9ed7ae7fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff2715-10f9-4bb9-bf8b-8a85495ac5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n, d_model):\n",
    "        super().__init__()\n",
    "        self.n, self.d_model = torch.tensor(n), d_model\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        seq_len = len(tokens)\n",
    "        embeddings = torch.zeros((seq_len, self.d_model))\n",
    "        \n",
    "        for p in range(seq_len):\n",
    "            for i in range(self.d_model):\n",
    "                denominator = torch.pow(self.n, exponent=2*i/self.d_model)\n",
    "                embeddings[p][i] = torch.cos(p/denominator) if i%2==0 else torch.sin(p/denominator)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d583528-8f6b-4bcf-aec7-4dc9c255f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_encoding = PositionalEncoding(n=200, d_model=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b3aba-c345-43d8-bfc4-644dd2f02aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf036e-6885-43ea-9905-a39084fe669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.5403,  0.1198,  0.9999,  0.0017,  1.0000],\n",
       "        [-0.4161,  0.2379,  0.9996,  0.0035,  1.0000],\n",
       "        [-0.9900,  0.3526,  0.9991,  0.0052,  1.0000],\n",
       "        [-0.6536,  0.4622,  0.9983,  0.0069,  1.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a85c7-8cc6-4354-a852-bc67be529428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(layer, inp, out):\n",
    "    add_log(layer, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e6d4d-6753-4108-870f-856a0de5256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hook(model):\n",
    "    for layer in model.layers:\n",
    "        layer.register_forward_hook(func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
