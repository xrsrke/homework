{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d9130b-b45d-4a71-a04d-622e039ad503",
   "metadata": {},
   "source": [
    "No, the cross-entropy and the KL divergence are not the same, although they are closely related.\n",
    "\n",
    "The cross-entropy between two probability distributions $p$ and $q$ is defined as:\n",
    "\n",
    "$$H(p, q) = -\\sum_x p(x)\\log q(x)$$\n",
    "\n",
    "The KL divergence between two probability distributions $p$ and $q$ is defined as:\n",
    "\n",
    "$$D_{KL}(p|q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "While the cross-entropy involves only the probabilities of one distribution, the KL divergence involves the probabilities of both distributions. In fact, the KL divergence is the difference between the cross-entropy of $p$ and $q$ and the entropy of $p$:\n",
    "\n",
    "$$D_{KL}(p|q) = H(p, q) - H(p)$$\n",
    "\n",
    "where $H(p)$ is the entropy of $p$. Thus, we can see that the KL divergence and the cross-entropy differ by a term that depends only on $p$, the so-called \"self-information\" of $p$. The KL divergence is often used to measure the dissimilarity between two probability distributions, while the cross-entropy is often used as a loss function in machine learning, where the goal is to minimize the cross-entropy between the predicted and true probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b804aa1-b327-4d13-aeea-d48f2e4ea4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
