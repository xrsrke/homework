{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f80a32-5ffe-430e-b0c4-327e2819922b",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c9a855-8541-40cb-bef9-82259c1ebb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipe, dataset):\n",
    "        self.pipe = pipe\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def latency(self): pass\n",
    "\n",
    "    def memory(self): pass\n",
    "    \n",
    "    def accuracy(self): pass\n",
    "\n",
    "    def run_metrics(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d3449d-10f8-4ace-ba25-bfd435fa41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd83c8a-ad53-4f33-93e3-db770a77efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f3598-2d50-4c6d-b236-05be13f120ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(\n",
    "    F.log_softmax(student_logits, dim=-1),\n",
    "    F.softmax(teacher_logits, dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c30ae-2208-44b9-ae6e-de5063b2a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9396e07-c8dc-4c3a-8f8e-871a9fcb89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cf061-f02f-41d4-a0e4-909a3d3eda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.loss_func = nn.KLDivLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, returns_tensor=False):\n",
    "        output_student = model(inputs)\n",
    "        loss_ce = output_student.loss\n",
    "        \n",
    "        output_teacher = self.teacher_model(inputs)\n",
    "        \n",
    "        temperature = self.args.temperature\n",
    "        alpha = self.args.alpha\n",
    "        \n",
    "        loss_kl = temperature**2 * self.loss_func(\n",
    "            F.log_softmax(output_student.logits),\n",
    "            F.softmax(output_teacher.logits)\n",
    "        )\n",
    "        \n",
    "        loss = alpha*loss_ce + (1-alpha)*loss_kl\n",
    "        \n",
    "        return (loss, output_student) if returns_tensor else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439a44f-c900-4711-b9b3-488b99b1cc3d",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834f92b3-6285-4a6b-a203-00576bd3b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431b492-ff15-4c33-b9e4-b02e545f5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = reduce(images, 'b c h w -> b h w', reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c956b-f025-4076-885d-d0ddb629aba7",
   "metadata": {},
   "source": [
    "minimize the kl divergence between the probability distribution of the policy and the prob dist of the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821baaf-a2be-4034-bbf3-ef48f3d382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a514c9-ed89-439a-9a5b-20acda224fe8",
   "metadata": {},
   "source": [
    "the ratio between the prob dist of the current policy and the previous policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907b467-004a-459d-ba11-0353f9046afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory, latency, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e0a30-2bc7-4632-8626-aa777f26ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2f775-1e35-40fb-9202-2e5d3ffd3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.loss_func = nn.KLDivLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        output_student = model(inputs)\n",
    "        student_logits = output_student.logits\n",
    "        loss_ce = output_student.loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_teacher = self.teacher_model(inputs)\n",
    "            teacher_logits = output_teacher.logits\n",
    "        \n",
    "        temperature = self.args.temperature\n",
    "        alpha = self.args.alpha\n",
    "        \n",
    "        loss_kl = temperature**2 * self.loss_func(\n",
    "            F.log_softmax(student_logits / temperature, dim=-1),\n",
    "            F.softmax(teacher_logits / temperature, dim=-1)\n",
    "        )\n",
    "        \n",
    "        loss = alpha * loss_ce + (1-alpha) * loss_kl\n",
    "        \n",
    "        return (loss, output_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930eaf21-cc26-4ce5-8ea6-50aca80336f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07bf6d1-d6c3-4327-8676-3d3daaffff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, n_hidden):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        critic_value = self.critic(state)\n",
    "        \n",
    "        return action, log_prob, entropy, critic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed742506-0119-4e69-8e0d-6e983ed1fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_function = current_policy_probs / prev_policy_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40ad1f2-eed3-475e-921b-3df87af90a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44705679-adf2-4bc4-b74d-838c065f1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        k_permuted = k.permute(3, 2)\n",
    "        qk_matmul = torch.matmul(q, k_permuted)\n",
    "        scores = qk_matmul / math.sqrt(self.d_head)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b12d44-35cd-4a39-941a-223924460060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio(current, prev):\n",
    "    return currbbent - prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b64814-f7d9-4113-b6cf-1393c3165da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_steps):\n",
    "    state, action = states[t], actions[t]\n",
    "    q[state, action] = get_reward(state, action) + gamma * v_func(state + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6219f-6e7c-492d-88d5-68412da3fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_encoding = encoding.masked_fills(mask = False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5572ddc5-5668-41f3-9b75-d26d191b005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n, d_model):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, idxs):\n",
    "        seq_len = len(idxs)\n",
    "        \n",
    "        embeddings = torch.zeros(seq_len, self.d_model)\n",
    "        \n",
    "        for p in range(seq_len):\n",
    "            for i in range(self.d_model):\n",
    "                denominator = torch.pow(self.n, (2*i)/self.d_model)\n",
    "                embeddings[p][i] = torch.sin(p/denominator) if i % 2 == 0 else torch.cos(p/denominator)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e6c67-bea8-4e42-9dab-8680a0c006ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
