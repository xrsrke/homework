{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8643ca9",
   "metadata": {},
   "source": [
    "### Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee70c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "import sympy.physics.units as units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a774627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pint\n",
    "ureg = pint.UnitRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc83ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_1, q_2, r, k = sym.symbols('q_1, q_2, r, k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0546ca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle q_{1}$"
      ],
      "text/plain": [
       "q_1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5385c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = k * (q_1 * q_2)/r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92049fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{k q_{1} q_{2}}{r^{2}}$"
      ],
      "text/plain": [
       "k*q_1*q_2/r**2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.subs({\n",
    "    q_1: 25*1e-9*units.coulomb,\n",
    "    q_2: -75*1e-9*units.coulomb,\n",
    "    r: 3.0*units.centiliter\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5c0c1",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d1bf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865793ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation, sequence split, sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b2fa4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 3\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(n_images, \u001b[38;5;241m3\u001b[39m, \u001b[43mlatent_height\u001b[49m, latent_width)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_height' is not defined"
     ]
    }
   ],
   "source": [
    "n_images = 3\n",
    "\n",
    "noise = torch.randn(n_images, 3, latent_height, latent_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040a55d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7539ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.permute(3, 1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc015950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5086739",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tfms.Compose([\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize(0.3, 0.9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for all batch, the last token, and all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = output.logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_probs = F.softmax(next_token, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = torch.argmax(next_token_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516d022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa682c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ea155",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(\"Persistence is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aee8df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d778386",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.loss_func = nn.KLDivLoss()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs):\n",
    "        \n",
    "        output_student = model(**inputs)\n",
    "        logits_student = output_student.logits\n",
    "        loss_ce = output_student.loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_teacher = self.teacher(**inputs)\n",
    "            logits_teacher = output_teacher.logits\n",
    "        \n",
    "        temperature = self.args.temperature\n",
    "        alpha = self.args.alpha\n",
    "        \n",
    "        loss_kl = temperature * self.loss_func(\n",
    "            input=F.log_softmax(logits_student),\n",
    "            target=logits_teacher\n",
    "        )\n",
    "        \n",
    "        loss = alpha*loss_ce + (1-alpha)*loss_kl\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29985cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0706cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenClassifier(nn.Module):\n",
    "    def __init__(self, checkpoint, n_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel(checkpoint)\n",
    "        \n",
    "        # classifier head\n",
    "        final_hidden_state = 768\n",
    "        self.linear_head = nn.Linear(final_hidden_state, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        output = self.linear_head(last_hidden_state)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1, 2, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(logits, targets):\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    loss = -log_probs(range(targets.shape[-1]), targets).float().mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7892f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_steps):\n",
    "    q[t] = get_reward(states[t], actions[t]) + gamma * value_func(states[t]+1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.attention = attention\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, n_heads * d_head)\n",
    "        self.to_k = nn.Linear(d_model, n_heads * d_head)\n",
    "        self.to_v = nn.Linear(d_model, n_heads * d_head)\n",
    "        self.mha_linear = nn.Linear(n_heads * self.d_head, d_model)\n",
    "    \n",
    "    def s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687a1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rearrange(images, 'b c h w -> b c (h w )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285d361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cf5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5e38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64428980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380173dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa099a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba3b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9949c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cd814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af2890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84026c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76369e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49678ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13655fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d63c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.attention = attention\n",
    "        self.d_model, self.n_heads = d_model, n_hedas\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        self.to_k = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        self.to_v = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        self.mha_linear = nn.Linear(self.d_heads * n_heads, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, self.n_heads, seq_len, self.d_head)\n",
    "    \n",
    "    def concat(self, x):\n",
    "        batch_size, n_heads, seq_len, d_head = x.size()\n",
    "    \n",
    "    def forward(self, pre_q, pre_k, pre_v):\n",
    "        \n",
    "        q, k, v = self.to_q(pre_q), self.to_k(pre_k), self.to_v(pre_v)\n",
    "        \n",
    "        # split heads\n",
    "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        \n",
    "        output, attention_weights = self.attention(q, k, v)\n",
    "        \n",
    "        projection = self.mha_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694c7f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(size):\n",
    "    return torch.ones((size, size)).triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e1ad2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c4ce3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_layer, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.text_embedding = TextEmbedding(\n",
    "            vocab_size=1000, d_model=d_model, padding_idx=0\n",
    "        )\n",
    "        self.positional_embedding = PositionalEmbedding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model, n_head, d_ff, dropout\n",
    "            ) for _ in range(n_layer)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.text_embedding(tokens)\n",
    "        embeddings = self.positional_embedding(embeddings)\n",
    "        \n",
    "        out = embeddings\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            out, out_attn = encoder_layer(out)\n",
    "        \n",
    "        return out, out_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f52da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = embedding.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf301262",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_distribution(states):\n",
    "    values = q_function(states)\n",
    "    values = torch.exp(values)\n",
    "    value_sum = values.sum(dim=-1)\n",
    "    \n",
    "    return values / value_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54dbfeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=3, out_channels=5,\n",
    "    kernel_size=3, stride=1, padding=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38c7e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f35d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image. for x in sevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7bc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_tensors = torch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
