{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3311abd-77c7-4ec1-bd23-010b08283f86",
   "metadata": {},
   "source": [
    "### SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04ec055-db78-4bcc-95d5-860016a4e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a716adb0-6182-4f6d-92d7-f33ef005a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo() -> Callable[[int, int], int]:\n",
    "    def add(x: int, y: int) -> int:\n",
    "        return x + y\n",
    "\n",
    "    return add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf38a335-eef3-45fd-bc50-7a0f66944284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ae6af4-a9dc-49d4-8694-9d895a676ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Money:\n",
    "    currency: str\n",
    "    value: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd00bb5-dfd8-4876-a6e1-1532b9e72d1f",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fded98ae-6994-40e0-af32-a2feec2ec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e134cdc-117b-4026-bac3-88293223cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25038f36-87c3-488f-934e-7fc5184e8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        k_transposed = k.transpose(3, 2)\n",
    "        qk_matmul = torch.matmul(q, k_transposed)\n",
    "        scores = qk_matmul / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weight = F.softmax(qk_matmul, dim=-1)\n",
    "        output = torch.matmul(scores, v)\n",
    "        \n",
    "        return output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e9cfc4-1426-40db-8685-65012ca4b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1108b14-5cc9-4724-b1eb-2bb26f91c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenClassifier(nn.Module):\n",
    "    def __init__(self, checkpoint, n_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(checkpoint)\n",
    "        \n",
    "        # custom head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifer = nn.Linear(784, dropout)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        output = self.dropout(last_hidden_state)\n",
    "        output = self.classifer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f13b62-3907-41d8-a65b-a0b68b703196",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU, ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a24d8-ef0a-477c-a3d8-1ad784221618",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = output.logits[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19dcb4-5ab3-4484-823d-84a26af75a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_probs = F.softmax(next_token, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a21c05-4c5e-4c63-86fc-8906dcb7f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = torch.argmax(next_token_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f5764e-0c80-4b53-8f2d-7f561a88a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e9125-ef95-4b79-8e7c-7d7dddf04846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, n_hidden):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        logits = self.actor(state)\n",
    "        \n",
    "        dist = Categorical(logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f95667-f24a-4b71-80ef-1742d9540711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        k_transposed = k.transpose(3, 2)\n",
    "        qk_matmul = torch.matmul(q, k_transposed)\n",
    "        scores = qk_matmul / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8ea8c-f68e-4d4b-84d4-ddeeaee03524",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_similarity = images_embeddings @ image_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441ff5-f46d-4070-9cb0-ef7a078a08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_similarity = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f9762-cb06-4310-b45e-430becf62cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_episode):\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    \n",
    "    while True:\n",
    "        predicted_rewards = model(state)\n",
    "        action = torch.argmax(predicted_rewards, dim=-1)\n",
    "        predicted_reward = predicted_rewards[action]\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state)\n",
    "        \n",
    "        if done:\n",
    "            target_reward = reward\n",
    "        else:\n",
    "            next_reward = model(next_state)\n",
    "            max_next_reward = next_reward.max()\n",
    "            \n",
    "            target_reward = reward + GAMMA * max_next_reward\n",
    "        \n",
    "        loss = loss_func(predicted_reward, target_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4c478-eef9-49f2-9264-ef26bf46da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Categorical(logits=raw_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a721ff-02da-42ef-9625-cc5e3ae052e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970406ab-53c5-4e61-b9a2-9a0bcfbe5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_return_each_timestep(rewards, discount_factor=0.99):\n",
    "    arr = []\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        \n",
    "        discounted_return = 0\n",
    "        for k, reward in enumerate(rewards[i:]):\n",
    "            discounted_return += discount_factor**k * reward\n",
    "        \n",
    "        arr.append(discounted_return)\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f70357e-ddf5-47c1-9129-cac71dde9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3e786a-baeb-42ff-8258-3db918448208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(14.6045), tensor(13.7419), tensor(11.8605), tensor(8.9500), tensor(5.)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_each_timestep(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b165140-14a1-49fa-aff9-a13f14f25dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMMOdel(nn.Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        super().__init__()\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = 0.\n",
    "        for i in range(3):\n",
    "            h = self.i_h(x[i, :])\n",
    "            h = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189159de-2db8-4ffb-a33a-8a6fcfce0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = embedding.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c356dd-f231-4ad2-b2b0-a8807e001e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03398b7b-cbc0-49ac-906a-5c0d493c25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.text_embedding = TextEmbedding(\n",
    "            vocab_size=1000, d_model=d_model, padding_idx=0\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_mode, n_heads, d_ff, dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        embeddings = self.text_embedding(tokens)\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        \n",
    "        output = embeddings\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output, output_attn = encoder_layer(output)\n",
    "        \n",
    "        return output, output_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f1b98-1272-42b3-a9db-2e8855253919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_loop(model, x, y):\n",
    "    pred = model(x)\n",
    "    \n",
    "    loss = loss_func(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad5fef4-6043-414e-949b-2b43bffe7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=5, kernel_size=6,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(\n",
    "                in_channels=5, out_channels=16, kernel_size=16\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7092a66-0e6d-405f-8eff-3014d8cdee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortcutProjection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db57c70b-4823-4a6a-bd30-7501ff32f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, mom, eps):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.adds = nn.Parameter(torch.zeros(1))\n",
    "        self.mults = nn.Parameter(torch.ones(1))\n",
    "        self.register_buffer('means', torch.zeros(1))\n",
    "        self.register_buffer('vars', torch.ones(1))\n",
    "        \n",
    "    def update_stats(x):\n",
    "        mean, var = x.mean(), x.var()\n",
    "        \n",
    "        self.means.lerp_(mean, self.mom)\n",
    "        self.vars.lerp_(var, self.mom)\n",
    "        \n",
    "        return mean, var\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            mean, var = self.update_stats(x)\n",
    "        \n",
    "        # normalize\n",
    "        x = x-mean/(var + self.eps).sqrt()\n",
    "        \n",
    "        # scale and shift\n",
    "        x = self.mults * x + self.adds\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6fa12-0df3-4e7e-81e6-f3f0827360b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
