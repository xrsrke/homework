{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fcfe9f-892d-477c-9733-f3415091f9a1",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd47db-8b93-4551-a483-95e2678b9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "local rank, local world size, processgroup, ranks in group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411226a-6d0c-4919-93e5-5c5f112e9bc4",
   "metadata": {},
   "source": [
    "step 1: determine a list of global rank in that group\n",
    "step 2: if the process's global rank in that list, init a distributed group\n",
    "step 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa132afe-c640-47aa-8ef6-24972554dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySharding:\n",
    "    def __init__(self, module, parallel_context):\n",
    "        self.module = module\n",
    "        self.parallel_context = parallel_context\n",
    "        \n",
    "    def shard(self):\n",
    "        module = self.module\n",
    "        \n",
    "        self._shard_parameters()\n",
    "        \n",
    "        for name, param in module.named_parameters():\n",
    "            pass\n",
    "    \n",
    "    def _shard_parameters(self):\n",
    "        module = self.module\n",
    "        world_size = self.parallel_context.get_world_size()\n",
    "        \n",
    "        for p in module.parameters():\n",
    "            assert not hasattr(p, \"_is_sharded\")\n",
    "            \n",
    "            if world_size > 1:\n",
    "                orig_data = p.data\n",
    "                p.data = self._get_shard(p.data)\n",
    "    \n",
    "    def _get_shard(self, data):\n",
    "        world_size = self.parallel_context.get_world_size()\n",
    "        rank = self.parallel_context.get_rank()\n",
    "        \n",
    "        chunks = list(data.flatten().chunk(world_size))\n",
    "        while len(chunks) < world_size:\n",
    "            chunks.append(torch.empty(0))\n",
    "        \n",
    "        shard = chunks[rank].clone()\n",
    "        num_to_pad = chunks[0].numel() - shard.numel()\n",
    "        if num_to_pad > 0:\n",
    "            shard = F.parallel_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd319c-6af7-4169-bff0-56b984476e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batches,\n",
    "        partitions,\n",
    "        devices,\n",
    "        scheduler = DetermisticScheduler()\n",
    "    ):\n",
    "        self.batches = batches\n",
    "        self.partitions = partitions\n",
    "        self.devices = devices\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def fit(self):\n",
    "        batches = self.batches\n",
    "        partitions = self.partitions\n",
    "        devices = self.devices\n",
    "        scheduler = self.scheduler\n",
    "        \n",
    "        with spawn_worker(devices) as (in_queues, out_queues):\n",
    "            for schedule in scheduler:\n",
    "                self._compute(schedule, in_queues, out_queues)\n",
    "    \n",
    "    def _compute(self, schedule, in_queues, out_queues):\n",
    "        batches = self.batches\n",
    "        \n",
    "        for microbatch_idx, partition_idx in schedule:\n",
    "            batch = batches[microbatch_idx]\n",
    "            \n",
    "            def task_func(microbatch, partition):\n",
    "                def wrapper():\n",
    "                    return partition(microbatch)\n",
    "                return wrapper\n",
    "            \n",
    "            task = Task(compute=task_func)\n",
    "            in_queues[partition_idx].put(task)\n",
    "        \n",
    "        for microbatch_idx, partition_idx in schedule:\n",
    "            output = out_queues[partition_idx].get()\n",
    "            batches[microbatch_idx].put(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9142750-7bdb-44d6-b822-602e02191855",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine a list of global ranks in that group\n",
    "step 2: check whether the process's global rank in that list\n",
    "step 3: if yes, init a distributed group\n",
    "step 4: determine the process's local rank\n",
    "step 5: save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3db90a0-862c-4f88-8ae2-b91233612442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ee1b5-9f65-4eb7-bf0b-f77228233aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvenSampler(Sampler):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return [x for x in range(0, len(self.data), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a15b20-6a79-4190-8d24-0aa149e39f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "void addTwoVectors(int* a, int* b, int* c, int total_elements):\n",
    "    int gid = (threadBlock.x * threadDim.x) + threadIdx.x\n",
    "    \n",
    "    if (gid < total_elements) {\n",
    "        c[gid] = a[gid] + b[gid]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acee876-caec-4521-a561-e91f5b55c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: normalize the loss, loss / n_steps\n",
    "step 2: sum the grad\n",
    "step 3: if epoch = n_steps, then update, otherwise, repeat step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb8782-c891-4e92-b24a-a7a5cb9b4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: gather the weights\n",
    "step 2: do the backward pass\n",
    "step 3: release the non-relevant weights\n",
    "step 4: reduce-scattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a0abd-c4a1-4700-a0eb-08454df666d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "pos_embed = cache[\"pos_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b867ca-d89f-4de8-af24-0ec6ac7e2f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "components = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a12e6-05e7-47b4-a846-ee45bd6a49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.append([embed, pos_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128c8bcf-3737-40d8-b096-5f6cfa9be20e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b299a1d-54bf-429a-a452-42ff2bfd84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(3):\n",
    "    mlp_name = get_act_name(\"mlp_out\", layer_idx)\n",
    "    attn_name = get_act_name(\"attn_out\", layer_idx)\n",
    "    components.append(cache[attn_name])\n",
    "    components.append(cache[mlp_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1495d1f-a0f1-46c2-a935-d1466f9d0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = model.W_O[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43096f92-a0c3-4280-b16a-5e6e26f99fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe75bda-780a-42ac-afe5-082895c31360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f69524-92d3-44a3-aaf9-bc7a59504f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.add_trace(go.Line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d844e-4d11-45fa-a53c-851904785a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7272343-bf54-48dc-ad8a-5f9b2081ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227b097-5e3e-4f15-a9b9-b9ade54e7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "pos_embed = cache[\"pos_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5dbe9-5089-44f7-a4fa-11beaff1da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = cache[\"result\", layer_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba067b-aca5-43b1-94c2-2497a76aa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_components = torch.cat([\n",
    "    embed,\n",
    "    pos_embed,\n",
    "    heads,\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fac114-c73e-4df5-bb00-d08a4e693be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1544207-7e51-466f-a4d7-c601f189f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[layer_idx]\n",
    "query_components = einsum(\n",
    "    input_components,\n",
    "    W_Q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33a7e-b1f0-4bc1-b897-30a462b413c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = model.W_K[layer_idx]\n",
    "key_components = einsum(\n",
    "    input_components,\n",
    "    W_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1983d-e35c-44a1-905b-0462f2e3e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = einsum(\n",
    "    query_components,\n",
    "    key_components,\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5399956-db03-41e2-be1e-192cdb82abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd050b-8b9e-4a9b-92cf-b3283cedcac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"pattern\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b9503-cf67-4d76-9414-5559317f9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_pattern = cache[hook_name][:, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8616a-8ee3-4280-a4a4-9f4d3438bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_pattern[target_query_positions].mean(dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c44d6-4159-404a-bdff-8fb30483f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prob(0) = sigmoid(logit0 - logit1)\n",
    "step 2: logit0 = resid2 @ W_U[0], logit1 = resid2 @ W_U[1]\n",
    "step 3:\n",
    "logit0 - logit1 = resid2 @ W_U[0] - resid2 @ W_U[1]\n",
    "logit0 - logit1 = resid2 @ (W_U[0] - W_U[1])\n",
    "step 4:\n",
    "resid2 = resid1 @ ln2 @ W_OV^{0, 2}\n",
    "step 5:\n",
    "ln2 @ W_OV^{0, 2} @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eff25e-4a72-4d00-99db-9b561d1be2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864dcd5-3034-40c2-9216-c04f9572f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafbee3-e31e-4295-bd15-f6b12ba68bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_OV_circuit = W_E @ OV_circuit @ W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a3aac-aee4-4ca2-8f3a-ebf1097b5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8934c-3ab4-4700-a200-ba76ee8bc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment, configmap, service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ef970-8cb5-421a-b60e-0cb4e6279150",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine the number of workers\n",
    "step 2: flatten the params\n",
    "step 3: chunks\n",
    "step 4: add empty chunk if len(chunks) < world_size\n",
    "step 5: get shard\n",
    "step 6: pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbcaaf-66b5-49da-8a66-71f1ed68923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear, layernorm, embedding, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135f1f29-a190-410c-8b81-efef83dc4b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa937bcf-6deb-48cd-8c6f-c3db816984fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySharding:\n",
    "    def __init__(self, module, parallel_context):\n",
    "        self.module = module\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def shard(self):\n",
    "        self._shard_params()\n",
    "    \n",
    "    def _shard_params(self):\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            assert hasattr(param, \"_is_sharded\")\n",
    "            \n",
    "            orig_data = param.data\n",
    "            shard = self._get_shard(param.data)\n",
    "            free_memory(orig_data)\n",
    "            param.data = shard\n",
    "            param._is_sharded = True\n",
    "    \n",
    "    def _get_shard(self, data):\n",
    "        world_size = self.parallel_context.get_world_size()\n",
    "        rank = self.parallel_context.get_rank()\n",
    "        \n",
    "        chunks = data.flatten().chunks(world_size)\n",
    "        \n",
    "        while len(chunks) < world_size:\n",
    "            chunks.append(torch.empty(0))\n",
    "        \n",
    "        shard = chunks[rank]\n",
    "        num_to_pad = chunks[0].numel() - shard.numel()\n",
    "        \n",
    "        if num_to_pad > 0:\n",
    "            shard = F.pad(shard, pad=num_to_pad)\n",
    "        \n",
    "        return shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26015ec3-8cd7-47a5-bb2b-ee7dd97e7555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_paralell_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2772ee46-4b3d-4cb6-8a8d-7f7c60c9984d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tensor_model_parallel_groups = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c9bc70-cd63-468c-8a67-c4f88b12ded9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 2)\n",
      "range(2, 4)\n",
      "range(4, 6)\n",
      "range(6, 8)\n",
      "range(8, 10)\n",
      "range(10, 12)\n",
      "range(12, 14)\n",
      "range(14, 16)\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_tensor_model_parallel_groups):\n",
    "    ranks = range(\n",
    "        i*tensor_model_paralell_size,\n",
    "        (i+1)*tensor_model_paralell_size\n",
    "    )\n",
    "    print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7065ef0c-0574-4c34-8a6a-2899d1bc6c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6482d-5b54-4aef-adde-af2fecff240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = random_split(dataset, lengths=[6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88691472-514d-49e5-a467-9c12a5576d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=2)\n",
    "test_loader = DataLoader(test_set, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abeaa5-389b-46ea-9b0b-9a7611174d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "3, host, torchstate, elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142e971-5f5a-4bb2-ad5a-2ea7a9c1e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: partitioning\n",
    "step 2: gather, do forward pass, release\n",
    "step 3: gather, do backward pass, release\n",
    "step 4: reduce-scatter\n",
    "step 5: update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ad4471-c4e8-4710-a592-cb12aa6dbd90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probability_scores(image_embedding, text_embedding):\n",
    "    image_norm = image_embedding.norm(dim-1, keepdim=True)\n",
    "    image_embedding = image_embedding / image_norm\n",
    "    \n",
    "    text_norm = text_embedding.norm(dim=-1, keepdim=True)\n",
    "    text_embedding = text_embedding / text_norm\n",
    "    \n",
    "    similarities = image_embedding @ text_embedding.T\n",
    "    probs = F.softmax(\n",
    "        similarities,\n",
    "        dim=-1\n",
    "    )\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0398b-06b4-48cd-95ee-f3cb96d3d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad, dtype, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be75d7-b108-4c7f-b03d-331bcb6613b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: gather\n",
    "step 2: register pre backward hook\n",
    "step 3: do forward\n",
    "step 4: register post backward hook\n",
    "step 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26440ad-9f5b-468d-94ec-4e7c3da7d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "optic nerve > thalamus > visual cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e7282e-a1a7-40ec-acd5-26e995614e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b321231-91da-4a60-bf45-96b35f15850e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4b5f4d5-c3e3-46e8-b4bc-a4e3eb683cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p2 = p + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2be1bbba-095f-4820-864c-64bd5730830e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_tmp = p2.expand_as(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1543aeff-4707-4d26-8905-3015e969fee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5315])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b6eccb9-0c82-483a-9912-3a095f5aeaa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_tmp.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676b334-f7c0-4550-beb6-958b7782f227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37460703-de1d-4bac-aedf-f9dcfda2a9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10dbc8d1-7888-4977-bfad-197425defcc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0319]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35986dc-7eb2-4761-9cae-26ce743ebe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
