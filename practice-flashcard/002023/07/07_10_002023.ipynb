{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac163906-d6a7-4dac-b755-50620443114e",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014526bc-9c69-46f3-b0b8-bcec5bc10529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3bde8c-2b9c-492f-9b6f-63a4e2cf607d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        output, input_leaf = ctx.recomputed\n",
    "        input_leaf = input_leaf.detach().requires_grad_(\n",
    "            input_leaf.requires_grad\n",
    "        )\n",
    "        \n",
    "        with torch.grad_enabled():\n",
    "            torch.autograd.backward(output, grad_input)\n",
    "        \n",
    "        grads = [None, None, None]\n",
    "        if input_leaf.requires_grad:\n",
    "            grads.extend([input_leaf.grad])\n",
    "        else:\n",
    "            grads.extend([None])\n",
    "        return tuple(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1361a-b38c-4d77-964a-7221c0288808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "using namespace std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece0f2d-8b22-4b53-9d49-993a7be266a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (auto i: xs) {\n",
    "    std::cout << \"value: \" << i << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af54dd1e-3801-4247-8d40-f6c0e1f4fa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HostUpdatedInterrupt(Exception):\n",
    "    def __init__(self, skip_sync):\n",
    "        self.skip_sync = skip_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d12519-021d-4094-bcea-fa0afe1b4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor\n",
    "reassign if nodes leave\n",
    "new communication ring if nodes join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f58bb6-ae52-44a9-936f-c2a24f1dda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStateHandler:\n",
    "    def __init__(self, model):\n",
    "        self.value = model\n",
    "        self.save()\n",
    "    \n",
    "    def save(self):\n",
    "        self._model_state_dict = self.value.state_dict()\n",
    "    \n",
    "    def restore(self):\n",
    "        self.value.load_state_dict(self._model_state_dict)\n",
    "    \n",
    "    def sync(self):\n",
    "        broadcast_parameters(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49cf8cb6-10e8-4522-877d-117e7e79ef7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_handler(v):\n",
    "    for handler_type, handler_cls in handler_registry:\n",
    "        if isinstance(v, handler_cls):\n",
    "            return handler_cls(v)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1c30a5-1b2d-4346-85aa-e6ce9f29a595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_handlers(states):\n",
    "    handlers = {}\n",
    "    remainders = {}\n",
    "    \n",
    "    for k, v in states:\n",
    "        handler = get_handler(v)\n",
    "        if handler is None:\n",
    "            remainders[k] = v\n",
    "        else:\n",
    "            handlers[k] = v\n",
    "    \n",
    "    return handlers, remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811597c-bf50-4db3-911a-e5c28618db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_memory(model):\n",
    "    total_memory = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        total_memory += param.storage.size() * param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e44419-73f1-4fb1-aa52-24cc0aa33e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socketserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e8aa0-e120-48bc-b0a5-41904a256846",
   "metadata": {},
   "outputs": [],
   "source": [
    "with socketserver.ThreadingTCPServer(\n",
    "    (MASTER_HOST, MASTER_PORT),\n",
    "    EchoRequestHandler\n",
    ") as server:\n",
    "    server.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8acef9f-f508-4a53-931b-7179ff15fac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333b1c94-8117-4e25-a918-a7d0e2841887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelStateHandler:\n",
    "    def __init__(self, model):\n",
    "        self.set_value(model)\n",
    "    \n",
    "    def save(self):\n",
    "        self._model_state_dict = copy.deepcopy(\n",
    "            self.value.state_dict()\n",
    "        )\n",
    "    \n",
    "    def restore(self):\n",
    "        self.value.load_state_dict(self._model_state_dict)\n",
    "    \n",
    "    def sync(self):\n",
    "        broadcast_parameters(self.value)\n",
    "    \n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5aca57-0cc6-4dee-aa6f-13ddf3750ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_handler(v):\n",
    "    for handler_type, handler_cls in handler_registry:\n",
    "        if isinstance(v, handler_cls):\n",
    "            return handler_cls(v)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605ff50b-b9a1-4c43-a8a3-01f0db765ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_handlers(states):\n",
    "    handlers = {}\n",
    "    remainders = {}\n",
    "    \n",
    "    for k, v in states:\n",
    "        handler = get_handler(v)\n",
    "        if handler is None:\n",
    "            remainders[k] = v\n",
    "        else:\n",
    "            handlers[k] = v\n",
    "    return handlers, remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a4c5e-5e13-4a86-b634-ea31069046e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = torch.distributed.get_rank()\n",
    "group = None\n",
    "ranks = [0, 1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dacdcea-f2d2-41e9-8710-811bc8b1fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank in ranks:\n",
    "    group = torch.distributed.new_group(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa03f10-eb09-48ee-8dbf-384831e97750",
   "metadata": {},
   "outputs": [],
   "source": [
    "if group is not None:\n",
    "    torch.distributed.broadcast(x, src=0, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7139bff9-99c6-4ae8-8e0c-1a23d59e7f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20fca6a9-a410-4179-af79-399fd513ff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = threading.local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44c803f-e6bb-4316-8eac-67a0f380c026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_worker():\n",
    "    print_and_modify(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befc4f2-9a34-4771-b9e1-1ef8166bfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=run_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ea6130a-5d58-4132-852e-4d179b2670dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_worker(event):\n",
    "    print(\"waiting\")\n",
    "    event.wait()\n",
    "    print(\"received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9260ee19-7283-4668-b6a2-f5a335217c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event = threading.Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd56f7-2da4-479f-89ed-fefb91265e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_thread = threading.Thread(\n",
    "    target=run_worker,\n",
    "    args=(event,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02063f74-efb9-43a0-8feb-06f39780dbf6",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d96af3-3ba5-46da-a04c-06f02bdfc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: head 2\n",
    "step 2: logit difference directions\n",
    "step 3: einsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c30e0cd-9cf6-4638-8dd4-41fc7d110105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_shape(module, input):\n",
    "    print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99baa0d-5a64-4b03-b4a7-55e3b5f1c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[1].register_forward_pre_hook(print_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefbd33-ebc5-4898-aac8-97f6a529808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb132e0-cdf8-4f47-a704-810f0ec25abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"hook_enmbed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b1a97-81fa-47a1-9120-672f1d3d8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70068c46-03d3-4ae5-ae22-fe7d016356e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f6fd25-e7a6-4b3a-907e-c14df8742548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_neuron(activations, hook):\n",
    "    data = activations[:, :, neuron_idx]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3c592-c682-4579-a406-0697e02ec19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30f4df-c968-4d1f-8164-619f99b3ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_with_cache(\n",
    "    tokens,\n",
    "    fwd_hooks=[(hook, extract_neuron)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05f315-73fc-4cfd-a4dd-166948cd7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_neuron = torch.argmax(data, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b829f0b-9300-4535-94f8-74a6b4190982",
   "metadata": {},
   "outputs": [],
   "source": [
    "x@W_Q\n",
    "\n",
    "x = embed + pos_embed + sum(12 heads)\n",
    "\n",
    "\n",
    "[embed + pos_embed + sum(12 heads)] @ W_Q\n",
    "\n",
    "embed @ W_Q + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ec6a8-c9f2-4ca8-a1b4-a70d08bc600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed + pos_embed + sum(12 heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f58b2-6354-4d14-9ec9-49d438a2b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534b3aa-c784-4dbb-aaed-66cb3198fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(head_1 + head_2 + head_3) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862b7a5-7aaf-4e2e-839b-c156720552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ddb97-6fba-434e-989f-7a88d36851d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_residual_stream = cache[final_residual_name]\n",
    "last_token_final_residual_stream = final_residual_stream[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0614bbd-26bb-4e7d-a421-6abd8f717e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_last_token_final_residual_stream = model.apply_ln_to_stack(\n",
    "    last_token_final_residual_stream,\n",
    "    layer=-1,\n",
    "    pos_slice=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42ab36-476c-4fa8-80a2-dc2cf29be9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "correct_residual_direction = W_E[:, correct_token]\n",
    "incorrect_residual_direction = W_E[:, incorrect_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76fb8c-38b3-4009-a47f-a7835470a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff_direction = correct_residual_direction - incorrect_residual_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6081686e-881f-4174-956b-db5264d55f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1bf88f-1963-449e-8f0b-f1ba0ed80dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_difference = einsum(\n",
    "    scaled_last_token_final_residual_stream,\n",
    "    logit_diff_direction,\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed08217-02bd-4c76-b9bc-363c006dd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11749c0a-2ad9-4d5d-9f37-1641dbf43d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e00e5-9a1b-4294-90dd-e0bbf7dcce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"hook_embed\"]\n",
    "unembed = cache[\"hook_unembed\"]\n",
    "head_outputs = cache[\"result\", layer_idx-1]\n",
    "input_components = torch.cat([embed, unembed, head_outputs], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f5eba-3bc9-453e-9397-76b6ed7fb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "query_components = einsum(input_components, W_Q, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3e90e-f524-4f96-a294-008c98d18c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = model.W_K[layer_idx, head_idx]\n",
    "key_components = einsum(input_components, W_Q, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3bcce-d613-4f44-be70-d4dc4b22c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = einsum(\n",
    "    query_components, key_components,\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0879e6-bb00-4869-9600-3077265622c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"blocks.1.attn.pattern_hook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "928d1dce-0bd4-4167-8446-70f91fe141fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_model(model, balances, devices):\n",
    "    layers = {}\n",
    "    partritions = []\n",
    "    partrition_idx = 0\n",
    "    \n",
    "    for name, layer in model.named_children():\n",
    "        layers[name] = layer\n",
    "        \n",
    "        if len(layers) == balances[partrition_idx]:\n",
    "            partrition = nn.Module(layers)\n",
    "            partrition.to(devices[partrition_idx])\n",
    "            partritions.append(partrtion)\n",
    "            layers.clear()\n",
    "    return partritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6f842-2364-4a96-b023-ad4df0afe07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortCutProjection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac84e6-d476-4866-a8b3-7bb20f3d5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f389df-815a-48b0-bb25-efd8bf24c262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5bbb2-b326-4e06-b4e1-f1c31880b4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8639ed5-dbc7-406b-88a2-eedb1f55a47f",
   "metadata": {},
   "source": [
    "The neuron matrix, $W_{\\text{neur}}$, represents the weights between the attention layer and the MLP (ReLU) layer in our transformer model. It tells us how to get from a weighted sum of initial embeddings (the output of the attention layer) to the inputs to the ReLU function.\n",
    "\n",
    "To calculate $W_{\\text{neur}}$, we simply multiply the relevant weight matrices:\n",
    "$W_{\\text{neur}} = W_E W_V W_O W_{\\text{in}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$W_E$ is the embedding matrix\n",
    "$W_V$ and $W_O$ are the value and output matrices for each attention head\n",
    "$W_{\\text{in}}$ is the input matrix for the MLP\n",
    "So if we had:\n",
    "\n",
    "$W_E \\in \\mathbb{R}^{100 \\times 512}$ (embedding dim = 512)\n",
    "4 attention heads\n",
    "For each head, $W_V, W_O \\in \\mathbb{R}^{512 \\times 64}$\n",
    "$W_{\\text{in}} \\in \\mathbb{R}^{64 \\times 2048}$ (MLP dim = 2048)\n",
    "Then $W_{\\text{neur}}$ would be:\n",
    "\n",
    "$W_{\\text{neur}} \\in \\mathbb{R}^{4 \\times 100 \\times 2048}$\n",
    "\n",
    "And we could calculate it as:\n",
    "\n",
    "py\n",
    "Copy\n",
    "W_E = torch.rand(100, 512)  # Embedding matrix\n",
    "W_V = torch.rand(512, 64)   # Value matrix for one head\n",
    "W_O = torch.rand(512, 64)   # Output matrix for one head \n",
    "\n",
    "W_in = torch.rand(64, 2048) # Input matrix for MLP \n",
    "\n",
    "# Stack value/output matrices for 4 heads \n",
    "W_V = torch.stack([W_V]*4)   \n",
    "W_O = torch.stack([W_O]*4)   \n",
    "\n",
    "W_neur = W_E @ W_V @ W_O @ W_in\n",
    "So $W_{\\text{neur}}$ tells us how to transform the output of the attention layer (a weighted sum of 4 embedding vectors) into the 2048-dimensional input for the ReLU activation in the MLP.\n",
    "\n",
    "Does this help explain the neuron matrix? Let me know if you have any other questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3623ac7-cf21-41cf-b292-08ae273ee98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
