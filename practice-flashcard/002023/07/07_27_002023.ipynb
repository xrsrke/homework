{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8b5c97-b475-42eb-8f08-d93119b961f6",
   "metadata": {},
   "source": [
    "You're right that the key factor is the relative dimensionality of the attention heads compared to the residual stream.\n",
    "\n",
    "Mathematically, we can show this as follows:\n",
    "\n",
    "Let $d$ be the dimension of the residual stream.\n",
    "\n",
    "Let $k$ be the dimension of each attention head.\n",
    "\n",
    "The attention head matrices $W_Q, W_K, W_V$ used to project into the head space are of size $d \\times k$.\n",
    "\n",
    "The output projection matrix $W_O$ is of size $k \\times d$.\n",
    "\n",
    "This means the rank of the subspace transformed by the attention head is at most $\\min(d,k) = k$.\n",
    "\n",
    "So when $k << d$, the attention head is operating in a low dimensional subspace compared to the full residual space. This leads to low interference between heads.\n",
    "\n",
    "Specifically, if we have two heads operating on orthogonal subspaces of dimension $k$, the overlap between their transformations will be at most $k^2 / d^2$. As $k/d \\rightarrow 0$, this goes to 0.\n",
    "\n",
    "In contrast, if $k = d$, then there is no dimensionality reduction and heads could potentially interfere maximally.\n",
    "\n",
    "So in summary, having head dimension $k$ much lower than residual dimension $d$ is what creates the independence between heads and enables them to specialize.To understand the interference between head communications in the residual stream, let's first clarify what's happening in Transformer models.\n",
    "\n",
    "In a Transformer, each attention head performs a task of mapping a query and a set of key-value pairs to an output, where the query, keys, and values are vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed using the query and the corresponding key. These weights are calculated using the softmax function applied to the dot product of the query and key vectors.\n",
    "\n",
    "The dimension of the attention head (often referred to as d_head) is the dimension of the vectors processed by the attention head. The dimension of the residual stream (often referred to as d_model) is the dimension of vectors in the input and output of the Transformer layer.\n",
    "\n",
    "Now, suppose we have two attention heads: one with the same dimension as the residual stream (i.e., d_head = d_model), and one with a much smaller dimension than the residual stream (i.e., d_head << d_model).\n",
    "\n",
    "Mathematically, the interference between head communications in the residual stream can be thought of as the overlap between the subspaces spanned by the key-query pairs in each head. If two heads span the same subspace, then they interfere with each other perfectly; if they span orthogonal subspaces, then they do not interfere at all.\n",
    "\n",
    "Given that the dimension of each head (d_head) determines the size of the subspace it can span, a head with a lower dimension (d_head << d_model) will span a smaller subspace and therefore will have less overlap with the other heads, resulting in lower interference.\n",
    "\n",
    "Let's make this more formal:\n",
    "\n",
    "Let $H_1$ and $H_2$ be two attention heads, with dimensions $d_{H_1}$ and $d_{H_2}$ respectively.\n",
    "Assume that the vectors processed by each head are randomly distributed, i.e., the keys and queries are random vectors.\n",
    "The expected overlap between the subspaces spanned by $H_1$ and $H_2$ can be approximated by $\\frac{d_{H_1} d_{H_2}}{d_{model}}$ (see the Johnson-Lindenstrauss lemma).\n",
    "If $d_{H_1} = d_{model}$ and $d_{H_2} << d_{model}$, then the expected overlap (interference) is approximately $\\frac{d_{model} d_{H_2}}{d_{model}} = d_{H_2}$, which is much smaller than $d_{model}$.\n",
    "\n",
    "So, in conclusion, a smaller attention head dimension will result in lower interference between head communications in the residual stream, assuming that the vectors processed by each head are randomly distributed.\n",
    "\n",
    "This is a simplified explanation, and the actual scenario in a Transformer model may be more complex due to the interaction between attention heads and the role of the residual connections and layer normalization. Also, the distribution of keys and queries may not be random and could be influenced by the training data and the learning dynamics of the model. But this gives a good intuition of why smaller attention head dimensions can reduce interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04106292-79a3-48bf-89da-2cdff009569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c104849-c9f6-4647-8984-4630a2772563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc85fdd-59bb-4b76-9482-b323ca481935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "646b8fb0-5372-4b98-9029-f2105b05f12b",
   "metadata": {},
   "source": [
    "To understand the interference between head communications in the residual stream, let's first clarify what's happening in Transformer models.\n",
    "\n",
    "In a Transformer, each attention head performs a task of mapping a query and a set of key-value pairs to an output, where the query, keys, and values are vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed using the query and the corresponding key. These weights are calculated using the softmax function applied to the dot product of the query and key vectors.\n",
    "\n",
    "The dimension of the attention head (often referred to as d_head) is the dimension of the vectors processed by the attention head. The dimension of the residual stream (often referred to as d_model) is the dimension of vectors in the input and output of the Transformer layer.\n",
    "\n",
    "Now, suppose we have two attention heads: one with the same dimension as the residual stream (i.e., d_head = d_model), and one with a much smaller dimension than the residual stream (i.e., d_head << d_model).\n",
    "\n",
    "Mathematically, the interference between head communications in the residual stream can be thought of as the overlap between the subspaces spanned by the key-query pairs in each head. If two heads span the same subspace, then they interfere with each other perfectly; if they span orthogonal subspaces, then they do not interfere at all.\n",
    "\n",
    "Given that the dimension of each head (d_head) determines the size of the subspace it can span, a head with a lower dimension (d_head << d_model) will span a smaller subspace and therefore will have less overlap with the other heads, resulting in lower interference.\n",
    "\n",
    "Let's make this more formal:\n",
    "\n",
    "Let $H_1$ and $H_2$ be two attention heads, with dimensions $d_{H_1}$ and $d_{H_2}$ respectively.\n",
    "Assume that the vectors processed by each head are randomly distributed, i.e., the keys and queries are random vectors.\n",
    "The expected overlap between the subspaces spanned by $H_1$ and $H_2$ can be approximated by $\\frac{d_{H_1} d_{H_2}}{d_{model}}$ (see the Johnson-Lindenstrauss lemma).\n",
    "If $d_{H_1} = d_{model}$ and $d_{H_2} << d_{model}$, then the expected overlap (interference) is approximately $\\frac{d_{model} d_{H_2}}{d_{model}} = d_{H_2}$, which is much smaller than $d_{model}$.\n",
    "\n",
    "So, in conclusion, a smaller attention head dimension will result in lower interference between head communications in the residual stream, assuming that the vectors processed by each head are randomly distributed.\n",
    "\n",
    "This is a simplified explanation, and the actual scenario in a Transformer model may be more complex due to the interaction between attention heads and the role of the residual connections and layer normalization. Also, the distribution of keys and queries may not be random and could be influenced by the training data and the learning dynamics of the model. But this gives a good intuition of why smaller attention head dimensions can reduce interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afbe55-58b5-426b-9da2-31d46833f751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a77992-d05a-43cb-9940-f887c6fbb167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d237f7-0f4b-4109-ba5e-4a4a227d83e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a463cf-448d-4774-a38f-322bf9fe223c",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722421c5-52c8-4bb6-97a4-b9b99c56c07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615389dc-2667-4668-9828-69491cc3ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready, running, suceed, failed, blacklisted, cooldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93f386-6574-4d88-8b1c-28125591076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOPs / memory bandwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb63adf-9d68-44c2-ae1e-e8cef2b55e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency between batches, data transfer and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80687e7-3eec-47d4-9f6d-049e7511f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c325799-c919-4c6e-bc09-1dcf3863a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    int numbers[] = {1, 2, 3, 4, 5};\n",
    "    \n",
    "    for (i=0; i=3; i++) {\n",
    "        std::cout << i;\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b2db0-6762-4396-b37e-b14c346a8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef int age;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a27877-381d-4f38-ae58-dce28c76afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    int i = 0;\n",
    "    \n",
    "    while i <= 5 {\n",
    "        std::cout << i;\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccbe5b-ff9a-4750-9e26-20e203c35218",
   "metadata": {},
   "outputs": [],
   "source": [
    "void numberToColor(int number) {\n",
    "    switch number {\n",
    "        case\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87d772-f3db-4820-8c75-b5d94f9fd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid > thread block > thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27d722-c555-4e32-8c3a-4195c0015f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recompted, function, input):\n",
    "        ctx.recomputed = recompted\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        output, input_leaf = ctx.recomputed.pop()\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            torch.autograd.backward(output, input_leaf)\n",
    "        \n",
    "        return None, None, None, input_leaf.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d4879-30fa-4dd1-bd1f-2139d835e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "void addVector(int* a, int* b, int* c, int total_elements) {\n",
    "    int gid = (blockIdx.x * blockDim.x) + threadIdx.x\n",
    "    \n",
    "    if (gid < total_elements) {\n",
    "        c[gid] = a[tid] + b[tid]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d31bb2-a039-41fc-b328-cf353de4253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6268494-2577-459c-b79c-5a07a658b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_event.record()\n",
    "\n",
    "hardshit()\n",
    "\n",
    "end_event.record()\n",
    "\n",
    "elapsed_time = end_event.elapsed_time(start_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc00526-8f4a-492b-b114-ff729ace3811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69dadd-ccc6-46e3-9d9e-854aab44cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prob(0) = sigmoid(logit0 - logit1)\n",
    "step 2: logit0 = resid2 @ W_U[0], logit1 = resid2 @ W_U[1]\n",
    "step 3: logit0 - logit1 =\n",
    "= resid2 @ W_U[0] - resid2 @ W_U[1]\n",
    "= resid2 @ (W_U[0] - W_U[1])\n",
    "step 4: resid2 = resid1 @ ln1 @ W_OV^{2.0}\n",
    "step 5: resid1 @ ln1 @ W_OV^{2.0} @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eade88e-c707-403b-a76b-dded92e3f809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870497b-89f5-45bd-8948-91174e7d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in = model.W_in[layer_idx]\n",
    "\n",
    "input_direction = W_in[:, neuron_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6fa374-6a3c-47d4-8976-2196aebcdbb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76a3c1-ce8e-4d0f-b8d3-ea541b19ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"pattern\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbeeae6-3f74-44cc-b5c6-c7fe8daf8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens,\n",
    "    names_filter=lambda x: x == hook_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5ae86-90ea-433f-9894-102ab48cf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_pattern = cache[hook_name][:, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d196af-785b-4f95-ab46-798bb2772e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_pattern[target_query_positions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975f2e5-40fb-49cf-be01-6c3c0ad3b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "adding relu\n",
    "correlated vs non-correlated features\n",
    "sparity\n",
    "feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c1db4-e6ff-4203-8015-d2780f2b3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: disverse distribution\n",
    "step 2: record\n",
    "step 3: extract attention pattern\n",
    "step 4: average\n",
    "step 5: plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d3357-b000-42fc-b03e-cad0f4547465",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69dbc08-41ee-4f1f-bcf9-8e742e8f8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "population activity, stimulus-evoked, motor imagery, conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e9786-ff5b-4916-a6db-7d5b82462c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank, tensor, paralllel size, world size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af49de-3526-4c9d-9243-1f0b7d469f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: init\n",
    "step 2: bind\n",
    "step 3: accept\n",
    "step 4: communicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23bb88-1ef3-4ee5-b1ee-331a5385f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear, layer norm, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69469b54-a1fa-4ac2-93fe-6852a2a2d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "void addVector(int* a, int* b, int* c, int total_elements) {\n",
    "    int gid = (blockIdx.x * blockDim.x) + threadIdx.x\n",
    "    if (gid < total_elements) {\n",
    "        c[gid] = a[gid] + b[gid]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee023c1-3bcc-4186-9b3e-b17ed9212e2d",
   "metadata": {},
   "source": [
    "step 1: detertmine a list of global ranks in that parallel group\n",
    "step 2: if the global rank in the list of ranks, then init a distributed group\n",
    "step 3: device\n",
    "step 4: seed\n",
    "step 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f7806-b642-4355-891e-69fada4560b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prob(0) = sigmoid(logit0 - logit1)\n",
    "step 2: logit0 = resid2 @ W_U[0], logit1 = resid2 @ W_U[1]\n",
    "step 3:\n",
    "logit0 - logit1 = resid2 @ W_U[0] - resid2 @ W_U[1]\n",
    "= resid2 @ (W_U[0] - W_U[1])\n",
    "step 3: resid2 = resid1 @ ln2 @ W_OV^{0, 2}\n",
    "step 4: resid1 @ ln2 @ W_OV^{0, 2} @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1873e1c-a44f-4a43-aac9-e54b282cd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU(x@W_in)@W_U\n",
    "\n",
    "= sum over d_mlp neurons ReLU(x@W_in[:, i])@W_out[i, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb287940-5424-42b7-9906-c07f90833737",
   "metadata": {},
   "source": [
    "step 1: start with a diverse data distribution\n",
    "step 2: record the activations of the target head\n",
    "step 3: extract the attention pattern between the target position and all other positions\n",
    "step 4: aveage across batch\n",
    "step 5: graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dc010-689c-404d-a5f1-38af3a57e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cace11c0-2c56-4933-8a42-53c2875ca82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        k = k.permute(-2, -1)\n",
    "        scores = torch.matmul(q, k) / (self.d_head*0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill(mask == True, -1e9)\n",
    "        \n",
    "        attention_pattern = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_pattern, v)\n",
    "        return output, attention_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0bd67-74cf-4778-97e1-bcddb47762b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(logits, targets):\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    return -log_probs[torch.arange(targets.shape[-1]), targets].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b65a1c2-03ee-44a0-8647-ce5a7dfc078e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ff47-2651-4826-b859-7fb50fba7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_communication(name, rank, world_size):\n",
    "    rpc.init_rpc(\n",
    "        name=name,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d15706-9e9e-47d7-85c2-e406357b6796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94667344-4ea8-4b4d-8147-2239c4e2faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ein_output = einsum(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42651738-0395-41b2-a2c0-e44c1a72a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "local rank, local world size, ranks in group, ProcessGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda79a63-bd1f-4d66-a991-a1e36deaff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, linear, attention, layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c960f65-b0be-435b-a540-6ff9574488d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, downsample):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            stride=stride, \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d15af9-5226-4ef3-8f85-4c4532847757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1765a3e0-c8d8-4fa7-9d88-579f800472bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Component(ABC):\n",
    "    @abstractclassmethod\n",
    "    def activate(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2881d982-f128-4e99-8eb6-12e143a29455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Screen(Component):\n",
    "    def activate(self):\n",
    "        print(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e43c9-f6bd-4842-b8e1-63aaa939bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Computer:\n",
    "    def __init__(self, components):\n",
    "        self.components = components\n",
    "    \n",
    "    def activate(self):\n",
    "        for component in self._components:\n",
    "            component.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4df1b8-07b6-41cc-bc02-930cdf28d2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12d1e7-4954-43de-ac24-8a9be31fcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "einsop_output = einsum(x, y, \"b dim, b dim2 -> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e573622-addd-474e-8016-d6d90408cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(first_part @ filter1).sum() + (first_part @ filter2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba428e-0186-4db3-af2d-cb2880b850e3",
   "metadata": {},
   "source": [
    "Yes, your understanding is correct. Each attention head processes the information within its own subspace, which is of lower dimensionality. When we project this output back up to the full dimensionality of the residual stream, the resulting vector remains within the subspace defined by the attention head. This means it can't influence the residual stream outside of this subspace.\n",
    "\n",
    "The mathematical intuition behind this involves the concepts of vector spaces and linear transformations.\n",
    "\n",
    "To illustrate this, let's consider an example where the dimensionality of the residual stream, $d_{\\text{model}}$, is 3 (i.e., we're working in three-dimensional space), and the dimensionality of an attention head, $d_{\\text{head}}$, is 2 (i.e., the head operates in a two-dimensional subspace).\n",
    "\n",
    "Suppose we have a matrix $W$ (of size 3x2) that projects vectors from the 3D space to the 2D subspace. The columns of $W$ form a basis for the subspace in which the attention head operates.\n",
    "\n",
    "Now, suppose we process some input vector with the attention head, resulting in a 2D output vector.\n",
    "\n",
    "To bring this back up to the full 3D space, we multiply it by the transpose of $W$ (which is of size 2x3). This gives us a 3D vector, which we'll call $v$.\n",
    "\n",
    "However, even though $v$ is a 3D vector, it still lives within the 2D subspace defined by the attention head. This is because $v$ is a linear combination of the columns of $W$, which form a basis for the 2D subspace.\n",
    "\n",
    "In other words, any vector that can be expressed as a linear combination of the columns of $W$ must lie within the subspace spanned by these columns. This means that, even though we've projected the output of the attention head back up to the full 3D space, it can't influence the residual stream outside of this 2D subspace.\n",
    "\n",
    "This is a general property of linear transformations: they preserve the structure of the vector space. In this case, a vector that starts in a certain subspace will stay in that subspace, even after it's been transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e965ff0-5946-457e-b15b-ec4939f47357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.arange(8).float().view(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba5f9a9-1a8b-44fb-9b57-f1703ae5d4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.],\n",
       "        [4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcea37db-e0e7-4bac-85f1-4d5024496fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = torch.randn(8).view(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb4cd62e-4509-45a9-8319-06cc080757ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333, -0.9169],\n",
       "        [-1.6866,  0.2643],\n",
       "        [-0.7698,  0.5458],\n",
       "        [ 0.8686,  0.4755]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a35441a-0e4d-4c72-99f4-ea06d6e10c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6205,  2.7822],\n",
       "        [-8.3048,  4.2565]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29274d13-ef13-4a80-8f01-a0f6f9ff7410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
