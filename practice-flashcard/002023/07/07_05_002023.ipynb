{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc69d4a-6d51-4cd9-b86a-c4b24581010b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16a80e7e-fb72-466b-b6df-1506c65f1123",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38462262-2ed2-42bf-8919-ec2ab40d8f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0448b2-edae-442f-b7eb-92e486bc70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef int age_type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e86e44-5e2f-425d-89b7-316d4adcab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "int* ptr = (int*)malloc(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda2c336-c610-4a3f-80bd-a3dcbded312a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6e3fa-ca81-418a-93b9-a6607337ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_decorator(func):\n",
    "    @functools.wraps\n",
    "    def wrapper():\n",
    "        pass\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3222b7b-9a92-4509-b5f5-8573f4ab4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset\n",
    "sync\n",
    "save\n",
    "restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b42742-6d39-4ac1-9ea5-3cc0350a1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "int* h_a, int* h_b, int* h_c;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa6acf-9ba9-4339-bbcd-fc0df78cf554",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_t bytes = size(int) * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b286bbe-c6fa-473f-91e1-670e065f504b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_decorator(func):\n",
    "    @functools.wraps\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56bb78a-5f11-4f56-8c6a-4a2e6c6e9f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08e89ac-8f3f-48af-8a43-bce3cedb11e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EvenSampler(Sampler):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return [self.data[i] for i in range(len(self.data), 2)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b80e63-9237-4715-ab6c-5b416f77fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "register memory > cache > ram > drive > external harddrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64b09f-3ba6-45e6-b0ac-42a0c3e43ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: load data\n",
    "step 2: idxs\n",
    "step 3: size(idxs)\n",
    "step 4: create a chunk in memory\n",
    "step 5: load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb32d96-9fe2-4294-8129-603b53e0e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_row_parallelism(inputs, weights):\n",
    "    inp_partrition_size = inputs.shape[-1] // 2\n",
    "    w_partrition_size = weights.shape[0] // 2\n",
    "    \n",
    "    inp1 = inputs[:, :inp_partrition_size]\n",
    "    inp2 = inputs[:, inp_partrition_size:]\n",
    "    \n",
    "    w1 = weights[:w_partrition_size, :]\n",
    "    w2 = weights[w_partrition_size:, :]\n",
    "    \n",
    "    out1 = inp1 @ w1\n",
    "    out2 = inp2 @ w2\n",
    "    \n",
    "    return out1 + out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c0028-2409-405d-87a5-92e3a67cb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message passing, collective communication, p2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf300e07-d9fa-4094-90e7-e5ba301e052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int* h_a, int* h_b, int* h_c;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936646ec-abfb-4cdd-8ca6-860d27dac4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_t bytes = sizeof(int) * n;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7453d-89d6-4ee7-8f59-9b350c606207",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a = (int*)malloc(bytes)\n",
    "h_b = (int*)malloc(bytes)\n",
    "h_c = (int*)malloc(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4dd79f1-275d-421a-8807-17837bd07e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5fe5ae-3e8a-4cd2-bce3-8ef1d78fe0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class User(BaseModel):\n",
    "    user_id: int\n",
    "    username: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f269d3eb-7d61-46d9-b02e-a25b194ff14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def by_column_parallelism(inputs, weights):\n",
    "    partrition_size = weights.shape[-1] // 2\n",
    "    w1, w2 = weights[:, :partrition_size], weights[:, partrition_size:]\n",
    "    out1 = inputs @ w1\n",
    "    out2 = inputs @ w2\n",
    "    return torch.cat([out1, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39361cd1-af8e-4f22-b135-6f042c602518",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: init epoch, procossed data\n",
    "step 2: get the set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f032-7c6b-4c43-b7ce-9f66aa2c6d66",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041166f8-dd89-4477-a45f-62b675b6927d",
   "metadata": {},
   "source": [
    "step 1: read, a linear transformation that selectively extract the subspace\n",
    "\n",
    "step 2: do computation\n",
    "\n",
    "step 3: write back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd95a21-bb9b-427e-8f08-8c04beaeae4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformerConfig, HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af4bcc-fab4-4cb5-997f-125b341a3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = HookedTransformerConfig(\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af91348-3b0c-453c-9e22-e008f0c8300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e811d2-fbb4-4eec-a61c-1bf303cac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prompt 1, prompt 2\n",
    "step 2: choose a component\n",
    "step 3: record all the interdimate activations\n",
    "step 4: move the activations\n",
    "step 5: compute logit difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac06e8-e04e-466d-a165-1ff774bb1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "A^T W_E + A^T W_E W_OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e9d46-4ca7-4cad-bbcb-b413d35f97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152bfe3-c737-4967-8860-85cb543c16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa68de9-b188-486a-a04b-f7f29ae161ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ed253-822b-4556-9b6b-6608f526e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_attributions = einsum(\n",
    "    model.W_E[: tokens[1:]]\n",
    "    embed[:-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c51c2-e8fb-4a3e-a004-1a207592ed80",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1443cada-52d8-418f-acb1-d6648a03e365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NoReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52670938-a7d4-447b-a1c5-35adcb2d4a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exit() -> NoReturn:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43accc8f-0eec-4d0f-8e30-3e944fb36912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4272d75-17e9-4446-8b7d-bc39cc162e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(BaseModel):\n",
    "    user_id: int\n",
    "    username: str\n",
    "    \n",
    "    \n",
    "    @validator(\"username\")\n",
    "    def uppercase_username(self, v):\n",
    "        return v.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fa6b1c3-8405-439b-a426-f2f17f1cf1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341554ae-2b9e-482d-8573-0080ec2da922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportLessThan(Protocol):\n",
    "    def __lt__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6e3c6-2eee-4a1c-991e-ef21fd442b8a",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22eda495-0306-467e-9027-f469ffd99fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e616720-0592-4243-b307-4e1736fcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split(x, [1, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35606e-4226-4ba7-a9e6-db60ffa0360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.register_forward_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2771e29-b997-4840-bc6d-91f7b5eda783",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_rref.rpc_sync().init(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ba2b2-3865-4bad-94d8-9632e9650bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc.rpc_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dc350-28b7-4112-9551-a36d13ec7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(repeated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730d375-d8c3-423a-a6f3-63643e67bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adee4e-d684-4c0d-ae7b-3ce9c88a9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx, layer_idx = 6, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f683c-06e5-4de4-bdc3-471685955a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_pattern = cache[\"attn\", layer_idx][:, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ec3f8-f9ed-4510-b30c-ea2e37b9e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_similarities = text_embeddings @ text_embeddings.T\n",
    "image_similarities = image_embeddings @ image_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98941b-6e9e-434a-9b50-e2521da16ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = F.softmax(\n",
    "    (text_similarities + image_similarties) / (temperature*2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba65c9ff-13ef-42f7-b066-128ffb81d014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.distributed.rpc import RRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb0d1d-349e-4f3b-a06b-5d07272aa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.id = RRef(self)\n",
    "    \n",
    "    def run_episode(self, agent_rref):\n",
    "        state, _ = self.env.reset()\n",
    "        \n",
    "        for _ in range(69):\n",
    "            action = agent_rref.rpc_sync().select_action(self.id, state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6083c32d-5db3-46c0-91ef-95b73bf4c946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.id = RRef(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "570d231d-1640-4e9b-853e-1be17012945d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, n_hidden):\n",
    "        super().__init__()\n",
    "        self.actor_network = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.critic_network = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        logits = self.actor_network(observations)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample() \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        critic_value = self.critic_network(observations)\n",
    "        \n",
    "        return action, log_prob, entropy, critic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55993-a24c-4531-ae6f-34013c8734ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b5632-69d1-4258-9ef0-810520d4b762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab90a4f-9e0e-4b9c-9cbf-11957472f3f6",
   "metadata": {},
   "source": [
    "This passage is discussing a theoretical interpretation of transformer models' architecture, focusing particularly on the role and operation of the attention mechanism. To break it down:\n",
    "\n",
    "\"Products of attention heads behave much like attention heads themselves.\" This is saying that the outputs (or products) of multiple attention heads, when combined, behave similarly to a single attention head. The attention heads in a transformer model operate in parallel, each calculating its own attention weights and applying them to the input data. The result of these operations can be combined or multiplied, and the outcome will still exhibit the characteristics of an attention mechanism. This essentially reflects the distributive property of the attention mechanism, meaning that the order in which operations are performed does not affect the final outcome.\n",
    "\n",
    "\"$\\left(A^{h_2} \\otimes W_{O V}^{h_2}\\right) \\cdot\\left(A^{h_1} \\otimes W_{O V}^{h_1}\\right)=\\left(A^{h_2} A^{h_1}\\right) \\otimes\\left(W_{O V}^{h_2} W_{O V}^{h_1}\\right)$\". This is a mathematical expression of the point above. Here, $A^{h_1}$ and $A^{h_2}$ are the attention weights calculated by two different attention heads, and $W_{O V}^{h_1}$ and $W_{O V}^{h_2}$ are the transformations applied to the inputs by these heads. The left-hand side of this equation is the operation of two separate attention heads, and the right-hand side is the combined operation of these heads. The equality of these two sides demonstrates the distributive property of the attention mechanism.\n",
    "\n",
    "\"The result of this product can be seen as functionally equivalent to an attention head, with an attention pattern which is the composition of the two heads $A^{h_2} A^{h_1}$ and an output-value matrix $W_{O V}^{h_2} W_{O V}^{h_1}$.\" This is the conclusion drawn from the above points. The combined operation of two attention heads can be seen as the operation of a single attention head, where the attention pattern (weights) and the transformation applied to the input are the compositions of those used by the individual heads. This combined operation is functionally equivalent to a single attention head.\n",
    "\n",
    "\"We call these 'virtual attention heads', discussed in more depth later.\" The term \"virtual attention heads\" is introduced here to refer to these composite operations. These are not physical heads in the model but rather a theoretical concept to understand the overall effect of the operation of multiple attention heads. The discussion indicates that this concept will be explored further in later sections of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810b5c7-c8db-4421-a715-ea2e51730b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64f33a-92be-4d83-80ec-65ffd230c1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ee7b0-8bff-4dc1-9a14-36f0c1790ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a759d76e-073f-4f52-bb15-bb9689dce526",
   "metadata": {},
   "source": [
    "Sure! Here is how we can rewrite attention using tensor products:\n",
    "\n",
    "Typically, attention is computed as:\n",
    "\\begin{align*}\n",
    "\\operatorname{Attention}(Q, K, V)&=\\operatorname{softmax}\\left(QK^T\\right)V\\\n",
    "&=\\left[\\begin{array}{ccc}\n",
    "q_1 & q_2 & \\dots \\\n",
    "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
    "k_1 & k_2 & \\dots \\\n",
    "\\end{array}\\right]^T\\left[\\begin{array}{c}\n",
    "v_1\\\n",
    "v_2\\\n",
    "\\dots\n",
    "\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "We can replace $Q, K,$ and $V$ with their definitions in terms of the model inputs $x$:\n",
    "\\begin{align*}\n",
    "Q &= W_Qx\\\n",
    "K &= W_Kx\\\n",
    "V &= W_Vx\n",
    "\\end{align*}\n",
    "Plugging this in, we have:\n",
    "\\begin{align*}\n",
    "\\operatorname{Attention}(W_Qx,W_Kx,W_Vx)&=\\operatorname{softmax}\\left(x^T W_Q^T W_K x\\right) W_V x\\\n",
    "&= \\left(A\\otimes W_O W_V \\right) \\cdot x\n",
    "\\end{align*}\n",
    "Where $A = \\operatorname{softmax}\\left(x^T W_Q^T W_K x\\right)$ and $W_O = W_V$.\n",
    "\n",
    "So we've shown that attention can indeed be written as a tensor product of a softmax attention pattern $A$ and an output/value matrix $W_O W_V$, applied to the input $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52402b51-3a22-4091-ba12-a45fb7bd3462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92644b41-a915-45a2-b177-e906a4030662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2086ec0d-f678-4cf6-b52f-5f3ca3317283",
   "metadata": {},
   "source": [
    "Sure, here are the steps to rewrite attention as a tensor product:\n",
    "\n",
    "Express $Q, K,$ and $V$ in terms of the model input $x$:\n",
    "\\begin{align*}\n",
    "Q &= W_Qx\\\n",
    "K &= W_Kx\\\n",
    "V &= W_Vx\n",
    "\\end{align*}\n",
    "\n",
    "Substitute these into the attention equation:\n",
    "\\begin{align*}\n",
    "\\operatorname{Attention}(W_Qx, W_Kx, W_Vx)&=\\operatorname{softmax}\\left(\\frac{x^T W_Q^T W_K x}{}\\right)W_Vx\\\n",
    "&=\\left(A\\otimes W_O W_V\\right) \\cdot x\n",
    "\\end{align*}\n",
    "Where $A = \\operatorname{softmax}\\left(x^T W_Q^T W_K x\\right)$ and $W_O=W_V$.\n",
    "\n",
    "Recognize that $A$ mixes information across context indices, while $W_O W_V$ acts independently on each vector. So we can write attention as:\n",
    "\\begin{align*}\n",
    "h(x)&=\\left(A \\otimes W_O W_V\\right) \\cdot x\n",
    "\\end{align*}\n",
    "\n",
    "Note that we usually compute an attention pattern by computing keys $k_i=W_Kx_i$, queries $q_i=W_Qx_i$ and taking a softmax of their dot product. But we can collapse this into a single step:\n",
    "\\begin{align*}\n",
    "A&=\\operatorname{softmax}\\left(x^T W_Q^T W_K x\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Finally, recognize that although mathematically equivalent, computing attention this way (multiplying by $W_O W_V$ and $W_Q^T W_K$) would be very inefficient in practice. We derive this formulation mainly for theoretical understanding.\n",
    "\n",
    "So in summary, the steps are:\n",
    "\n",
    "Express $Q, K$ and $V$ in terms of $x$\n",
    "Substitute into attention equation\n",
    "Recognize $A$ and $W_O W_V$ as mixing across context and acting per vector\n",
    "Collapse key/query computation into a single step\n",
    "Note this formulation is mathematically equivalent but computationally inefficient\n",
    "Does this help explain the process? Let me know if you have any other questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae500ba6-0c39-49aa-9715-a5a2971f8be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
