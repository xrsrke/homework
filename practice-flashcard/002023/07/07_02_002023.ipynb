{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fd735c-0f2f-46dc-b887-842b895e54b2",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fe649-105d-4b65-8650-1a0bde1c6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = final_residual_stream @ unembed\n",
    "\n",
    "\n",
    "logits = (embed + layer_1 + layer_2) @ unembed\n",
    "logits = embed @ unembed + layer_1 @ unembed + layer_2 @ unemebed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b91bad-a180-41b5-a130-305989387fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf0a10-06f4-4a4a-9c85-4709866bc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [model.register_forward_pre_hook(x) for x in hooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d784e5-b787-4e96-9376-fc99a2a12b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles[1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b08f48-6e4d-4acf-82a0-38d698f5fd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23a6d8b-7197-443e-8155-32e78d512aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def use_hooks(model, hooks):\n",
    "    try:\n",
    "        module = model.blocks[1]\n",
    "        handles = [module.register_forward_pre_hooks(h) for h in hooks]\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd19ddd-fab7-4010-820c-a826b6e2b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3deeb-580d-43bf-84ef-9a8a7602595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f997be14-0db0-49ce-8a77-442f8161c0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59807ce5-da0f-420f-922d-6d5b164f3c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"pattern\", 0, \"attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91de231f-0e7f-4ace-a2dc-ed1a998a4a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_pattern = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f366de3-78d5-4c7e-ab91-51d73b036011",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = model.to_str_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04742d99-bd44-41c7-825b-8824b7245075",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831c0cb-b42d-455f-8bb8-5a3204bf6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96310592-38cf-4e37-a185-a0d05d5181c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42732a-d704-4074-940a-69a9c2f53d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede76e91-0428-4a67-9aca-c3b3c7e02696",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_max = torch.argmax(activations[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aaac1-f5c7-4351-ac45-76bc9003899f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1ae175-74ff-4d69-9d8a-4acc16b2c38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, neuron_idx = 3, 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308edd72-4a0c-4ec2-95e8-9821abda7044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96720ac-2b57-4f06-84ad-df80afd00bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_neuron_activation(activations, hook):\n",
    "    data[neuron_idx] = activations[0, :, neuron_idx]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87305ce0-360e-42c5-86ec-afd0988f8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03007b18-a6bc-4d85-a938-a320f8840d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ca19a-7192-4a86-9f19-145c25fa13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: extract unembed[correct]\n",
    "step 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bd42e-b3d7-4a0a-af37-d80323b1af5f",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8baabcd2-6531-4670-8432-c469109cc916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd47885-d514-4e6c-8ecb-65c8faad10fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e0a3db-f178-4cce-a4a4-5267cb319d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event = threading.Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd3f348-da27-4139-882c-6be707422bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_worker():\n",
    "    print(\"waiting\")\n",
    "    event.wait()\n",
    "    print(\"received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7a634-bc7a-4dae-8dcf-e382605f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_thread = threading.Thread(run_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700fea4-2cb8-46bb-b798-11ec1f90bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0f39b-103c-4fd6-8d98-08a13f453f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    int x = 1;\n",
    "    std::cout << &x;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f72b14-aae2-4dac-94c9-1d3b4311e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Number() {\n",
    "    public:\n",
    "        int value;\n",
    "    \n",
    "        int isLargerThanZero() {\n",
    "            if (value > 0) {\n",
    "                return 1;\n",
    "            } else {\n",
    "                return 0;\n",
    "            }\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4082d7-94c9-47c6-9e83-9a0e2ad52980",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "    const pi = 3.14;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caec8c-9407-4e45-8704-ffdf69e804a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global memory > shared memory > register memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f3ec2-c9b8-4b34-98a3-5e863d388220",
   "metadata": {},
   "outputs": [],
   "source": [
    "using name std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebc777-ce57-4a60-afe7-d3ff7357e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "string say_name(string name) {\n",
    "    std::cout << name;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a3a94-010f-4188-8f19-695c501044f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "void say_hello() {\n",
    "    std::cout << \"hello\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856fcece-a5b4-4e75-b464-8f4bd1e298f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_microbatches = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47e6f1e-defe-46be-a6df-01c62d738580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_partritions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55be8c65-5850-40d2-bbae-47ffa97405e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_clock_cycles = n_microbatches+n_partritions-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22aca76-db61-4239-bac7-c86dcdec0826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_clock_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4757f965-ac38-4a3d-bc91-0f7ba66ef987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n",
      "[(1, 0), (0, 1)]\n",
      "[(2, 0), (1, 1), (0, 2)]\n",
      "[(3, 0), (2, 1), (1, 2)]\n",
      "[(3, 1), (2, 2)]\n",
      "[(3, 2)]\n"
     ]
    }
   ],
   "source": [
    "for clock_idx in range(total_clock_cycles):\n",
    "    start_partrition = max(clock_idx+1-n_microbatches, 0)\n",
    "    end_partrition = min(clock_idx+1, n_partritions)\n",
    "    \n",
    "    tasks = []\n",
    "    for partrition_idx in range(start_partrition, end_partrition):\n",
    "        microbatch_idx = clock_idx - partrition_idx\n",
    "        tasks.append((microbatch_idx, partrition_idx))\n",
    "    \n",
    "    print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0c880e-2368-44ac-8cbc-c16ed1386d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, input_leaf = ctx.recomputed.pop()\n",
    "        input_leaf = input_leaf.detach().requires_grad_(\n",
    "            input_leaf.requires_grad\n",
    "        )\n",
    "        \n",
    "        grads = [None, None, None]\n",
    "        \n",
    "        if input_leaf.requires_grad:\n",
    "            torch.autograd.backward(input_leaf, grad_output)\n",
    "            grads.extend([input_leaf.grad])\n",
    "        else:\n",
    "            grads.extend([None])\n",
    "        \n",
    "        return tuple(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a16efb-11c3-402b-842b-4118610dcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiting for the minimum number of nodes that requires for the training task. and runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b1aae75-35c0-4911-9c74-b2630d122bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socketserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c46aad-6adb-46b5-a864-ac7cdb3c04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with socketserver.ThreadingTCPServer(\n",
    "    (MASTER_HOST, MASTER_PORT),\n",
    "    EchoRequestHandler\n",
    ") as server:\n",
    "    server.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d531c946-7450-4915-951c-ae19df2e82f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            # GPU waits for GPU\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            # GPU waits for CPU\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4609c9c6-a70e-44cd-88e5-308e5b1e3fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wait(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        wait_stream(\n",
    "            source_stream=next_stream,\n",
    "            target_stream=prev_stream\n",
    "        )\n",
    "        \n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        waif_stream (\n",
    "            source_stream=ctx.prev_stream,\n",
    "            target_stream=ctx.next_stream\n",
    "        )\n",
    "        return tuple([None, None] + grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef72fd1-0fbd-49c1-a5ff-ef29a43f0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        return phony\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        input = ctx.input\n",
    "        function = ctx.function\n",
    "        input_leaf = input.detach().requires_grad_(\n",
    "            input.requires_grad\n",
    "        )\n",
    "    \n",
    "        with torch.grad_enabled():\n",
    "            output = function(input)\n",
    "        \n",
    "        grads = [None, None, None]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7af96-1957-48d2-aed6-1b17317cbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoRequestHandler(socketserver.StreamRequestHandler):\n",
    "    def handle(self):\n",
    "        print(self.rfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "312723c7-9b1a-4609-8ebf-0d374350be96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad836c4-c3ec-4620-a50b-97e0544728f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b321479c-6c68-4156-bd88-93a8a1c80e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46007a07-04e9-4387-ad3c-69c7c60e1566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_neuron_activations(activations, hook):\n",
    "    data[neuron_idx] = activations[:, :, neuron_idx]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68f39d-ed49-4679-bdbe-3a1bbad02616",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.run_with_cache(\n",
    "    tokens,\n",
    "    fwd_hooks=[(hook_name, extract_neuron_activations)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9e969-20a7-41b9-b240-e5cab65d4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_token = torch.argmax(data[neuron_idx], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6883ef-757a-43cd-a3fb-6e1950d97a37",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13ec2b93-f50d-4ea0-8817-53af9deefb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1f210-2d39-4dcd-b8ec-9b906d05a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: script\n",
    "step 2: peri\n",
    "step 3: compare\n",
    "step 4: return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eda351-c023-42b8-a3c8-12bd57f324e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        \n",
    "        return phony\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        input = ctx.input\n",
    "        function = ctx.function\n",
    "        input_leaf = input.detach().requires_grad(\n",
    "            input.requires_grad\n",
    "        )\n",
    "        \n",
    "        with torch.grad_enabled():\n",
    "            output = function(input)\n",
    "        \n",
    "        grads = [None, None, None]\n",
    "        \n",
    "        if input_leaf.requires_grad:\n",
    "            grads.extend([input_leaf.grad])\n",
    "        else:\n",
    "            grads.extend([None])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd13808-510a-4b90-ba12-c8b9d758db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore, sync, commit, reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cfbf8-7198-4d2d-bc9c-6c3959e2dce4",
   "metadata": {},
   "source": [
    "checkpoint.forward() > recompute.forward() > recompute.backward() > checkpoint.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a51ca1-cae5-4b85-818f-50312baf2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main thread > worker thread > task > cuda stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8476f071-0101-434e-886e-130d273b64f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.input = input\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return ctx.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4679-4af2-4c89-82e1-5cfbd475fd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d596d-1db9-40db-8cfe-5faf72c824f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a648e76-c6b6-4ace-bfcf-882543a10f57",
   "metadata": {},
   "source": [
    "The full circuit matrix $W_E W_{O V}^{h_1} W_{Q K}^{h_2} W_E^T$ can be thought of as a bilinear form that provides insight into the composition of two attention heads, one from an earlier layer $h_1$ (the output-value or OV head) and one from a later layer $h_2$ (the query-key or QK head). The matrix itself is $d_{vocab} \\times d_{vocab}$ in dimension, which in practical terms can be extremely large and computationally intensive to calculate.\n",
    "\n",
    "Let's break down each part of the full circuit matrix.\n",
    "\n",
    "$W_E$: This is the embedding matrix that transforms one-hot encoded token vectors into the model's embedded representation space. This is a linear operation that has shape $(d_{vocab}, d_{model})$. The operation $A^T W_E$ would give you the embedding vector of a specific token A.\n",
    "\n",
    "$W_{O V}^{h_1}$: This is the output-value circuit of the earlier attention head $h_1$. It represents the function that this attention head applies to transfer information from the source token to the destination token in the residual stream. The operation $A^T W_E W_{OV}^{h_1}$ gives the vector that would be written to the residual stream at the destination position, if the destination token only pays attention to A.\n",
    "\n",
    "$W_{Q K}^{h_2}$: This is the query-key circuit of the later attention head $h_2$. It represents the function that this head applies to determine which tokens attend to which other tokens in the residual stream. This bilinear form determines the attention scores.\n",
    "\n",
    "$W_E^T$: This is the transpose of the embedding matrix, transforming from the embedded space back into the one-hot token space.\n",
    "\n",
    "The resulting matrix, $W_E W_{O V}^{h_1} W_{Q K}^{h_2} W_E^T$, then represents an overall circuit describing where information is moved to and from in head $h_2$, given that the query-side vector is formed from the output of head $h_1$. This is an instance of Query-Composition. Specifically, for two one-hot encodings for tokens A and B, $A^T W_E W_{OV}^{h_1} W_{QK}^{h_2} W_E^T B$ represents the attention score paid to token B by any token which attended strongly to an A-token in head $h_1$. This process helps understand the interaction between two different attention heads in subsequent layers of the model.\n",
    "\n",
    "In reality, because of the vast vocabulary size in models like GPT-3 or GPT-4, these matrices can be enormous and difficult to work with directly. Therefore, finding efficient ways to analyze these matrices, such as approximations or sampling methods, is an essential part of understanding these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7734c1-4d50-4766-a633-85e45cb4a2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf66eb4-8230-4ddf-ac10-b95d4747b120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07848dc-03ca-4e5b-9013-80614240a543",
   "metadata": {},
   "source": [
    "The equation $A^T W_E W_{O V}^h$ describes the transformation of a token to a vector that would be written to the residual stream at the destination position if the destination token only pays attention to token A.\n",
    "\n",
    "Breaking this down:\n",
    "\n",
    "$A^T$: This is the transposed one-hot encoded vector representation of token A. A one-hot vector has a dimension equal to the size of the vocabulary and has all zeros except a single one at the index that corresponds to the specific token in the vocabulary. So, $A^T$ is a row vector.\n",
    "\n",
    "$W_E$: This is the token embedding matrix. It is a learned matrix that converts one-hot encoded tokens into continuous vector representations or embeddings. The size of $W_E$ is $(d_{\\text{vocab}}, d_{\\text{model}})$ where $d_{\\text{vocab}}$ is the vocabulary size and $d_{\\text{model}}$ is the dimension of the model. When we multiply $A^T$ with $W_E$, we get the embedding of token A.\n",
    "\n",
    "$W_{O V}^h$: This is the OV circuit for head h. It is a learned matrix that dictates what information gets moved from source to destination, in the residual stream. It is the product of the output and value weight matrices for a specific attention head h. The size of $W_{O V}^h$ is $(d_{\\text{model}}, d_{\\text{model}})$.\n",
    "\n",
    "Therefore, the expression $A^T W_E W_{O V}^h$ gives us the vector which would get written to the residual stream at the destination position, if the destination token only pays attention to A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb4651-6ea8-427d-bb4d-2505f90b8678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51eac0-943c-4ae6-9cc3-590794b4a6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35cfec3-d31a-4947-8741-6fe91328edc5",
   "metadata": {},
   "source": [
    "$A^T W_E W_{Q K}^h$ is the query for token A.\n",
    "\n",
    "$(W^h_K)^TW_E^TB$ can be rewritten as $(B^TW_EW^h_K)^T$, which is the key for token B.\n",
    "\n",
    "Explain $A^T W_E W_{O V}^{h_1} W_{Q K}^{h_2} W_E^T B$\n",
    "\n",
    "This represents the attention score paid to token B, by any token that strongly attended to token A in head $h_1$. Head $h_1$ first incorporates information from token A, then head $h_2$ attends to the output of head $h_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53ec50-24de-4e80-afb6-cf87bbacd12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2c8b4-34cb-402d-8948-1142e9a1c2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3418c74b-117f-464e-ab4f-04dec02d30a1",
   "metadata": {},
   "source": [
    "Sure, let's break it down further in a simpler and more concrete way.\n",
    "\n",
    "Question: What does the output of $A^T W_E W_{Q K}^h W_E^T B$ represent?\n",
    "\n",
    "Answer: This equation calculates the attention score that one token (A) pays to another token (B) in the context of a specific attention head (h).\n",
    "\n",
    "Let's unpack this:\n",
    "\n",
    "$A^T W_E$ is the query. It's the embedding of token A. Essentially, this represents token A in the model's \"language.\"\n",
    "\n",
    "$W_{Q K}^h$ represents the attention mechanism for a specific head (h). It's telling us how this particular head calculates attention scores between pairs of tokens.\n",
    "\n",
    "$(W_K^h)^T W_E^T B$ is the key. It's the embedding of token B, transformed by the key weights of head h. This represents how relevant token B is when we're considering what token A should pay attention to.\n",
    "\n",
    "So, when you multiply these all together: $A^T W_E W_{Q K}^h W_E^T B$, it represents the attention score that token A assigns to token B for this particular attention head (h).\n",
    "\n",
    "Now let's give a concrete example. Let's say we have a sentence of five words: \"I like to play games\". Each word is a token. If we use this formula and replace A with 'I' and B with 'play', the formula will give us the attention score that the token 'I' assigns to the token 'play'. Higher score means 'I' is paying more attention to 'play' when it's processing information.\n",
    "\n",
    "Remember, the final output of the model is the result of many such attention scores, from many attention heads, across all pairs of tokens. This is just a look at one specific piece of that larger process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77ac4-fa50-4a5f-9cae-fe5356c0c5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
