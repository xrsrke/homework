{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50260b79-ded3-4a9e-bf69-b172356dfd07",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5809cdad-532a-4f43-909e-d5793282fbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5792a59d-47f8-404e-bd95-7be8ad4e995d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_model_parallel_size = 2\n",
    "pipeline_model_parallel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4835b849-fed6-4bde-b021-12a4fad90338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fa8b4b-1c4b-4aab-9353-140aef660550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "[1, 3]\n",
      "[4, 6]\n",
      "[5, 7]\n",
      "[8, 10]\n",
      "[9, 11]\n",
      "[12, 14]\n",
      "[13, 15]\n"
     ]
    }
   ],
   "source": [
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_rank = i*num_pipeline_model_parallel_groups\n",
    "    end_rank = (i+1)*num_pipeline_model_parallel_groups\n",
    "    \n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(\n",
    "            start_rank+j,\n",
    "            end_rank,\n",
    "            tensor_model_parallel_size\n",
    "        ))\n",
    "        \n",
    "        print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e37534-d2b0-42fb-b14c-7a1b2a6b62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74254871-0169-484b-a6c1-f7f42b22dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_per_rank = [0 for _ in range(world_size)]\n",
    "param_per_rank = [[] for _ in range(world_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18734811-2075-49f8-8e59-2bd4413303c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_param = sort(param_list, key=lambda f: f.numel(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0527518-b737-4849-83fd-44bf8da8bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in sorted_param:\n",
    "    next_rank = min(numel_per_rank) # return the idx of the smallest rank\n",
    "    param_per_rank[next_rank].append(param)\n",
    "    numel_per_rank[next_rank] += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fd4ff-18b5-49bd-8f37-9f6ca37e543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment, configmap, services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d89da-0ab1-42a3-bed0-c65aad02a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubelet, kube-proxy, container runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d305b-7139-4733-8b99-d950a0071e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "process\n",
    "thread\n",
    "vectorization\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145cf49b-d9ec-479b-bdeb-7319beb7474e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socketserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e77daf-5afa-4bea-852b-24485dbbb57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EchoRequestHandler(socketserver.StreamRequestHandler):\n",
    "    def handle(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68ba03-c3db-4ec3-8798-6ee430463cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start of a batch: num of processed data\n",
    "end of a batch: num of processed data\n",
    "start of an epoch: repartritoni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a91b03f-55d7-462e-946d-593497365228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9544f1de-ade4-4e7f-b245-8aaf8b91982b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda1b4ca-d6cd-4e6b-8229-f8e2095e2b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_worker():\n",
    "    with lock:\n",
    "        print_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cdbac-76f1-4fb1-a576-8671d656a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = threading.Thread(target=run_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a98ad7-29ed-4ad7-8c89-d14ce3b9e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "j-partrition on j device\n",
    "forward(m, n) is executed before m+1, n\n",
    "backward(m, n) is executed before m-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8790ee03-1017-4ed8-807c-df9cc2446fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f567a03-4cf5-42c9-9810-ffb1db198fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_tensors(x):\n",
    "    world_size = torch.distributed.get_world_size()\n",
    "    xs = [torch.zeros_like(x) for _ in range(world_size)]\n",
    "    \n",
    "    torch.distributed.all_gather(xs, x)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15629d-e470-4068-b8df-c5930d2c5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x == nullptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92765060-557f-4b9f-a758-cb407aa941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: load data\n",
    "step 2: idxs\n",
    "step 3: size(idxs)\n",
    "step 4: create a chunk of memory\n",
    "step 5: load(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed99f5-b907-471a-a916-f8843de5ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = torch.distributed.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32b207-cfaf-41ac-8b47-d8126fff4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "    torch.distributed.broadcast(x, src=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe923e-e8f8-486c-936c-2cc948a1768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message passing, p2p, collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d357c24-60fd-46e1-a60c-7d42fd1ee01e",
   "metadata": {},
   "source": [
    "checkpoint.forward() > recompute.forward() > recompute.backward() > checkpoint.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ce6fd-6016-408e-b351-64af2ce51c31",
   "metadata": {},
   "source": [
    "clock cycle 1: backward(4, 3)\n",
    "clock cycle 2: backward(3, 3), backward(4, 2)\n",
    "clock cycle 3: backward(2, 3), backward(3, 2), backward(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebee0f0-d976-4d52-ad7d-85b4cae6bfaa",
   "metadata": {},
   "source": [
    "step 1: cpu memory\n",
    "step 2: gpu memory\n",
    "step 3: cpu -> gpu\n",
    "step 4: computation\n",
    "step 5: gpu -> cpu\n",
    "step 6: gpu memory dellocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c0c75-2aa8-4eb0-9790-8a72bab839f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: residual = embed(tokens) + unembed(tokens)\n",
    "step 2: residual = blocks(residual)\n",
    "step 3: residual = final_ln(residual)\n",
    "step 4: logits = unembed(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83125dea-96c7-4dfb-a7ce-c8ceb53a1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: ln_1 = ln_1(resid_pre)\n",
    "step 2: head_outputs = head_1(ln_1) + head_12(ln_1)\n",
    "step 3: mid_resid += head_outputs @ resid_pre\n",
    "step 4: ln2 = ln2(mid_resid)\n",
    "step 5: mlp = mlp(ln2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7398cac7-0f81-43c9-a60c-77694fd2e3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd64c8d-44f9-4644-8130-5309814c7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sidebar.header(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd98ff-523f-4152-8acb-50f3b6bae7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    layer_idx = st.sidebar.slider(\"layer\", 0, 12, 5)\n",
    "    head_idx = st.sidebar.sidebar(\"head\", 0, 12, 4)\n",
    "    return layer_idx, head_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c0dad-2114-461f-bb96-72bf23f8ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx, head_idx = func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56042c2-9472-4f9e-b103-a5cec12398e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca75f12-8901-4d47-8ef0-d74f10caa632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbbf62-607f-4c17-9e59-88d4cab64c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens,\n",
    "    names_filter=lambda x: x.endswith(\"mlp_out\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a814f2d-c0ae-4e8e-8d56-94226eb2131b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e512cdf-cd2a-4366-a30b-7718c2319df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = product(range(n_heads), range(n_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27e052-608c-4a2d-aab7-23ce9159b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pos = model.W_pos\n",
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "W_K = model.W_K[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527dc9-c8a6-40a4-93a8-1325bbf2d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK_circuit = W_K @ W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23563bb7-1f22-4030-9616-04661da3c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_scores = W_pos @ QK_circuit @ W_pos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87da7d9a-c864-48ba-880f-a6acd97a7b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask(scores):\n",
    "    masks = torch.tril(torch.ones_like(scores)).bool()\n",
    "    neg_inf = torch.tensor(-1e6)\n",
    "    return torch.where(masks, scores, neg_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdfae5-93f9-47b7-bcfe-3a2ba537f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_masked = mask(\n",
    "    pos_by_pos_scores/(d_head**0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed9e19-49a8-4078-9b99-6db993d490c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_pattern = F.softmax(pos_by_pos_masked, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b94320-46bc-4a01-8ff1-6907b69498ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "A^T@W_E + A^T@W_E@W_OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d8022-ffef-4050-a572-2e48801cc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = product(range(n_layers), range(n_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2e26da-25d9-4487-8e4b-ad915216d042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7418ae9-6ed4-4e15-b54d-859434c32c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = FactoredMatrix(A=A, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16688ea7-df6f-425a-afc2-fab0c1e39305",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pos = model.W_pos\n",
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "W_K = model.W_K[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb50e7-a222-4914-95ce-3271d4404388",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK_circuit = W_K @ W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb11035-7ef1-4660-868a-4770aaff56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_score = W_pos @ QK_circuit @ W_pos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3f770-f205-470d-8a1f-ce664c7d9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(scores):\n",
    "    masks = torch.triu(torch.ones_line(scores)).bool()\n",
    "    neg_inf = torch.tensor(-1e9)\n",
    "    return torch.where(masks, scores, neg_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffafb63-f35d-4ee2-b8fb-cfe4af6e8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_pattern = F.softmax(\n",
    "    mask(pos_by_pos_score / 0.5**d_head)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24be409-a0e1-4e54-a46c-d6b7d7a2a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28ae82-f44e-46ed-916a-611592e0fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_residual_direction = W_U[:, correct_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c53288-4e06-4343-b049-e0469e721a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_residual_direction = W_U[:, incorrect_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e490a-335f-46e5-a96e-f9b593757674",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_difference_direction = correct_residual_direction - incorrect_residual_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b14949-7ea7-4292-9a13-cae029c0bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10390f-991e-4839-b608-2063cafbbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_residual_direction = W_U[:, correct_token]\n",
    "incorrect_residual_direction = W_U[:, incorrect_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbd22f-fc35-469d-bd59-7b72de71579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff_direction = correct_residual_direction - incorrect_residual_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d900abef-a359-4a4b-afc7-82276d237b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7b1c5-efe6-460b-8998-2b1c6013f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = model.W_O[layer_idx, head_idx]\n",
    "W_V = model.W_V[layer_idx, head_idx]\n",
    "\n",
    "W_E = model.W_E\n",
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751f085-9668-4cca-9f57-1037511378ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "OV_circuit = FactoredMatrix(A=W_V, B=W_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342e244-6cd8-4f7b-8b85-97dc0a67f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_OV_circuit = W_E @ OV_circuit @ W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1e5fa-136e-447d-99ac-aee34322a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbb352-f4b2-4e43-afcd-3d2b9d38fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350c7ec-3c66-4368-a1d4-aa2b5dd804b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad465a47-16ab-43ff-b0a9-fbdc47bd809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: residual = embed(tokens) + pos_embed(tokens)\n",
    "step 2: residual = blocks(residual)\n",
    "step 3: residual = fn_ln(residual)\n",
    "step 4: logits = unembed(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743476b6-0f97-4c78-9cac-1857040e3806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f20a9-8286-4fbd-987f-54bf7276d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfbed7-365e-4f18-b821-de1086edab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_residual_direction = W_U[:, correct_token]\n",
    "incorrect_residual_direction = W_U[:, incorrect_residual_direction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fa447-e705-4cbe-99d4-ba2479688639",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_difference_direction = correct_residual_direction - incorrect_residual_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b26a2b-8b5b-4883-a3c7-acf1827c4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9aa1b1-8121-4af1-8f7d-49e09082d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_residual = cache[final_residual_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197b920-6900-43ce-ab5e-d43b7aac47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_final_residual = final_residual[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933aef40-8236-43af-ba51-5a7dcc74840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_last_token_final_residual = model.apply_ln_to_stack(\n",
    "    last_token_final_residual,\n",
    "    layer=-1,\n",
    "    pos_slice=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6198dfb-374f-4cdf-8fc9-8060ed5043c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a19c6-3a25-489b-b2ee-ff4cfc07ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_difference = einsum(\n",
    "    scaled_last_token_final_residual\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3a960-2ed0-4745-a1e1-1d3f24574519",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5d4ce-38b3-4b66-9c0f-c171d1570d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf009d4-da92-47b9-a579-b80196c80128",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "unembed = cache[\"embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b51a6-5107-4c54-81ef-7e5d19e7c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_outputs = cache[\"result\", prev_layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f41f1-f6ef-4fda-a423-20c471d6ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = torch.cat([embed, unembed, head_outputs], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d623362-14b7-42bc-81b2-82c13d64c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[next_layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5a0bc-4848-4612-bf92-fde0b69a34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_components = einsum(\n",
    "    components,\n",
    "    W_Q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2b473-5088-4a27-a675-5fd5169ad566",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_contributions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66733050-211b-4e90-b0ec-f35bba963393",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e54593-6122-4b59-a171-24a4e2c18cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: ln_1 = ln_1(resid_pre)\n",
    "step 2: head_output = sum_heads(resid_pre)\n",
    "step 3: mid_resid = resid_pre + head_output\n",
    "step 4: ln_2 = ln_2(mid_resid)\n",
    "step 5: mlp = mlp(ln_2)\n",
    "step 6: post_resid = mlp + mid_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa462b-e7f8-4bfd-a8e2-2a92532cc67e",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceb264-65ba-4c60-89e8-bb95a1151493",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144deb7-d860-4a99-9b3b-5a5321177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.input = input\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return ctx.input + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e64c89-76b1-4273-9918-a73d604ebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: ln1 = ln1(resid_pre)\n",
    "step 2: head_outputs = head1(ln1) + head2(ln2)\n",
    "step 3: mid_resid = resid_pre + head_outputs\n",
    "step 4: ln2 = ln2(mid_resid)\n",
    "step 5: mlp = mlp(ln2)\n",
    "step 6: post_resid = mlp + mid_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cb222-34fe-4a90-9dd7-c2d1a64eecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: residual = embed(tokens) + unembed(tokens)\n",
    "step 2: residual = blocks(residual)\n",
    "step 3: residual = ln_final(residual)\n",
    "step 4: logits = unembed(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9864ffb-e2f2-4bcc-af09-3d98ae2003f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26cd78-3499-4f2a-9468-2536be888e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln, linear, masked, non-masked, skip conenction, ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aef63-0421-4c14-b4e6-bc841e0b216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model)\n",
    "        self.to_k = nn.Linear(d_model, d_model)\n",
    "        self.to_v = nn.Linear(d_model, d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        bs, seq_len, d_model = x.size()\n",
    "        return x.view(bs, self.n_heads, seq_len, self.d_head)\n",
    "    \n",
    "    def forward(self, pre_q, pre_k, pre_v):\n",
    "        q = self.to_q(pre_q)\n",
    "        k = self.to_q(pre_k)\n",
    "        v = self.to_q(pre_v)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6942b1c-2dd8-4ff9-869f-f066d5e71001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42267da4-b7cb-4f58-b2ba-df407dabe38f",
   "metadata": {},
   "source": [
    "Here is a step-by-step explanation of how to decompose attention scores, with a concrete example:\n",
    "\n",
    "We start with the standard attention score formula:\n",
    "$score_{ij} = Q_i \\cdot K_j$\n",
    "\n",
    "Where $Q_i$ is the i-th query vector and $K_j$ is the j-th key vector.\n",
    "\n",
    "Instead of having a single $Q_i$ and $K_j$ vector, we split them up into multiple components. So we have:\n",
    "$Q_i = q_0 + q_1 + ... $\n",
    "$K_j = k_0 + k_1 + ...$\n",
    "\n",
    "Where $q_0, q_1, ...$ are the different query components and $k_0, k_1, ...$ are the different key components.\n",
    "\n",
    "We then calculate the attention score for each pair of components:\n",
    "$score_{ij}^{00} = q_0 \\cdot k_0$\n",
    "\n",
    "$score_{ij}^{01} = q_0 \\cdot k_1$\n",
    "\n",
    "$score_{ij}^{11} = q_1 \\cdot k_1$\n",
    "\n",
    "and so on.\n",
    "\n",
    "These component scores give us the decomposed attention scores. If we sum over all the component scores, we get the total attention score:\n",
    "$score_{ij} = score_{ij}^{00} + score_{ij}^{01} + score_{ij}^{10} + ...$\n",
    "\n",
    "This allows us to analyze which components contribute most to the total attention score.\n",
    "Concrete example:\n",
    "\n",
    "$Q_i = \\text{token embedding} + \\text{positional embedding}$\n",
    "\n",
    "$K_j = \\text{prev. head output} + \\text{residual stream}$\n",
    "\n",
    "So we have:\n",
    "\n",
    "$q_0 = \\text{token embedding}$\n",
    "$q_1 = \\text{positional embedding}$\n",
    "\n",
    "$k_0 = \\text{prev. head output}$\n",
    "$k_1 = \\text{residual stream}$\n",
    "\n",
    "And the component scores would be:\n",
    "\n",
    "$score_{ij}^{00} = \\text{token embedding} \\cdot \\text{prev. head output}$\n",
    "\n",
    "$score_{ij}^{01} = \\text{token embedding} \\cdot \\text{residual stream}$\n",
    "\n",
    "$score_{ij}^{10} = \\text{positional embedding} \\cdot \\text{prev. head output}$\n",
    "\n",
    "$score_{ij}^{11} = \\text{positional embedding } \\cdot \\text{residual stream}$\n",
    "\n",
    "And so on. By analyzing these component scores, we can determine which components contribute most to the overall attention score $score_{ij}$.\n",
    "\n",
    "Hope this breakdown helps! Let me know if you have any other questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a57aaf-b6f8-45a2-bb8b-fbca7becee6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8f17b-aefc-43d7-9498-d0cff57945f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43e8fe03-5014-4f08-ab00-7ee193d0b560",
   "metadata": {},
   "source": [
    "In a Transformer model, each attention head computes a set of query, key, and value vectors for each token in the input. These are computed by applying learned linear transformations to the token embeddings (these transformations are represented by weight matrices, denoted $W_Q$, $W_K$, $W_V$ for queries, keys, and values respectively). Then, the attention score between each pair of tokens is computed as the dot product of the query vector for one token and the key vector for the other.\n",
    "\n",
    "So for a given token, its query vector $q$ is computed as $q = x W_Q$ where $x$ is the input vector for that token (the residual stream or token embedding), and similarly for the key vector $k = x W_K$. Therefore, the attention score between two tokens with input vectors $x_1$ and $x_2$ is $a = x_1 W_Q (x_2 W_K)^T$.\n",
    "\n",
    "In the section of the text you provided, the goal is to decompose this computation into separate parts, in order to analyze the relative contributions of different components of the input vectors to the attention scores. To do this, they break down the input vector $x$ into 14 separate components: the token embedding, the positional embedding, and the outputs of each of the 12 attention heads in the previous layer. These are referred to as $y_0, y_1, ..., y_{13}$.\n",
    "\n",
    "So the input vector $x$ for each token is expressed as a sum: $x = \\sum_{i=0}^{13} y_i$. As a result, we can express the query and key vectors as sums as well: $q = x W_Q = (\\sum_{i=0}^{13} y_i) W_Q = \\sum_{i=0}^{13} y_i W_Q$ and similarly for the key vector $k$.\n",
    "\n",
    "Now, the attention score between two tokens can be expressed as a sum of $14^2=196$ separate terms, one for each possible pair of components from the two input vectors: $a = q k^T = (\\sum_{i=0}^{13} y_i W_Q) (\\sum_{j=0}^{13} y_j W_K)^T = \\sum_{i=0}^{13} \\sum_{j=0}^{13} (y_i W_Q) (y_j W_K)^T$. This is the decomposition of the attention score into separate components.\n",
    "\n",
    "The benefit of this decomposition is that it allows us to analyze the relative contributions of different components of the input vectors to the attention scores. By computing the norm of each term in this sum, we can get a sense of which pairs of components have the largest impact on the attention scores.\n",
    "\n",
    "Let's give a simple, concrete example:\n",
    "\n",
    "Imagine we have a Transformer model with just three attention heads in the previous layer (so we'll be looking at just $y_0$ (token embedding), $y_1$ (positional embedding), and $y_2$, $y_3$, and $y_4$ (the outputs of the three heads)). Let's say we want to compute the attention score between the first and second tokens in a sequence.\n",
    "\n",
    "The input vectors $x_1$ and $x_2$ for these tokens would be the sum of the corresponding components: $x_1 = y_0 + y_1 + y_2 + y_3 + y_4$ and $x_2 = y_0' + y_1' + y_2' + y_3' + y_4'$ (the primes indicate that these are the values for the second token).\n",
    "\n",
    "The query vector for the first token and the key vector for the second token are then: $q = x_1 W_Q = (y_0 + y_1 + y_2 + y_3 + y_4) W_Q$ and $k = x_2 W_K = (y_0' + y_1' + y_2' + y_3' + y_4') W_K$.\n",
    "\n",
    "Therefore, the attention score between these two tokens would be: $a = q k^T = (\\sum_{i=0}^4 y_i W_Q) (\\sum_{j=0}^4 y_j' W_K)^T = \\sum_{i=0}^4 \\sum_{j=0}^4 (y_i W_Q) (y_j' W_K)^T$.\n",
    "\n",
    "Each of these 25 terms represents the contribution to the attention score from a different pair of components. By computing and comparing these terms, we can get a sense of which pairs of components have the largest influence on the attention score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9b5ae-83fa-4856-8ea7-8a50fb445a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
