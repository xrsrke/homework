{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4df6407-c141-422c-bec0-03e6703df1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5868ad-960d-4b93-97eb-8a17c7a6a18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06272eee-4048-441c-9aec-ed47d62165a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, parallel_context):\n",
    "        group = parallel_context.get_group(ParallelMode.TENSOR)\n",
    "        input = dist.all_reduce(\n",
    "            input,\n",
    "            group=group\n",
    "        )\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b692044-c5ec-4bb8-9f64-e2681956837b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, parallel_context):\n",
    "        super().__init__()\n",
    "        world_size = parallel_context.get_world_size(ParallelMode.TENSOR)\n",
    "        \n",
    "        num_embedding_per_partition = num_embeddings // world_size\n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            num_embedding_per_partition,\n",
    "            embedding_dim\n",
    "        ))\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self._get_vocab_range(\n",
    "            num_embedding_per_partition,\n",
    "            parallel_context\n",
    "        )\n",
    "    \n",
    "    def _get_vocab_range(self, num_embedding_per_partition, parallel_context):\n",
    "        rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "        start_idx = rank*num_embedding_per_partition\n",
    "        end_idx = start_idx+num_embedding_per_partition\n",
    "        return start_idx, end_idx\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_mask = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input.clone() - self.vocab_start_idx\n",
    "        masked_input[input_mask] = 0\n",
    "        \n",
    "        parallel_embeddings = F.embedding(masked_input, self.weight)\n",
    "        parallel_embeddings[input_mask, :] = 0.\n",
    "        \n",
    "        embeddings = Reduce.apply(parallel_embeddings, parallel_context)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21b609-6bed-4471-b4ca-1282c5dc89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: send metadata\n",
    "step 2: send data\n",
    "step 3: construct\n",
    "step 4: fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fff587-d07c-466f-a1bd-0c310ab7bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: map\n",
    "step 2: convert\n",
    "step 3: send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e81cb69-3167-485c-85e4-cdf36720468f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Broadcast(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19a6be0-6fbe-4230-baae-d5b5383c3aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Gather(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = dist.get_world_size()\n",
    "        inputs = [torch.zeros_like(input) for _ in range(world_size)]\n",
    "        dist.all_gather(inputs, input)\n",
    "        inputs = torch.cat(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        chunks = torch.split(\n",
    "            grad_input,\n",
    "            split_size_or_sections=grad_input.shape[-1]//world_size\n",
    "        )\n",
    "        return chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb3ab8e-2ea4-47cb-b63a-9c4a581d52ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, world_size):\n",
    "        super().__init__()\n",
    "        per_partition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            per_partition,\n",
    "            input_size\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.randn(\n",
    "            per_partition\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = Broadcast.apply(input)\n",
    "        output_parallel = F.linear(input, self.weight, self.bias)\n",
    "        outputs = Gather.apply(output_parallel)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b1be6-dda1-4b79-840b-269b0eddceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, requires_grad, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495f237-5cf6-487a-86dc-4f2ee41b0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: mask targets\n",
    "step 2: predicted logits\n",
    "step 3: all-reduce predicted logits\n",
    "step 4: log\n",
    "step 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92251b26-2822-4835-be10-14b9aa375335",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: partition weight\n",
    "step 2: mask input\n",
    "step 3: parallel_embedding\n",
    "step 4: embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8ca1d-3262-4633-adec-b6909d43080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1e468e-1da0-4646-be21-27f9670ec649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_head(acts, hook, clean_cache, corrupted_cache, target_head):\n",
    "    target_layer_idx, target_head_idx = target_head\n",
    "    \n",
    "    if hook.layer() == target_layer_idx:\n",
    "        acts[:, target_head_idx] = corrupted_cache[hook.name][:, target_head_idx]\n",
    "    else:\n",
    "        acts = clean_cache[hook.name]\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77fcaa8c-52a3-4316-9770-75071f014598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2da8ab-f622-4dbd-8450-faca4661c79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9e04e-4af0-443d-9138-1049c3118e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.zeros(n_layers, n_heads)\n",
    "combinations = product(range(n_layers), range(n_heads))\n",
    "\n",
    "for layer_idx, head_idx in combinations:\n",
    "    model.reset_hooks()\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_head,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache=corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    model.add_hook(hook_name, hook_func)\n",
    "    patched_logits, _ = model.run_with_cache(\n",
    "        clean_tokens\n",
    "    )\n",
    "    results[layer_idx, head_idx] = compute_ioi_metric(patched_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820be229-6823-48f1-b54a-fb0d3c41f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input embedding > weight > feature > activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e4281-51c4-4c06-8c9c-db8d16544cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(board_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2fd54a-95df-47cb-9f56-f56059f2e4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 5, 1393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325c6cfc-7efd-4563-b683-6ac58403be18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = f\"blocks.{layer_idx}.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58c137-3f9e-4aaa-9b54-58f3529e3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations = cache[hook_name][:, neuron_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036b49ff-07da-4d93-b5ef-419d9dd1b07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 9, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087dead-a35c-41e1-b824-b8f2c3f0dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45bfae4-4169-435e-862a-6d19f585a203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"z\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3573298c-da72-4b3b-b051-6df1d6f3c510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.9.attn.hook_z'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a8710-bf4d-4dd5-96e7-74afe4a4d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = model.W_O[layer_idx, head_idx]\n",
    "output = cache[hook_name][:, head_idx] @  W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65dfb82-5f46-4088-b12b-84236fbffa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U\n",
    "io_dir = W_U[:, io_tokens]\n",
    "s_dir = W_U[:, s_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52214f68-cce0-4740-ac13-1de575c4ff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736251-f33a-4267-a5cc-f51128ef5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_in_io_dir = einsum(\n",
    "    output,\n",
    "    io_dir\n",
    ")\n",
    "projection_in_s_dir = einsum(\n",
    "    output,\n",
    "    s_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad00288-9afa-4ba9-8144-08786838de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_prob = cache[\"pattern\", layer_idx][:, head_idx]\n",
    "attn_from_end_to_io = attn_prob[:, end_idxs, io_idxs]\n",
    "attn_from_end_to_s = attn_prob[:, end_idxs, s_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f96a5-fc85-4671-b721-d858a7708f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = linear_probe[..., 2]\n",
    "theirs = linear_probe[..., 1]\n",
    "\n",
    "mine_vs_theirs = mine - theirs\n",
    "extracted_direction = mine_vs_theirs[:, 5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9f4a0-10e2-4e70-81e0-96584acdd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(board_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1a7a7-3c11-4baa-a297-10814d994874",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_neurons = cache[hook_name][:, 1393]\n",
    "thr = mlp_neurons.quantile(0.99)\n",
    "top_neurons = mlp_neurons > thr\n",
    "\n",
    "(board_states == 1)[top_neurons].float().mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5062ef-14a5-4fa9-a05d-292eaf3de938",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = model.W_O[0, 1]\n",
    "W_V = model.W_V[0, 1]\n",
    "W_Q = model.W_Q[1, 2]\n",
    "W_K = model.W_K[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ed2bfd-5f73-4bbd-810a-fee9e31ebb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61402c06-2da6-4f3a-ba49-f70193162340",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_OV = W_V @ W_O\n",
    "W_QK = W_Q @ rearrange(\n",
    "    W_K, \"... d_model d_head -> ... d_head d_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea9fd5-95ec-499b-a2dd-61e58e8b721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_weight = W_OV @ W_QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8aad5-6abc-4919-b34b-605032e17068",
   "metadata": {},
   "outputs": [],
   "source": [
    "api scheduler, apiserver, ectd, contrl mangager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebd108-31d2-48e5-8e45-81cbff9b244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: normalize the loss / n_epoch)\n",
    "step 2: calculate gradients with respect to the normalized loss\n",
    "step 3: sum\n",
    "step 4: if current_epoch == n_epoch, update, otherwise, step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bd018-74d1-48be-9cb0-1c2014b31910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(self.dropout(x) + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4c4c5-2a99-4a9d-8596-55fbc4220476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610727ad-be92-4f44-934e-46997b7da8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, discount_factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
