{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9e4823-c915-4523-bb25-d7e909988839",
   "metadata": {},
   "source": [
    "step 1: duplication heads, s2 write to itself\n",
    "step 2: s-inhibition heads, move information from s2 to end\n",
    "step 3: name mover heads, attend to io from end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00ff6f-36f8-4880-a627-3f3f86c8a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=padding tokens,\n",
    "key=non-padding\n",
    "\n",
    "mask everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6166723-d7e4-43c6-8111-1b10be3ba11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformerConfig, HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2a7d6-bf02-48e7-87e5-e9f87a2e746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = HookedTransformerConfig(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fbb2d-0f0a-4b8c-9995-df2a0b779478",
   "metadata": {},
   "outputs": [],
   "source": [
    "HookedTransformer(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8cc51-cb48-472e-b78b-e0637296d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb11c0-e1af-40c1-bedc-88e7537d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = cache[\"resid\"]\n",
    "final_resid = resid[:, -1, :]\n",
    "scaled_final_resid = model.apply_ln_to_stack(final_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e3900-49b6-4f02-9c1b-a0d7ffd42e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42f493-5b22-4ee0-b685-fdd3ac680f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff = scaled_final_resid @ (\n",
    "    W_U[:, correct_token],\n",
    "    W_U[:, incorrect_token]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b04288-68a3-4b48-85d7-158c50677308",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf1ca8-249e-45ba-936b-3792e5d0dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d53fb-4706-4a19-9aa0-11eda07c0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_token = model.to_single_token(\" John\")\n",
    "incorrect_token = model.to_single_token(\" Mary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151b407-821e-4bde-8ad2-f12d8791d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc09efc0-4347-4f2b-bfaf-815a1e08c07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83add6d7-228a-4e83-b62c-fc6ae18f8102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_act(act, hook, corrupted_activations, position_idx):\n",
    "    act[:, position_idx, :] = corrupted_activations[hook.name][:, -1, :]\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76c458-9f1b-4577-8b88-cff292d06f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_diff(logits, correct_token, incorrect_token):\n",
    "    final_logits = logits[:, -1, :]\n",
    "    return final_logits[:, correct_token] - final_logits[:, incorrect_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75fc1ac-f8d9-4de2-9e07-6634c98f6379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df230a89-228c-4bc3-947c-63445e6c8f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = torch.zeros(n_layers, seq_len)\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    hook_name = get_act_name(\"resid_pre\", layer_idx)\n",
    "    \n",
    "    for position_idx in range(seq_len):\n",
    "        hook_func = partial(\n",
    "            patch_act,\n",
    "            corrupted_activations=corrupted_activations,\n",
    "            position_idx=position_idx\n",
    "        )\n",
    "        \n",
    "        patched_logits, _ = model.run_with_cache(\n",
    "            clean_tokens\n",
    "        )\n",
    "        \n",
    "        result[layer_idx, position_idx] = compute_logit_diff(\n",
    "            patched_logits,\n",
    "            correct_token,\n",
    "            incorrect_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7e475-3777-486d-91ae-2d2fa35d64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: project to fourier basis\n",
    "step 2: extract frequenciies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4a09b-27ea-4b19-b4e9-a84acc34d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755bc48-e6b0-4766-91bd-1a9111cc9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in_acts = cache[\"post\", 2]\n",
    "W_out = model.w_out[2]\n",
    "\n",
    "output = W_in_acts @ W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab69fe-598b-4f59-836f-ac59023c4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ x @ W_OV @ W_OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede97441-292f-4691-9ae8-634fb6fc3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f838c27-dd1f-407e-9338-cfa587709f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_head(act, hook, clean_cache, corrupted_cache, target_head):\n",
    "    target_layer_idx, target_head_idx = target_head\n",
    "    if target_layer_idx == hook.layer():\n",
    "        act[:, :, head_idx] = corrupted_cache[hook.name][:, :, head_idx]\n",
    "    else:\n",
    "        act = clean_cache\n",
    "    \n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd60fb2c-6130-4425-b055-37d80d409122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec261eb-f080-4ebc-a0e7-d8163a1d1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = product(range(n_layers), range(n_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2114f-6cf9-489c-8564-4af8a6ca8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.zeros(n_layers, n_heads)\n",
    "\n",
    "for layer_idx, head_idx in combinations:\n",
    "    model.reset_hooks()\n",
    "    hook_name = get_act_name(\"z\", layer_idx)\n",
    "    hook_func = partial(\n",
    "        patch_head,\n",
    "        clean_cache=clean_cache,\n",
    "        corrupted_cache,\n",
    "        target_head=(layer_idx, head_idx)\n",
    "    )\n",
    "    \n",
    "    model.add_hook(hook_name, hook_func)\n",
    "    patched_logits, _ = model.run_with_cache(\n",
    "        clean_tokens\n",
    "    )\n",
    "    \n",
    "    results[layer_idx, head_idx] = compute_ioi_metric(patched_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c33383-5e86-4975-91d3-be96150b4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = linear_probe[..., 0]\n",
    "mine = linear_probe[..., 1]\n",
    "theirs = linear_probe[..., 2]\n",
    "\n",
    "not_empty = (mine - theirs) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c45dc-4648-4d44-bf92-4e83fc877ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vs_not_empty = empty - not_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226af000-099a-4472-bc1d-100ad1840846",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: logit lens across residual stream\n",
    "step 2: logit lens across decomposed residual stream\n",
    "step 3: logit lens across decomposed attn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13a62ab-32d8-4526-a890-f0967ab88d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ba3ba-930a-4fe6-bee8-6d0ac8f2f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(**params)\n",
    "model = HookedTransformer(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdccfd-1ef1-4b50-8c57-47def6ae643c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be492f0-321a-4f23-9295-2854835a68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "[v0, v1] @ W_E @ W_K @ W_Q @ W_E @ [v_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257e8c78-e331-437d-8638-e847ff2d0940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72138a5-85b3-41f8-adbd-24ce4792e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, parallel_context):\n",
    "        super().__init__()\n",
    "        world_size = parallel_context.get_world_size(ParallelMode.TENSOR)\n",
    "        per_partition = num_embeddings // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            per_partition,\n",
    "            embedding_dim\n",
    "        ))\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self._get_vocab_range(\n",
    "            per_partition, parallel_context\n",
    "        )\n",
    "    \n",
    "    def _get_vocab_range(self, per_partition, parallel_context):\n",
    "        rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "        start_idx = rank*per_partition\n",
    "        end_idx = start_idx + per_partition\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_mask = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input[input_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14931d0f-722f-4d8f-a00f-ef1dc8c7c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast, gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571124d-bf57-49b5-88a5-38282c1afbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast > gather > scatter > all-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "177a793f-09c0-474d-a86d-432476367a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3727342b-fc28-40d6-a481-07f65ea0e5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scatter(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        inputs = torch.split(\n",
    "            input,\n",
    "            split_size_or_sections=input.shape[-1]//world_size\n",
    "        )\n",
    "        return inputs[rank]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        world_size = dist.get_world_size()\n",
    "        grads = [torch.zeros_like(grad_input) for _ in range(world_size)]\n",
    "        dist.all_gather(grads, grad_input)\n",
    "        grads = torch.cat(grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afbe737c-f313-4043-9599-7f81c11ed8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        dist.all_reduce(input)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00bda2c0-0a4f-47d1-8c78-85e6d7fcaf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RowParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        world_size = dist.get_world_size()\n",
    "        input_per_partition = output_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            output_size,\n",
    "            input_per_partition\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.randn(\n",
    "            output_size\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = Scatter.apply(input)\n",
    "        output_parallel = F.linear(input_parallel, self.weight)\n",
    "        outputs = Reduce.apply(input_parlalel)\n",
    "        return outputs + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9ac28a-758e-4597-a8fd-71f5f2e6c80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, parallel_context):\n",
    "        group = parallel_context.get_group(ParallelMode.TENSOR)\n",
    "        dist.all_reduce(input, group=group)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ce67d8-bd4e-42ed-9f28-9cf26f42319b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, parallel_context):\n",
    "        super().__init__()\n",
    "        world_size = dist.get_world_size()\n",
    "        per_partition = num_embeddings // world_size\n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            per_partition,\n",
    "            embedding_dim\n",
    "        ))\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self._get_vocab_range(\n",
    "            per_partition,\n",
    "            parallel_context\n",
    "        )\n",
    "    \n",
    "    def _get_vocab_range(self, per_partition, parallel_context):\n",
    "        rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "        start_idx = rank*per_partition\n",
    "        end_idx = start_idx+per_partition\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_mask = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input.clone() - self.vocab_start_idx\n",
    "        masked_input[input_mask] = 0\n",
    "        \n",
    "        parallel_embedding = F.embedding(masked_input, self.weight)\n",
    "        parallel_embedding[input_mask, :] = 0.\n",
    "        \n",
    "        embeddings = Reduce.apply(parallel_embedding, parallel_context)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf0b72-3235-4734-9b90-51b0a8aeb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scatter > All-reduce > Identity > Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e880d11-332b-4be7-a512-9a1d0218edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostdiscovery, elasticdriver, 3 notifications, torchstate, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef0db4-d939-4fca-a1ee-8bcde88570c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: num_processed=0\n",
    "step 2: unprocess\n",
    "step 3: work nodes\n",
    "step 4: get unprocessed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92977a9e-28ae-4f8e-b11c-293708ad369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        ctx.recomputed = recomputed\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        input_leaf = self.input.detach().requires_grad_(\n",
    "            input.requires_grad\n",
    "        )\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            outputs = self.function(input_leaf)\n",
    "        \n",
    "        ctx.recomputed.append((outputs, input_leaf))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c657bc-0846-4ce8-b863-35a3d0704b90",
   "metadata": {},
   "source": [
    "tensor_parallel_size, pipeline_parallel_size, data_parallel_size, world_size, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e86c0f-ee57-40dd-8be2-ae498a49890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(local_rank+1) % world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc93f83-24ab-47e5-a655-d91447203514",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: send metadata\n",
    "step 2: send data\n",
    "step 3: receive the metadata, reconstruct \n",
    "step 4: fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c86732e-35c9-431f-b52d-930de227b3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        output, input_leaf = ctx.recompute.pop()\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            torch.autograd.backward(output, grad_input)\n",
    "        \n",
    "        return tuple([None, None, None, input_leaf.grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0ba8d-c461-4a4b-8232-ffb3ee7d1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, dtype, requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02fa0c-a75b-4a4e-ba75-7ab372f8c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = dist.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57769a-ba67-413e-a7da-9db9a44cd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 69:\n",
    "    dist.isend(x, dst=42)\n",
    "elif rank == 42:\n",
    "    dist.irecv(tensor_will_be_received_data, x, src=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116b794-b88e-4c9e-9fc3-e4b150a72456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d1f8f-191c-4c47-b0a8-df782651dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.phony = phony\n",
    "        ctx.recomputed = recomputed\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        output, input_leaf = ctx.recomputed.pop()\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            torch.autograd.backward(output, grad_input)\n",
    "        \n",
    "        return tuple([None, None, None, input_leaf.grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001a6e8-58b3-4b40-9dd0-99f99021afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: create a mapping between datatype and integer\n",
    "step 2: convert\n",
    "step 3: send\n",
    "step $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48df764-4144-4de2-93cd-0b63b6329cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "register > cache > sram > disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98608d2e-67e6-498c-9739-0c1b22b1fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        function = ctx.function\n",
    "        input_leaf = ctx.input.detach().requires_grad(\n",
    "            ctx.input.requires_grad\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input_leaf)\n",
    "            \n",
    "        ctx.recomputed.append((output, input_leaf ))\n",
    "        \n",
    "        return tuple([None, None, None, grad_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76394ffc-6439-41d4-bb74-d0b69a9c0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ x @ W_OV^{1} @ W_OV^{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8ff37-61c5-4b83-a980-fa9f5f094824",
   "metadata": {},
   "source": [
    "step 1: logits = resid @ unembed\n",
    "step 2: resid = embed + pos_embed + sum(12 heads)\n",
    "step 3: logits = [embed + pos_embed + sum(12 heads)] @ unembed\n",
    "step 4: logits = embed @ unembed + pos_embed @ unembed + sum(12 heads) @ unembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88567d-5531-42c9-a809-fb9f96be98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu\n",
    "correlated vs anti-correlated\n",
    "feature importance\n",
    "feature sparity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48986f-423f-49d0-8555-79d24361e633",
   "metadata": {},
   "source": [
    "step 1: a clean prompt, a corrupted prompt\n",
    "step 2: record the interdimate actiavtions of the clean prompt and corrupted prompt\n",
    "step 3: run the clean prompt again, if a component is the target source head, or an mlp in between, then patch the activations from the corrupted prompt, otherwise, patch from the clean prompt\n",
    "step 4: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90651455-a199-46ec-8cf5-56f000f8e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd387dfd-0699-40fb-938d-88195c9c9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875383b-b91e-4229-b6a3-9469c4b88276",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_components = torch.cat([\n",
    "    cache[\"embed\"],\n",
    "    cache[\"pos_embed\"],\n",
    "    cache[\"result\", layer_idx-1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9087e64d-9ba3-4dec-8152-41bfa8afcaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb78c5a-22da-4b39-9a84-b07b7b2bbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "decomposed_query = einsum(\n",
    "    input_components,\n",
    "    W_Q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce30439-1f9b-4f5e-b36c-cda953ace58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = model.W_K[layer_idx]\n",
    "decomposed_key = einsum(\n",
    "    input_components,\n",
    "    W_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e57054-58a9-43fd-840f-77f60d3697a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = einsum(\n",
    "    decomposed_query,\n",
    "    decomposed_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d66a1-dcba-4982-b3e5-f6183844ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: x @ W_Q\n",
    "step 2: x = embed + pos_embed + sum(12 heads)\n",
    "step 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cd267-4906-4e53-9b0e-4aafe5ec80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned response, stimulus-evoked, motor imagery, populaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48a3ae-6d2a-44c2-9285-5d9890b565e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b74613-e697-49a1-89c8-44b93fcf70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation ranges: 0.42 - 0.69\n",
    "percentage 2%\n",
    "met hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf5f1b-05b6-44dd-80e0-b6bd6884900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplication head, name mover heads, s-inhibition heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c761fb-652c-4635-bf99-3bd91ca0605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron fire, but not the target pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13060285-351c-4bfd-ba96-a8651883144c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa53a4-d34a-485f-9eb3-5b633415a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank, process_group, ranks_in_group, local_world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc303b-a418-48db-a182-7ac33c9e7636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc5055a7-f2c7-439f-82b1-98b6a58da1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed.rpc as rpc\n",
    "from torch.distributed.rpc import RRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fffe1e4-8f9e-4a3a-976a-d4813ea31f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.id = RRef(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48360656-f976-4ae1-b9dc-2047bd47d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings.masked_fill(mask == False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea4d11d2-2956-465e-b034-789be6fb10ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.distributed.rpc import RRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ba3343d-e599-4b98-8f9c-fd1534c99a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.id = RRef(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0587d-02c8-4cb6-955b-97d4cb819df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "biocompabile,\n",
    "reliable recording\n",
    "neural plasiticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab07409-7c9e-464f-a7e4-6865b5346cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, discount_factor):\n",
    "    x = torch.pow(discount_factor, len(rewards))\n",
    "    return rewards*x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6e0596-0e69-42b1-a3c2-00f9ded66e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dec9f-6805-452b-bbdf-de15ebcca3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rearrange(\n",
    "    images,\n",
    "    \"bs c (p_height n_height) (p_width n_width) -> \\\n",
    "    bs (n_height n_width) (p_height p_width c)\n",
    "    \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ec26f-f143-421d-bf87-bb4dac78976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training task: training distribution, objective\n",
    "base optimizer: sgd, architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "979e85bc-c00e-47ab-b908-6b2888cb93a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cac5f0f8-9129-4771-a1e8-2b6ba9a035a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranks = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5036240-e654-4827-b530-25f480c1f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for other in ranks:\n",
    "    if other == rank:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a571f37-4158-47c8-9624-45d62af888c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = f\"worker_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163df83-1e2d-44f3-a75b-5ba1c778bd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
