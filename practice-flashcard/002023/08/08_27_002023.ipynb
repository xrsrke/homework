{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95afd44-5b26-45a5-8d8a-e6f77b05b38f",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0823c9bb-2adb-409b-80cb-be8a4f30bd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b09e0dd-3d8b-474b-9054-40d159a6c6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f4859c-87ea-4e15-ac95-0cacb76a3030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, parallel_context):\n",
    "        group = parallel_context.get_group(ParallelMode.TENSOR)\n",
    "        dist.all_reduce(input, group=group)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bef8e3-1ea0-45f1-8904-0cc7110f02ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _VocabParallelCrossEntropy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, parallel_logits, parallel_context):\n",
    "        def get_vocab_range():\n",
    "            rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "            partition_size = parallel_logits.shape[-1]\n",
    "            start_idx = rank*partition_size\n",
    "            end_idx = start_idx+partition_size\n",
    "            return start_idx, end_idx\n",
    "        \n",
    "        vocab_start_idx, vocab_end_idx = get_vocab_range()\n",
    "        target_mask = (targets < self.vocab_start_idx) | (targets >= self.vocab_end_idx)\n",
    "        masked_targets = targets.clone() - self.vocab_start_idx\n",
    "        masked_targets[target_mask] = 0.\n",
    "            \n",
    "        masked_targets_1d = rearrange(\n",
    "            masked_targets,\n",
    "            \"batch_size seq_len -> (batch_size seq_len)\"\n",
    "        )\n",
    "        parallel_logits = rearrange(\n",
    "            parallel_logits,\n",
    "            \"batch_size seq_len vocab_size -> (batch_size seq_len) vocab_size\"\n",
    "        )\n",
    "        predicted_logits = parallel_logits[torch.arange(targets.shape[0]), masked_targets_1d]\n",
    "        predicted_logits = torch.where(masked_targets_1d == False, predicted_logits, 0.)\n",
    "        \n",
    "        predicted_logits = Reduce.apply(predicted_logits)\n",
    "        \n",
    "        exp_logits = torch.exp(parallel_logits).sum(dim=-1)\n",
    "        exp_logits = Reduce.apply(exp_logits, parallel_context)\n",
    "        loss = exp_logits.log() - predicted_logits\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c313b3e-712a-4393-b571-8cb25afabe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabParallelCrossEntropy(nn.Module):\n",
    "    def __init__(self, parallel_context):\n",
    "        super().__init__()\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = _VocabParallelCrossEntropy.apply(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e213348d-dd22-478b-8c30-7f749d110804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, parallel_context):\n",
    "        group = parallel_context.get_group(ParallelMode.TENSOR)\n",
    "        dist.all_reduce(input, group=group)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffe6b31-b88c-4eba-a2b9-5ecde20a44d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, parallel_context):\n",
    "        super().__init__()\n",
    "        \n",
    "        world_size = parallel_context.get_world_size(ParallelMode.TENSOR)\n",
    "        per_partition = num_embeddings // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            per_partition,\n",
    "            embedding_dim\n",
    "        ))\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self._get_vocab_range(\n",
    "            per_partition,\n",
    "            parallel_context\n",
    "        )\n",
    "    \n",
    "    def _get_vocab_range(self, per_partition, parallel_context):\n",
    "        rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "        start_idx = rank*per_partition\n",
    "        end_idx = start_idx+per_partition\n",
    "        return start_idx, end_idx\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_mask = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input.clone() - self.vocab_start_idx\n",
    "        masked_input[input_mask] = 0.\n",
    "        \n",
    "        parallel_embeddings = F.embedding(masked_input, self.weight)\n",
    "        parallel_embeddings[masked_input] = 0.\n",
    "        \n",
    "        embeddings = Reduce.apply(parallel_embeddings, parallel_context)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc90da9e-333d-4c60-987d-67b028a48062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scatter(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        \n",
    "        chunks = torch.split(\n",
    "            input,\n",
    "            split_size_or_sections=input.shape[-1]//world_size\n",
    "        )\n",
    "        return chunks[rank]\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        \n",
    "        grads = [torch.zeros_like(grad_input) for _ in range(world_size)]\n",
    "        dist.all_gather(grads, grad_input)\n",
    "        grads = torch.cat(grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016a3233-580f-47c1-8acb-8402710f19b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        dist.all_reduce(input)\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return (grad_input, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689fba20-4c7e-4a07-93ec-e89f60392931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RowParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        world_size = dist.get_world_size()\n",
    "        inp_per_partition = input_size // world_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(\n",
    "            output_size,\n",
    "            inp_per_partition\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.randn(\n",
    "            output_size\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_parallel = Scatter.apply(input)\n",
    "        parallel_output = F.linear(\n",
    "            input_parallel, self.weight\n",
    "        )\n",
    "        outputs = Reduce.apply(input_parallel)\n",
    "        return outputs + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97a768-ec07-4760-9a1c-7f857079ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast > gather > scatter > all-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb87a3-d164-40c4-bbe6-d4ac1fbb5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3277a5-25b1-4e95-b836-277bbdedf7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a906526-b13d-497d-af42-387eaa761b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in_acts = cache[\"post\", layer_idx]\n",
    "W_out = model.W_out[layer_idx]\n",
    "\n",
    "output = W_in_acts @ W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e0d9e-2719-4a45-8674-712999d9ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_rref.rpc_sync().your_mom(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c42d3-7c43-4179-9e7c-ca7599dca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize communication\n",
    "maximize storage\n",
    "minimize flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febfe80-fece-48a9-bbcd-237af1d87d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "int zero() {\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed901409-a806-4e26-a6f2-38b18119dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "int numberToColor(int x) {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37961f-517c-4b8c-90d2-5cd8c92e4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "using namespace std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131f453-520a-451a-af0f-52c871e82bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Book() {\n",
    "    public:\n",
    "        string title;\n",
    "    \n",
    "        Book() {\n",
    "            std::cout << \"x\";\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4223155-4294-4956-a15d-2fccde784d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_microbatches = 4\n",
    "n_partitions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66031bc7-856b-45b4-9fc0-1042fdb8b895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clock_cycles = n_microbatches+n_partitions-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5046b69-40f5-4415-b88a-d23ee45e01fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n",
      "[(1, 0), (0, 1)]\n",
      "[(2, 0), (1, 1), (0, 2)]\n",
      "[(3, 0), (2, 1), (1, 2)]\n",
      "[(3, 1), (2, 2)]\n",
      "[(3, 2)]\n"
     ]
    }
   ],
   "source": [
    "for clock_idx in range(n_clock_cycles):\n",
    "    start_partition_idx = max(clock_idx+1-n_microbatches, 0)\n",
    "    end_partition_idx = min(clock_idx+1, n_partitions)\n",
    "    \n",
    "    xs = []\n",
    "    for partition_idx in range(start_partition_idx, end_partition_idx):\n",
    "        microbatch_idx = clock_idx - partition_idx\n",
    "        xs.append((microbatch_idx, partition_idx))\n",
    "    print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18dae6f0-fa5c-423a-9e29-c3692f14e0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _P2P:\n",
    "    def send(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f8063f-5e3c-416a-a952-a3b02f81c113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send(data, src_rank, dst_rank, parallel_context, parallel_mode):\n",
    "    rank = parallel_context.get_local_rank(paralllel_mode)\n",
    "    if rank == src_rank:\n",
    "        _P2P().send(data, dst_rank, parallel_context, parallel_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f95b63c-f540-4fc6-a011-75c30b91bbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_microbatches = 4\n",
    "n_partitions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94b9e9f0-b715-4cc7-a2c7-96519c6836fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clock_cycles = n_microbatches+n_partitions-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1f4188a-680b-4cd3-8850-1efa1ac10a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n",
      "[(1, 0), (0, 1)]\n",
      "[(2, 0), (1, 1), (0, 2)]\n",
      "[(3, 0), (2, 1), (1, 2)]\n",
      "[(3, 1), (2, 2)]\n",
      "[(3, 2)]\n"
     ]
    }
   ],
   "source": [
    "for clock_idx in range(n_clock_cycles):\n",
    "    start_partition_idx = max(clock_idx+1-n_microbatches, 0)\n",
    "    end_partition_idx = min(clock_idx+1, n_partitions)\n",
    "    \n",
    "    xs = []\n",
    "    for partition_idx in range(start_partition_idx, end_partition_idx):\n",
    "        microbatch_idx = clock_idx-partition_idx\n",
    "        xs.append((microbatch_idx, partition_idx))\n",
    "    print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8839acf9-c6c2-4d05-9821-e2c1200bbd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27586c73-47fe-40b0-aefe-99a592ca7c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = threading.local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a28cc-b1d5-42f3-98cd-547fddf54041",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(\n",
    "    target=print_and_modify\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770e73e-1fbd-4962-acb9-310c1f3d509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelGroupInitializer(ProcessGroupInitializer):\n",
    "    def init_dist_group(self):    \n",
    "        num_tensor_parallel_groups = self.world_size // self.tensor_parallel_size\n",
    "        process_group = None\n",
    "        local_rank = None\n",
    "        local_world_size = None\n",
    "        ranks = None\n",
    "        \n",
    "        for i in range(num_tensor_parallel_groups):\n",
    "            ranks = list(range(\n",
    "                i*self.tensor_parallel_size,\n",
    "                (i+1)*self.tensor_parallel_size\n",
    "            ))\n",
    "            process_group = dist.new_group(ranks)\n",
    "            \n",
    "            if self.rank in ranks:\n",
    "                local_rank = ranks.index(self.rank)\n",
    "                local_world_size = len(ranks)\n",
    "        \n",
    "                return {\n",
    "                    \"local\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1d3395-c06e-4c82-887f-31e7d8dd3eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _P2P:\n",
    "    def _recv_metadata(self, src_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        \n",
    "        dtype = torch.tensor(0)\n",
    "        dist.recv(dtype, src=src_rank, group=group)\n",
    "        dtype = DTYPE_TO_ID[dtype]\n",
    "        \n",
    "        shape = torch.tensor(0)\n",
    "        dist.recv(shape, src=src_rank, group=group)\n",
    "        \n",
    "        requires_grad = torch.tensor(0)\n",
    "        dist.recv(requires_grad, src=src_rank, group=group)\n",
    "        requires_grad = True if requires_grad == 1 else False\n",
    "        \n",
    "        return dtype, shape, requires_grad\n",
    "    \n",
    "    def recv(self, src_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_mode.get_group(parallel_mode)\n",
    "        dtype, shape, requires_grad = self._recv_metadata(src_rank, parallel_context, parallel_mode)\n",
    "        data = torch.zeros(shape, requires_grad=requires_grad, dtype=dtype)\n",
    "        dist.recv(data, src=src_rank, group=group)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134855c-ddac-4558-85c5-ee9409248153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recv(src_rank, dst_rank, parallel_context, parallel_mode):\n",
    "    rank = parallel_context.get_local_rank(parallel_mode)\n",
    "    if rank in dst_rank:\n",
    "        _P2P().recv(src_rank, parallel_context, parallel_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce5e46-538e-436f-a30f-51de0e59fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: script\n",
    "step 2: run\n",
    "step 3: compare\n",
    "step 4: notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f781f7c-1688-425c-9319-38fdcc9e2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "job selector > worker threads > pool watcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954f74d-f700-4801-afb1-c79ac216db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine global rank\n",
    "step 2: resize embedding\n",
    "step 3: parallelize embedding, linear, attn, layer norm\n",
    "step 4: resize lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a1026-ac22-4007-92d3-80e62eb08ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "received data\n",
    "jobs\n",
    "handshake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb704c-e24f-4109-bfcf-713d6d204b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary, entity, interactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d16ea00-de57-4fc0-9fc6-5a7df2f64f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716f76a-9545-48ba-b260-0130d80bc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = []\n",
    "\n",
    "for rank in range(3):\n",
    "    p = Process(target=say_hello, args=(rank,))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    \n",
    "for p in processes:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5066ce8-b766-4077-a67a-961711e037ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready\n",
    "begin forward\n",
    "finsihed forward\n",
    "finished backward\n",
    "finished batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5fd5fa1-39c1-4422-a3e2-7fd5b2c06fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _P2P:\n",
    "    def _send_metadata(self, dst_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        dtype = torch.tensor(DTYPE_TO_ID[data.dtype])\n",
    "        dist.send(dtype, dst=dst_rank, group=group)\n",
    "        \n",
    "        requires_grad = torch.tensor(\n",
    "            1 if data.requires_grad == True else 0\n",
    "        )\n",
    "        dist.send(requires_grad, dst=dst_rank, group=group)\n",
    "        \n",
    "        shape = torch.tensor(data.shape.to_list())\n",
    "        dist.send(requires_grad, dst=dst_rank, group=group)\n",
    "    \n",
    "    def send(self, data, dst_rank, parallel_context, parallel_mode):\n",
    "        group = parallel_context.get_group(parallel_mode)\n",
    "        self._send_metadata(data, dst_rank, parallel_context, parallel_mode)\n",
    "        dist.send(data, dst=dst_rank, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5ef51ec-f1af-41c3-9791-73f5be881bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send(data, src_rank, dst_rank, parallel_context, parallel_mode):\n",
    "    rank = parallel_context.get_local_rank(parallel_mode)\n",
    "    \n",
    "    if src_rank == rank:\n",
    "        _P2P.send(data, dst_rank, parallel_context, parallel_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8128533-803e-4625-a154-6606f22ed96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: initialize partitioned weight\n",
    "step 2: mask targets\n",
    "step 3: calculate local embedding\n",
    "step 4: calculate global embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d1818ec-c43b-47a4-af98-aa9d189509e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14c7aaa8-0046-4ac1-b01a-fba1092a2ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f6b7054-9b1b-4c29-9b16-6065faf35bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    with lock:\n",
    "        print_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752cfb0-577c-4de9-ac83-d7f0ce05c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=run)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877599f8-6144-4d66-9f68-a6134ed933ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: mark a point in the forward pass's computation graph\n",
    "step 2: retrieve function, input\n",
    "step 3: recompute activations, put them into a shared memory queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc1824-50fa-4f72-9fc4-61b85a5ee0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool watcher, worker threads, job selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00460d5-6d1b-4a0c-b272-1db5d2dd68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "syncronization\n",
    "received data queue\n",
    "handhskae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd7dfc-8dfd-4981-8609-a522f710f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter > reduce > identity > gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee426d3-f434-4612-8cb8-ff8e84a12f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "net brackets\n",
    "no negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917004a-63bb-48d5-b164-fc3b5992a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_residal_stream @ (W_U[0]-W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfde99-d4b4-4352-9671-bb0960df5bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
