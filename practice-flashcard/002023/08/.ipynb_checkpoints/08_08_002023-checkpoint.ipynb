{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5154b56-1ebb-4552-b050-bf484086223c",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d92420-fde3-47b1-a263-792b50cfd7e5",
   "metadata": {},
   "source": [
    "n-dimensional activation space => n linearly independent basis vector => n non-polysemanticity neurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b58e0-cbad-46be-990b-c21b207711b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913e69d3-bcb9-4580-b66c-5080d7670e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrupted_prompts = [\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "    \"When Tom and James went to the part, James gave the ball to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcdff4-446d-4faf-a174-787fa5b1e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_tokens = model.to_tokens(\"Mary Tom\", prepend_bos=True)\n",
    "incorrect_tokens = model.to_tokens(\"John James\", prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c47c75-7d20-4f54-9be6-1941b4813e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9541bb1-58f9-429f-ad57-476f92f67542",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits, _ = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, _ = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80e6d23-4f2d-4888-9aeb-0885f4783fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_average_logit_difference(logits, correct_tokens, incorrect_tokens):\n",
    "    final_logits = logits[:, -1, :]\n",
    "    correct_logits = final_logits[:, correct_tokens]\n",
    "    incorrect_logits = final_logits[:, incorrect_tokens] \n",
    "    return (correct_logits - incorrect_logits).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e71cc-9c9a-46b5-96a3-ee0957106890",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logit_difference = compute_average_logit_difference(clean_logits, correct_tokens, incorrect_tokens)\n",
    "corrupted_logit_difference = compute_average_logit_difference(corrupted_logits, correct_tokens, incorrect_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c40efd-01c0-4426-8c51-be2c9e601175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ioi_metric(logits):\n",
    "    patched_logit_difference = compute_average_logit_difference(logits, correct_tokens, incorrect_tokens)\n",
    "    return (patched_logit_difference - corrupted_logit_difference) / (clean_logit_difference - corrupted_logit_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c4dab-7217-4b15-bccc-8dec211fb58d",
   "metadata": {},
   "source": [
    "step 1: start with a diverse batch of data\n",
    "step 2: record the activations of the target head\n",
    "step 3: extract the attention pattern between the target query position with all other positions\n",
    "step 4: take the average attention pattern across batch\n",
    "step 5: plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce09db-bc4d-4a88-9a18-08b3faea8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pos = model.W_pos\n",
    "W_Q = model.W_Q[layer_idx, head_idx]\n",
    "W_K = model.W_K[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072fa31-b71e-4b4f-922b-62db39fbe371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_scores = W_pos @ W_Q @ W_K @ W_pos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d181672-a007-4cc5-a4f7-e64149b016c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5066b69-ef85-4792-bcab-51f80e894c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_scores(scores):\n",
    "    masks = torch.triu(torch.ones_like(scores)).bool()\n",
    "    return torch.where(masks, scores, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64f2aa-dd75-404d-89d5-e09975eb6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_head = model.cfg.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b3ec4-89b2-4e42-be1b-8f096e0ce5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = mask_scores(scores / (d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ace0d-91e0-4eea-9fff-bed504a265d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_pos_pattern = F.softmax(masked, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3996438-a2c3-4d48-aa09-d4b268a8fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe611250-1e82-401b-85ac-e558cb314619",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3a64c-f965-473f-bffa-0087afd06bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "pos_embed = cache[\"pos_embed\"]\n",
    "head_outputs = cache[\"result\", prev_layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc847e3-1d10-4960-9914-ce19046cfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_components = torch.cat([\n",
    "    embed,\n",
    "    pos_embed,\n",
    "    head_outputs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abc4fa-f6aa-4fda-a430-cf00980ecb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = model.W_Q[layer_idx, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515da2f4-346c-4f17-9b4f-e02de1b8b890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4956a22-1630-41be-b4bd-949aa434d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_components = einsum(\n",
    "    input_components,\n",
    "    W_Q,\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c0a34-5a16-45a7-88cd-0e033c8b1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_contributions = query_components.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0995e-0821-4eed-9552-0c256bdd44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d79436-573f-42a6-a159-55c6f8117c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.run_with_hooks(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684e609-db07-4213-b2ec-fc07dea17b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c7f07-56b4-42f6-80d9-b2edf60c0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokens = tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a709822-68ae-4c94-9986-491ebdacec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_log_probs = log_probs[:, target_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff4763-c2c0-40ea-b05d-f7a894afd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb9d2b-8c0d-45bf-97d0-f746b62d61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 2\n",
    "head_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c5e32-44b2-48af-8632-2b816ee897ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_pattern = cache[\"pattern\", layer_idx][:, head_idx][:, target_query_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abd52e-0602-468f-be61-8f4e06fb0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attention_pattern = F.softmax(attn_pattern).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b654a-9ffe-4706-ba8c-e9455a87e50d",
   "metadata": {},
   "source": [
    "step 1: extract the feature \"this cell contains my piece\" from black's perspective in cell F1\n",
    "step 2: board history before the target move\n",
    "step 3: feed\n",
    "step 4: intervene the residual stream\n",
    "step 5: continue the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64a366-0a2c-45ae-80eb-1df4cbd0c3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4b467-3810-4cf4-8b55-2dd4758d112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding > weights > features/neurons > activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a5ee6-ea97-4ade-9be8-c1330315a609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49d467-5636-4021-a5ee-c319ca614846",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: logit lens across residual stream\n",
    "step 2: decompose residual stream, localize attention layer\n",
    "step 3: decompose the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5b5a3-2a1d-4d0e-83ea-5b9470a85ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "W_Q = model.W_Q[1, 4]\n",
    "W_K = model.W_K[1, 4]\n",
    "W_O = model.W_O[0, 7]\n",
    "W_V = model.W_V[0, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86487631-4fe4-4512-8137-4a8c72f3ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = W_E @ W_Q\n",
    "K = W_E @ W_V @ W_O @ W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac7d6a3-7f50-4774-8be6-497594e5e31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda63dc-d9d0-4857-902f-d485bb00f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_circuit = FactoredMatrix(Q, K.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba1d06-0c4b-4b27-9ccf-f95ef39841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: init a global distributed group\n",
    "step 2: initialize parallel groups\n",
    "step 4: set device\n",
    "step 5: seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42539d-9929-43ed-88f6-7616395f4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff38a2ed-1ba5-4b58-bd75-32d815938ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa944b76-bc84-4655-8777-71de473f2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStateHandler:\n",
    "    def __init__(self, model):\n",
    "        self.value = model\n",
    "        self.commit()\n",
    "    \n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "    \n",
    "    def commit(self):\n",
    "        self._saved_state_dict = deepcopy(self.value.state_dict())\n",
    "    \n",
    "    def restore(self):\n",
    "        self.value.load_state_dict(self._saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1cf514-d45e-4341-afac-00389c5fc921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461209b8-36ba-42b7-8abf-ad2d14227e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelGroupInitializer(ProcessGroupInitializer):\n",
    "    def init_process_group(self):\n",
    "        self.num_tensor_parallel_groups = self.world_size // self.tensor_parallel_size\n",
    "        local_rank = None\n",
    "        local_world_size = None\n",
    "        process_group = None\n",
    "        parallel_mode = ParallelMode.TENSOR\n",
    "        \n",
    "        for i in range(self.tensor_parallel_size):\n",
    "            start_rank = i*self.num_tensor_parallel_groups\n",
    "            end_rank = (i+1)*self.num_tensor_parallel_groups\n",
    "            \n",
    "            for j in range(tensor_parallel_size):\n",
    "                ranks = list(range(\n",
    "                    start_rank+j,\n",
    "                    end_rank,\n",
    "                    tensor_parallel_size\n",
    "                ))\n",
    "                process_group = dist.new_group(ranks=ranks)\n",
    "                \n",
    "                if self.rank in ranks:\n",
    "                    local_rank = ranks.index(self.rank)\n",
    "                    local_world_size = len(ranks)\n",
    "        \n",
    "        return {\n",
    "            \"local_rank\": local_rank,\n",
    "            \"local_world_size\": local_world_size,\n",
    "            \"process_group\": process_group,\n",
    "            \"parallel_mode\": parallel_mode\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b4c0b-b04a-4a16-ac5b-e8d7871301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: scale the loss using scaling factor\n",
    "step 2: calculate the gradients with respect to the scaled loss\n",
    "step 3: unscale the gradients using the scaling factor\n",
    "step 4: update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a05012-a49d-47c8-b6d3-d7e6798b8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "commit, restore, set_value, sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689d9e2-5701-4723-ab39-c128d2d82dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group(group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ecbd4-12d8-4f7e-89f0-967c52e7c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: initialize a global distributed group\n",
    "step 2: initialize parallel groups\n",
    "step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724d114-0dec-416a-b633-d87677cfd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        self.num_embeddings_per_partition = world_size // num_embeddings\n",
    "        \n",
    "        self.vocab_start_idx, self.vocab_end_idx = self.get_vocab_range(\n",
    "            self.num_embeddings_per_partition\n",
    "        )\n",
    "        \n",
    "        self.weight = nn.Parameter(self.num_embeddings_per_partition, embedding_dim)\n",
    "    \n",
    "    def get_vocab_range(self, num_embeddings_per_partition):\n",
    "        rank = torch.distributed.get_rank()\n",
    "        start_idx = rank*num_embeddings_per_partition\n",
    "        end_idx = start_idx+num_embeddings_per_partition\n",
    "        return start_idx + end_idx\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        masks = (tokens < self.vocab_start_idx) | (tokens > self.vocab_end_idx)\n",
    "        tokens[masks] = 0.\n",
    "        \n",
    "        embeddings = F.embedding(tokens, self.weight, padding_idx=0)\n",
    "        mask_idxs = torch.where(masks == True)[1]\n",
    "        embeddings[mask_idxs] = 0.\n",
    "        \n",
    "        dist.all_reduce(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b7370-cde5-4426-8dfc-5c1523d3248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process-based, thread-based, vectorization, stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ff151-edfb-4256-858f-095bba0551a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global distributed group, tensor, pipeline, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713659da-4c86-42ce-9a52-3ba671a3aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "api server, scheduler, ectd, control manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90247b0b-c080-4b11-8de7-3b1b258cbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: initialize a global distributed group\n",
    "step 2: initialize parallel groups\n",
    "step 3: set device\n",
    "step 4: set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22c31f-cffc-442b-8bc1-706eba4cbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: gather the weights\n",
    "step 2: set pre backward hook\n",
    "step 3: do forward\n",
    "step 4: set post backward hook\n",
    "step 5: release the irrelevant weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac5eca-c8b2-4a2c-88a3-b3d99c949079",
   "metadata": {},
   "source": [
    "step 1: determine a list of global ranks in that group\n",
    "step 2: if the process's global in that group, initialize parallel group\n",
    "step 3: get its local rank\n",
    "step 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9b4c1-c51e-4223-877b-4fee08609efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = attn_weights.diagonal(dim1=-2, dim2=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226b7c8-02e7-4d7a-867a-1114d455b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api server, sccheduler, ectd, control manager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6b96-3571-426b-afdf-f1c77fc6e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_pass_using_data_parallelism(model, input, device_ids, output_id):\n",
    "    models = nn.parallel.replicate(model)\n",
    "    inputs = nn.parallel.scatter(input)\n",
    "    \n",
    "    outputs = nn.parallel.parallel_apply(models, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834a0eb0-04d0-42dc-9ff2-e931048ff20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed.rpc as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77178105-f55c-477c-aaa8-f1cf55b0a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc.get_worker_info(worker_name).id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521cbc1-5e5d-4dc3-b704-1d153134876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps):\n",
    "        super().__init__()\n",
    "        self.adds = nn.Parameter(torch.zeros(features))\n",
    "        self.mults = nn.Parameter(torch.ones(features))\n",
    "        \n",
    "        self.mean = nn.Parameter(torch.zeros(features))\n",
    "        self.var = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11db82-1a2e-4532-aa07-cf997dbae7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = attn_weights.diagonal(dim1=-2, dim2=-1, offset=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18415357-e8b4-43de-a011-597bf2914a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: global dist group\n",
    "step 2: initialize parallel groups\n",
    "step 3: set device\n",
    "step 4: set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c9823-c11d-4063-8fbe-656912d3e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training task: training distribution, function\n",
    "base optimizer: sgd and archtecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
