{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0a56d0-0faa-450c-98aa-8fee05b48688",
   "metadata": {},
   "source": [
    "### MechInterp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd51a2-3fce-427d-9e0e-17a8d715d220",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfe5723-a17d-4ab0-aaf5-0462260dbded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17cb154-51dc-45b7-bc53-730786e53e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d15534-6643-4c47-b2c1-bd866677a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterSharding:\n",
    "    def __init__(self, param_groups, parallel_context):\n",
    "        self.param_groups = param_groups\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def shard(self):\n",
    "        world_size = self.parallel_context.get_world_size()\n",
    "        partitioned_params = [[] for _ in range(world_size)]\n",
    "        sizes = [0 for _ in range(world_size)]\n",
    "        \n",
    "        for param_group in self.param_groups:\n",
    "            param_list = [[] for _ in range(world_size)]\n",
    "            \n",
    "            for p in param_groups[\"params\"]:\n",
    "                next_rank = sizes.index(min(sizes))\n",
    "                param_list[next_rank].append(p)\n",
    "                sizes[rank] += p.numel()\n",
    "            \n",
    "            for rank, params in enumerate(param_list):\n",
    "                param_group_rank = copy.copy(param_group)\n",
    "                param_group_rank[\"rank\"] = params\n",
    "                \n",
    "                partitioned_params[rank].append(param_group_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f469b88-cc6b-49b9-815e-44ea6840affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, linear, attention, layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9c9ec-f320-4aa3-bd90-ee7d5a9293ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: determine the global rank of the current process\n",
    "step 2: resize embedding layer\n",
    "step 3: parallelize embedding, linear, attention, layer norm\n",
    "step 4: resize vocab space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c691cbe0-10bc-437f-8ceb-9edcfc54f463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wait_stream(source_stream, target_stream):\n",
    "    if isinstance(target_stream, torch.cuda.Stream):\n",
    "        if isinstance(source_stream, torch.cuda.Stream):\n",
    "            # GPU waits GPU\n",
    "            source_stream.wait_stream(target_stream)\n",
    "        else:\n",
    "            # CPU waits GPU\n",
    "            target_stream.syncronous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8cfffd-4bc1-4186-841a-b0c462b84894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Copy(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, prev_stream, next_stream, x):\n",
    "#         ctx.prev_stream = prev_stream\n",
    "#         ctx.next_stream = next_stream\n",
    "        \n",
    "#         wait_stream(\n",
    "#             source_stream=next_stream,\n",
    "#             target_stream=prev_stream\n",
    "#         )\n",
    "        \n",
    "#         return x\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad):\n",
    "#         prev_stream = ctx.prev_stream\n",
    "#         next_stream = ctx.next_stream\n",
    "        \n",
    "#         wait_stream(\n",
    "#             source_stream=prev_stream,\n",
    "#             target_stream=next_stream\n",
    "#         )\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42faacb-1174-42f7-b8f6-c1c2f20326da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Copy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        compute_stream = torch.cuda.default_stream(next_stream.device)\n",
    "        \n",
    "        with torch.cuda.stream(prev_stream), torch.cuda.stream(next_stream):\n",
    "            moved_input = input.to(next_stream.device)\n",
    "            input.record_stream(prev_stream)\n",
    "            moved_input.record_stream(compute_stream)\n",
    "        \n",
    "        return moved_input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        compute_stream = torch.cuda.default_stream(prev_stream.device)\n",
    "        \n",
    "        with torch.cuda.stream(prev_stream), torch.cuda.stream(next_stream):\n",
    "            moved_grad_input = grad_input.to(prev_stream.device)\n",
    "            \n",
    "            grad_input.record_stream(next_stream)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316aec8c-c839-4a5c-afa2-0ef7df8e0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = embed + pos_embed + attn00 + attn01 + mlp0 + attn10 + attn11 + mlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8fd51-2c6e-4b97-873e-b7ce5fe99640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f8c89-6e26-438f-8c3b-22341847dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads\n",
    "        )\n",
    "        self.norm1 = ResidualLayerNorm(\n",
    "            d_model=d_model, dropout=dropout\n",
    "        )\n",
    "        self.mlp = PositionWiseFeedForward(\n",
    "            d_model=d_model, d_ff=d_ff, dropout=dropout\n",
    "        )\n",
    "        self.norm2 = ResidualLayerNorm(\n",
    "            d_model=d_model, dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        attn_output, att_weights = self.mha(\n",
    "            pre_q=embeddings,\n",
    "            pre_k=embeddings,\n",
    "            pre_v=embeddings\n",
    "        )\n",
    "        norm1 = self.norm1(attn_output, residual=embeddings)\n",
    "        mlp = self.mlp\n",
    "        norm2 = self.norm2(mlp, residual=norm1)\n",
    "        \n",
    "        return norm2, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92983947-b84f-4612-9d92-5740988df157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53083f5-9def-4038-95e2-1e8a44667889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, xb, yb):\n",
    "    logits = model(xb)\n",
    "    \n",
    "    logits = rearrange(logits, \"bs sq n_embed -> (bs sq) n_embed\")\n",
    "    yb = rearrange(yb, \"bs sq -> (bs sq)\")\n",
    "    \n",
    "    loss = F.cross_entropy(\n",
    "        logits,\n",
    "        yb\n",
    "    )\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1392620d-9cc7-471b-a5ec-24340ba39e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probability_scores(image_embedding, text_embedding):\n",
    "    image_norm = image_embedding.norm(dim=-1, keepdim=True)\n",
    "    image_embedding = image_embedding / image_norm\n",
    "    \n",
    "    text_norm = text_embedding.norm(dim=-1, keepdim=True)\n",
    "    text_embedding = text_embedding / text_norm\n",
    "    \n",
    "    similarities = image_embedding @ text_embedding.T\n",
    "    probs = F.softmax(similarities, dim=-1)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396cdf2-5bd0-43e0-afbc-f6947e878597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545e6b3-70ea-4047-acce-3fe0b11e47cc",
   "metadata": {},
   "source": [
    "step 1: elasticdriver spots a change in the worker nodes\n",
    "step 2: it sends a hostupdatedrequest to the notification service of the coordinating worker\n",
    "step 3: if there are changes in node, the notification service passes the request to the notification manager\n",
    "step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b94076-41e4-477f-9e91-2ef391336d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b925309-78a4-427c-9a12-3183c3e50faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFruit:\n",
    "    def setup_method(self):\n",
    "        self.fruit = Fruit(name=\"banana\")\n",
    "    \n",
    "    def test_init(self):\n",
    "        assert self.fruit.name == \"banana\"\n",
    "    \n",
    "    def teardown_method(self):\n",
    "        del self.fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1a437f-7a7d-44fd-8fbc-2e6fcf2cff8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584d86d-137e-48a6-bd2c-cb6da4986eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvenSampler(Sampler):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return [x for x in range(0, len(self.data), 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049d856-530a-4c9b-9ca9-4f1f97eb7635",
   "metadata": {},
   "source": [
    "step 1: normalize the loss with current_epoch / n_epoch\n",
    "step 2: calculate the gradients with respect to the normalized loss\n",
    "step 3: accumulate the gradients\n",
    "step 4: if current_epoch == n_epoch, update, otherwise, repeat step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf886cb0-9af1-44e0-b7ae-ab2ca75de343",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticdriver, torchstate, 3notification, hostdiscovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914d6c12-1197-4093-bc20-e89fa7e7cbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Copy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        \n",
    "        compute_stream = torch.cuda.default_stream(next_stream.device)\n",
    "        \n",
    "        with torch.cuda.stream(prev_stream), torch.cuda.stream(next_stream):\n",
    "            moved_input = input.to(next_stream.device)\n",
    "            \n",
    "            input.record_stream(prev_stream)\n",
    "            moved_input.record_stream(compute_stream)\n",
    "        \n",
    "        return moved_input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        prev_stream = ctx.prev_stream\n",
    "        next_stream = ctx.next_stream\n",
    "        \n",
    "        compute_stream = torch.cuda.default_stream(prev_stream.device)\n",
    "        \n",
    "        with torch.cuda.stream(prev_stream), torch.cuda.stream(next_stream):\n",
    "            moved_grad_input = grad_input.to(prev_stream.device)\n",
    "            \n",
    "            grad_input.record_stream(next_stream)\n",
    "            moved_grad_input.record_stream(compute_stream)\n",
    "        \n",
    "        return None, None, moved_grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75846e-c75f-44e8-b9b5-22d38d354cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef6500a-ce85-4bc8-a528-b594562d5897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestFruit:\n",
    "    def setup_method(self):\n",
    "        self.fruit = Fruit(\"x\")\n",
    "    \n",
    "    def test_fruit(self):\n",
    "        assert self.fruit.name == \"x\"\n",
    "    \n",
    "    def teardown_method(self):\n",
    "        del self.fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197b7d30-0536-4f14-a564-59827bffec52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac627e60-7f7b-4942-9ca9-07d261a4ab28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\n",
    "    \"test_input, expected\"\n",
    ")\n",
    "def test_square(test_input, expected):\n",
    "    assert square(test_input) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f85791-7197-4613-9ac9-ac90f1a0ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: record all the interdimate activations\n",
    "step 2: analyze attention patterns\n",
    "step 3: spot induction heads\n",
    "step 4: decompose the attetion scores of the induction heads\n",
    "step 5: identify pair q-k that produces that induction charaterstic\n",
    "step 6: trace backward\n",
    "step 7: construct the full circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe121618-de73-4665-8eaf-9f27cdeb6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prob[0] = sigmoid(logit0 - logit1)\n",
    "step 2: logit0 = resid2 @ W_U[0], logit1 = resid2 @ W_U[1]\n",
    "step 3: logit0 - logit1 = resid2 @ (W_U[0] - W_U[1])\n",
    "step 4: resid2 = resid1 @ ln1 @ W_OV^{1, 0}\n",
    "step 5: resid1 @ ln1 @ W_OV^{1, 0} @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003be27-ddee-47b5-991a-6f289c7fc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(board_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9deff-0f1f-497a-9e15-5ef29b236fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_acts = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66161a22-eb73-4a1f-aecc-e608058da01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx = 5\n",
    "neuron_idx = 1393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa80894-1b4c-47c4-ae28-eceb382776ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts = mlp_acts[:, neuron_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac91f5e-e67d-4971-ba2e-3fb91418407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threashold = neuron_acts.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622d9ca-3574-4937-a00c-d01d547c16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_idxs = neuron_acts > threashold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efe716-9ff3-40fc-83a0-02620787dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(board_states == 1)[:, :, move_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8db44d-736a-4939-8e6a-c38613050846",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = torch.zeros(seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9661f7e9-518a-4f65-bbbb-c56fac22d00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrupted_prompt = \"X told Y: 'Persistence is all you need.' Z replied back to \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb771bc-9b29-4a6f-8250-5c6feaffa49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7df55b29-6f48-47ba-990f-0f55771f406c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 6, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b4cf-da59-4523-b11b-0ef4d799bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pos = model.W_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c89fb1-8d92-4fee-ab2c-040cd1c2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_one = W_pos[:, 0]\n",
    "pos_two = W_pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b73c49-6b41-4cfc-bf6c-7d93bdc305fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_one /= pos_one.norm(dim=-1, keepdim=True)\n",
    "pos_two /= pos_two.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8db3e-7cfd-43c4-9642-a8d0e5c0b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47880599-e010-4d22-bd85-5ed623b85d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(board_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ed0f7f-35b5-4807-9a58-555ade0cdb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = \"blocks.5.mlp.hook_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1824a3-efcd-41fe-9f73-dc57485fceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_acts = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55665d-4b53-4f6f-b2b7-03634c13cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts = mlp_acts[:, 1393]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4ae5e-e01f-45c1-bd86-e30fe03c268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A^1 @ x @ W_OV^1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62207d1-8ede-4ac4-b2bb-a0d5fe3d1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(past_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c90c9-ccfe-4e53-975f-a77beb975712",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_acts = cache[hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e539c-ca47-4e2b-948e-a3bc12ee7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neurons = mlp_acts.std(dim=[0, 1]).argsort(descending=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603a7f57-9307-4196-a4d4-a01c3d3d85f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12ee892-1f66-46c9-9684-9f627045e955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd711bb-06d5-441a-a741-b13f852c7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consine_similarity(neuron_idx, feature):\n",
    "    W_out = model.W_out[layer_idx, neuron_idx, :]\n",
    "    W_out /= W_out.norm(dim=-1, keepdimt=True)\n",
    "    \n",
    "    feature /= feature.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return einsum(\n",
    "        W_out,\n",
    "        feature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c54b8-6291-43df-8f8b-8e4237c303a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_blanks = []\n",
    "for neuron_idx in top_neurons:\n",
    "    heatmap_blanks.append(calculate_consine_similarity(\n",
    "        neuron_idx,\n",
    "        blank_dir\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc1562-a827-441c-a06d-13cf020d59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed + emed + sum(12 heads in layer 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a873f-d54d-4700-ab4d-690128dfe7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: prob[0] = sigmoid(logit0 - logit1)\n",
    "step 2: logit0 = resid @ W_U[0], logit1 = resid @ W_U[1]\n",
    "step 3: logit0 - logit1 = resid @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f97339-9eef-4897-9a77-b22ab81701cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A@x@W_OV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0143a9-7b3d-46d9-bcdf-ddcab9d958aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal = x.diagonal(dim1=-2, dim2=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66237ca2-dd93-438f-aa39-ec931f22d659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrupted_prompt = \"X told Y: 'Persistence is all you need.' Z replied back to \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ad8f2-d3e0-454e-be6f-f560f1e5baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1e95e-4358-4e09-b1fb-12378bee73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df480fa3-e2bc-4d99-b179-71b94b461229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc97eb3-4d4e-4b4b-bc34-4e0348323e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, head_idx = 6, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e2a6a29-99b9-44f9-bd12-4b612c91f84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"result\", layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3ebeb-7e83-4b21-9d16-649795405496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_acts = corrupted_cache[hook_name][:, head_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7602df29-63a8-437e-87f2-9b7f041a1241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_corrupted_sender_activations(activations, hook):\n",
    "    activations = sender_acts\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1f622-a4ab-4673-970b-4f0787202b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    clean_tokens,\n",
    "    fwd_hooks=[(hook_name, patch_corrupted_sender_activations)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b879a2-6edf-4872-a527-cf804088a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_acts = cache[receiver_hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a53f2d-17a3-492e-b1ed-3432312065e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_receiver_acts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b944c-c29f-42df-a8ee-90e1d7a297c1",
   "metadata": {},
   "source": [
    "step 1: a clean prompt, a corrupted prompt\n",
    "step 2: record all the interdimate activations of the clean prompt and corrupted prompt\n",
    "step 3: choose a sender component, and a receiver component\n",
    "step 4: run the corrupted prompt and record the activations of the sender\n",
    "step 5: run the clean prompt and patch the corrupted sender activations\n",
    "step 6: record the receiveer activations from step 5\n",
    "step 7: run the clean prompt again and patch the receiver activations from step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec8364-b686-4b02-ae87-e83eeba6d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A^{1}@x@W_OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09b098-de7a-46d1-bc6d-1ba40ed4b7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c61eea-1929-4c2a-883d-e560e4043cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create a model and an optimizer\n",
    "model = SimpleNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step the optimizer to initialize its internal states\n",
    "input_tensor = torch.randn(5, 10)\n",
    "output = model(input_tensor)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Check if the optimizer states reference the same memory as the model parameters\n",
    "for group in optimizer.param_groups:\n",
    "    for param in group['params']:\n",
    "        param_memory = param.data_ptr()\n",
    "        optimizer_memory = optimizer.state[param].get('momentum_buffer')\n",
    "        \n",
    "        break\n",
    "        print(f\"Param memory: {param_memory}, Optimizer state memory: {optimizer_memory}\")\n",
    "        if param_memory == optimizer_memory:\n",
    "            print(\"Memory references are the same.\")\n",
    "        else:\n",
    "            print(\"Memory references are different.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a28af0fd-c483-4b23-b797-6ee895c2885e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46c59b-8f59-45a1-b930-abaf804a8475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
