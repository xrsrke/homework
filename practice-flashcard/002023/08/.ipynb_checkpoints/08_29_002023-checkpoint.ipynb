{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f93e144-f076-4b23-a5ce-f9319db70104",
   "metadata": {},
   "source": [
    "### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d144ed47-3dd2-490d-ad7d-df19f00bac4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b05745-0796-49ec-a3ba-01938877cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync, data, handshake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58e50a-fc8c-4549-b453-241ad6df6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterSharding:\n",
    "    def __init__(self, params_groups, parallel_context):\n",
    "        self.params_groups = params_groups\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def shard(self):\n",
    "        world_size = self.parallel_context.get_world_size()\n",
    "        params_per_rank = [[] for _ in range(world_size)]\n",
    "        numel_per_rank = [0 for _ in range(world_size)]\n",
    "        \n",
    "        for param_group in self.param_groups:\n",
    "            # partitioned params per rank\n",
    "            params = [[] for _ in range(world_size)]\n",
    "            for p in param_groups[\"params\"]:\n",
    "                next_rank = numel_per_rank.index(min(numel_per_rank))\n",
    "                params[next_rank].append(p)\n",
    "                numel_per_rank[next_rank] += p.numel()\n",
    "            \n",
    "            # now assign those partitioned\n",
    "            for p in params:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23504064-f5bc-4ea6-8d52-140c47b7c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1c8c0-55d4-41d8-8dec-7b2d5d904caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_max = torch.max(xs)[0]\n",
    "global_max = dist.all_reduce(\n",
    "    local_max,\n",
    "    op=dist.ReduceOp.MAX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18ae27-fa91-442c-a71f-38c25d62647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_xs = xs - global_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb906b5c-10ad-4fdd-87d5-f85b8abe5be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78e2c76-bca5-4b83-afe3-b9747f5bfd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _ParallelCrossEntropy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, parallel_logits, targets, parallel_context):\n",
    "        def get_vocab_range(partition_size, rank):\n",
    "            start_idx = partition_size*rank\n",
    "            end_idx = start_idx+partition_size\n",
    "            return start_idx, end_idx\n",
    "    \n",
    "        def get_predicted_logits(parallel_logits, targets):\n",
    "            partition_size = parallel_logits.shape[-1]\n",
    "            rank = parallel_context.get_local_rank(ParallelMode.TENSOR)\n",
    "            vocab_start_idx, vocab_end_idx = get_vocab_range(partition_size, rank)\n",
    "            \n",
    "            target_mask = (targets < self.vocab_start_idx) >= (targets > self.vocab_end_idx)\n",
    "            masked_targets = targets.clone() - self.vocab_start_idx\n",
    "            masked_targets[target_mask] = 0\n",
    "            \n",
    "            masked_targets_1d = rearrange(\n",
    "                targets, \"b s -> (b s)\"\n",
    "            )\n",
    "            parallel_logits = rearrange(\n",
    "                parallel_logits, \"b s v -> (b s) v\"\n",
    "            )\n",
    "            predicted_logits = parallel_logits[torch.arange(masked_targets_1d.size(0)), masked_targets_1d]\n",
    "            predicted_logits[masked_targets_1d] = 0.\n",
    "            \n",
    "            predicted_logits = all_reduce(\n",
    "                predicted_logits,\n",
    "                parallel_context=parallel_context,\n",
    "                parallel_mode=ParalellMode.TENSOR\n",
    "            )\n",
    "            return predicted_logits\n",
    "        \n",
    "        predicted_logits = get_predicted_logits(parallel_logits, targets)\n",
    "        exp_logits = all_reduce(\n",
    "            parallel_logits.exp().sum(dim=-1),\n",
    "            parallel_context=parallel_context,\n",
    "            parallel_mode=ParallelMode.TENSOR\n",
    "        )\n",
    "        \n",
    "        loss = exp_logits.log() - predicted_logits\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbb95ef-116f-4dcb-aafa-e4a8ede10220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelCrossEntropy(nn.Module):\n",
    "    def __init__(self, parallel_context):\n",
    "        super().__init__()\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = _ParallelCrossEntropy.apply(\n",
    "            logits, targets, self.parallel_context\n",
    "        )\n",
    "        \n",
    "        loss /= len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e3c310-b4c3-4dbb-bb26-b64d6b35cac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03887b39-a68d-400b-8735-fcec737508c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def use_stream(stream):\n",
    "    if not isinstance(stream, torch.cuda.Stream):\n",
    "        yield\n",
    "        return\n",
    "    \n",
    "    with torch.cuda.stream(stream):\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5ff71-d143-415e-84ed-3cd5c1c83a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4f02f-310a-49fd-8bf0-9cb63a67fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start.record()\n",
    "\n",
    "hardshit()\n",
    "\n",
    "end.record()\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5a4c5-c9f4-4c09-95dc-56b6ea2bf0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = start_event.elapsed_time(end_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522be6ce-6bf7-409a-bcd0-f805e4adccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int *h_a, *h_b, *h_c;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f3eb1-4efb-4ad7-a303-b897153a1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_t bytes = sizeof(int)*n;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0301889-38f9-40d8-b4a7-90c26ed969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a = (int*)malloc(bytes)\n",
    "h_b = (int*)malloc(bytes)\n",
    "h_c = (int*)malloc(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa134da-1fc7-40fd-9969-c32bcb40f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticdriver, torchstate, 3notifications, hostdiscovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff2be90-ce8d-4e06-ac9b-d4fd4511d2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Copy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prev_stream, next_stream, input):\n",
    "        ctx.prev_stream = prev_stream\n",
    "        ctx.next_stream = next_stream\n",
    "        compute_stream = torch.cuda.default_stream(\n",
    "            next_stream.device\n",
    "        )\n",
    "        \n",
    "        with torch.cuda.stream(prev_stream), torch.cuda.stream(next_stream):\n",
    "            moved_input = input.to(next_stream.device)\n",
    "            \n",
    "            input.record_stream(prev_stream)\n",
    "            moved_input.record_stream(next_stream)\n",
    "        \n",
    "        return moved_input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        compute_stream = torch.cuda.default_stream(ctx.prev_stream.device)\n",
    "        \n",
    "        with torch.cuda.stream(ctx.prev_stream), torch.cuda.stream(ctx.next_stream):\n",
    "            moved_grad = grad_input.to(ctx.prev_stream.device)\n",
    "            \n",
    "            grad_input.record_stream(ctx.next_stream)\n",
    "            grad_input.record_stream(ctx.prev_stream)\n",
    "        return moved_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683947b-2323-446d-a1d4-27bd84d05847",
   "metadata": {},
   "outputs": [],
   "source": [
    "message passing, file system, shared memor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b6d32-0904-4803-9a5c-7d13fe32172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ W_E @ W_Q @ W_K @ W_E @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015b1f5-a66e-477c-b9c8-7d5bc1104a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: num b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53652c-3bdd-4158-9605-da42e0c884e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = []\n",
    "for hook_func in hooks:\n",
    "    handles.append(model.ln_f.register_forward_pre_hook(hook_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34e9f3-2065-4fd1-96ea-465f4d867eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles[1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaba78ff-6147-4d51-aff2-e12b6fcb2cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_score(pattern, target_pattern):\n",
    "    return (pattern*target_pattern).sum() / (pattern.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4b6d6-fabe-4918-a0f7-6072349a5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(text)\n",
    "_, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625d437-8cd9-453b-a5ff-ce43e14f0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(n_layers, n_heads)\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    for head_idx in range(n_heads):\n",
    "        hook_name = f\"blocks.{layer_idx}.attn.hook_pattern\"\n",
    "        pattern = cache[hook_name][:, head_idx]\n",
    "        data[layer_idx, head_idx] = compute_score(\n",
    "            pattern, target_pattern\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6467f1-b7a6-4090-98c3-69a5346705f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_2 @ W_E @ W_Q @ W_K @ W_E @ [v0, v_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb422333-c88f-41f2-adb0-24271abc843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1:  logit0 - logit1\n",
    "step 2: logit0 = final_resid @ W_U[0], logit1 = final_resid @ W_U[1]\n",
    "step 3: logit0 - logit1 = final_resid @ (W_U[0] - W_U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945faff6-8b54-4626-8339-88e358e8d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_direction = model.W_in[layer_idx, :, neuron_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12afd5f-d638-4758-914b-3e492543a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(x @ W_Q @ W_K.T @ x) @ x @ W_V @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53f230-7753-43dc-a534-5c88d0b09fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "W_V = model.W_V[layer_idx, head_idx]\n",
    "W_O = model.W_O[layer_idx, head_idx]\n",
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c01fab-181c-4811-a13c-43097e481637",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_OV_circuit = W_E @ W_V @ W_O @ W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d48df4-9bb8-4313-81e1-416cec0f6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.W_E\n",
    "W_Q = model.W_Q[1, 4]\n",
    "W_K = model.W_K[1, 4]\n",
    "W_O = model.W_O[0, 7]\n",
    "W_V = model.W_V[0, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a1d25-3e2e-4a31-99f3-fa44d02bcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = W_E @ W_Q\n",
    "K = W_E @ W_V @ W_O @ W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef4906-ca21-4c3b-aea8-e0b78dff3b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dc5a3-370c-4e0b-9b31-8c620d2cf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381f28a-d39b-4ac9-8c8e-15b362429c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(past_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edbbe2-9397-4a84-8b9f-bc3c182e290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neurons = cache[\"post\", layer_idx].std(dim=[0, 1]).argsort(descending=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f293e3-08af-4250-873b-2c5cb3e7054e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a5d134-0f6f-468b-b3d5-e76a91b37ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_consine_similarity(neuron_idx, feature):\n",
    "    W_out = model.W_out[layer_idx, neuron_idx]\n",
    "    W_out /= W_out.norm(dim=-1)\n",
    "    \n",
    "    feature /= feature.norm(dim=-1)\n",
    "    \n",
    "    return einsum(\n",
    "        W_out, feature,\n",
    "        \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee92fd4-ec92-4b9b-a927-1c7564d0ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_blank = []\n",
    "for neuron_idx in top_neurons:\n",
    "    heatmap_blank.append(compute_consine_similarity(\n",
    "        neuron_idx,\n",
    "        feature=blank_dir\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fe411-e028-4c8b-8067-1cf32eede616",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(board_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b14b8b-6801-4c2b-b734-e3e45115ab9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_idx, neuron_idx = 5, 1393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfdf233-8f0e-4fbe-b3a6-722a07eb2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_neurons = cache[hook_name][:, :, neuron_idx]\n",
    "threshold = mlp_neurons.quantile(0.99)\n",
    "top_neurons = mlp_neurons > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64df5b7-ef46-42e1-9536-66936e408868",
   "metadata": {},
   "outputs": [],
   "source": [
    "(board_states == 2)[top_neurons].float().mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32964a4f-71ec-477d-b80a-4273f145e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: accumul\n",
    "step 2: calcualte the logit difference direction\n",
    "step 3: proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78113f-398e-4565-a3e1-acfd4bd0d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: duplication heads detect duplicated tokens, write the information to S\n",
    "step 2: s-inhibition heads move that information to ENd\n",
    "step 3: name mover heads bsaed on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50092b-56cf-4234-aec4-d9d32c070b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokens = model.to_tokens(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060bd1a-75cb-42d2-b7de-65460f5d4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.embed\n",
    "mlp0 = model.blocks[0].mlp\n",
    "ln1 = model.blocks[0].ln1\n",
    "unembed = model.unembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada32a85-7e3f-4711-84b0-ba6be52116fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = embed(name_tokens)\n",
    "resid_after_mlp0 = text_embeddings + mlp0(ln1(text_embeddings)a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85dc1c3f-4297-4ba6-9af7-19358d1ec173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483500ed-58f2-4ae2-9f3b-73a8b4688965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid2logits(resid):\n",
    "    return model.unembed(model.ln_final(resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633eba2-b6e8-469b-901d-51076dade101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(n_layers):\n",
    "    for head_idx in range(n_heads):\n",
    "        W_OV = model.W_V[layer_idx, head_idx] @ \\\n",
    "            model.W_O[layer_idx, head_idx]\n",
    "        \n",
    "        resid = resid_after_mlp0 @ W_OV\n",
    "        logits = resid2logits(resid)\n",
    "        top_predictions = torch.topk(top_logits, dim=-1, k=5)\n",
    "        percentage = (top_predictions == name_tokens).any().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5f516-6c06-4744-b1ba-c6cad82edc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "register > cache > ram > hardrive > external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8bb364-127e-4128-b578-f0677210b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(local_rank - 1) % local_world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81391524-0b69-421b-a9a2-c1df18ded187",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: acts\n",
    "step 2: split\n",
    "step 3: local self attention\n",
    "step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00bc27-8cde-46c4-a13b-47214e66dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "step 1: wait inp\n",
    "step 2: get inp\n",
    "step 3: construct\n",
    "step 4: put\n",
    "step 5: wait oup\n",
    "step 6: get\n",
    "step 7: put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44778730-f14d-4738-bdae-1857a7ddbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step 1: initialize global distributed group\n",
    "step 2: initialize parallel groups\n",
    "step 3: set device\n",
    "step 4: set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201ef50d-a4e7-4265-b6db-1d6515a52ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58899135-9213-4cc0-9987-2d6ead1ac83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event = threading.Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbaf92f-0ae0-4b51-b7fd-e09a997c6557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"wait\")\n",
    "    event.wait()\n",
    "    print(\"received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13c012-9e73-46b8-a0fd-6cc06787456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce41a8-49d3-4bcb-b4c3-d26b1b64764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5239fa-79fa-49bc-8a06-6f31a9d819ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterSharding:\n",
    "    def __init__(self, param_groups, parallel_context):\n",
    "        self.param_groups = param_groups\n",
    "        self.parallel_context = parallel_context\n",
    "    \n",
    "    def shard(self):\n",
    "        world_size = parallel_context.get_world_size()\n",
    "        params_per_rank = [[] for _ in range(world_size)]\n",
    "        numel_per_rank = [0 for _ in range(world_size)]\n",
    "        \n",
    "        for param_group in param_groups:\n",
    "            # partitioned params of the current group\n",
    "            param_lists = [[] for _ in range(world_size)]\n",
    "            \n",
    "            for p in param_groups[\"params\"]:\n",
    "                next_rank = numel_per_rank.index(min(numel_per_rank))\n",
    "                param_lists[next_rank].append(p)\n",
    "                numel_per_rank[next_rank] += p.numel()\n",
    "            \n",
    "            for rank, params for param_lists:\n",
    "                param_g = copy.copy(param_group)\n",
    "                param_g[\"params\"] = params\n",
    "                params_per_rank[rank].append(param_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3aa2c6-5b4b-452b-9584-abbef2263921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d57ade-7ce5-4afc-ace4-9606d1d4b51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_distribution(states):\n",
    "    q_values = q_function(states)\n",
    "    return q_values.exp() / q_values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915f1e9-61c0-414b-8692-81265f079d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "record, send, memory, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa9766-acfb-4f44-a275-96cb05d1fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(N_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        states = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f5a387d-dfd4-47e9-b36a-cc1482034cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7568701-7e01-4011-85d0-b4e94ae027b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device=torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feed8cf-82a2-45a5-9b2d-e4c72eccea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
