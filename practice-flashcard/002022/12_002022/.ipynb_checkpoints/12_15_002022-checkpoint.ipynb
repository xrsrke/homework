{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba5d33c-db15-4092-b4d8-8d96d46ab23b",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e196d4-4d37-43aa-b484-c549d2782b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942e9ae3-a283-42fb-8d2f-b89da175b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8757c7c-b651-4f14-a615-effc53461428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum('ij->ji', a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1540c156-1399-46d0-b985-d90fe2d3b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dae4d45-200f-4374-ba5c-cd9729ec421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, mom, eps):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.adds = nn.Parameter(torch.zeros(1))\n",
    "        self.mults = nn.Parameter(torch.ones(1))\n",
    "        self.register_buffer('means', torch.zeros(1))\n",
    "        self.register_buffer('vars', torch.ones(1))\n",
    "    \n",
    "    def update_stats(x):\n",
    "        mean, var = x.mean(dim=-1), x.var(dim=-1)\n",
    "        self.means.lerp_(mean, self.mom)\n",
    "        self.vars.lerp_(var, self.mom)\n",
    "        return mean, var\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            mean, var = self.update_stats(x)\n",
    "        \n",
    "        # normalized\n",
    "        x = (x - mean) / (var + self.eps).sqrt()\n",
    "        \n",
    "        return self.mults * x + self.adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fbd27-df9a-486e-abe1-4528bff462ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "einsum('ij->ji', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8f427-a007-4776-8742-131a0a254de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.masked_fill_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092d34de-f6e2-433f-a755-aac865ec2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n, d_model):\n",
    "        super().__init__()\n",
    "        self.n, self.d_model = n, d_model\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        n_token = len(tokens)\n",
    "        embedding = torch.zeros(n_token, self.d_model)\n",
    "        \n",
    "        for p in range(n_token):\n",
    "            for i in range(self.d_model):\n",
    "                denominator = torch.pow(self.n, (2*i)/self.d_model)\n",
    "                embedding[p][i] = torch.sin(p, denominator) if i % 2 == 0 else torch.cos(p, denominator)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ec4b07-f6a2-4d7f-8b77-127635edfb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        # shape(k) = [batch_size, n_heads, n_words, d_head]\n",
    "        perumuted_k = k.permute(3, 2)\n",
    "        scores = torch.matmul(q, k)\n",
    "        scaled_scores = scores / math.sqrt(self.d_head)\n",
    "        attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30bfcb-9f21-48e8-a6f9-5b4334ca3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return x.exp() / 1 + x.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b2d96-c8a8-4907-ae72-75e745621868",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2, ___, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977e964-e663-48b5-8fea-9df3470236cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(tokens):\n",
    "    return { for w, i in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d8e0aa1-8f1e-461b-8555-7ffafa08149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    softmax = x.exp() / x.exp().sum()\n",
    "    return softmax.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebfb5e-7e46-4326-a926-5d12771255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors = nn.Embedding(n_users, 5)\n",
    "movie_factors = nn.Embedding(n_users, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9729438c-3c62-4620-9a65-96a55bb90276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(pred, targ):\n",
    "    return (pred - targ).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc0191-a672-41c3-ad52-c895f30fd16a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2472e2c2-601a-4331-90a6-8b9e575b2aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mN_EPISODES\u001b[49m):\n\u001b[1;32m      2\u001b[0m     observation, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m     observation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(observation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "for i_episode in range(N_EPISODES):\n",
    "    observation, _ = env.reset()\n",
    "    observation = torch.from_numpy(observation)\n",
    "    in_progress = True\n",
    "    \n",
    "    while in_progress:\n",
    "        predicted_rewards = model(observation)\n",
    "        action = torch.argmax(predicted_rewards, dim=-1)\n",
    "        new_observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            in_progress = False\n",
    "            observation, _ = env.reset()\n",
    "            observation = torch.from_numpy(observation)\n",
    "        else:\n",
    "            new_observation = torch.from_numpy(new_observation)\n",
    "            predicted_next_rewards = model(new_observation)\n",
    "            max_predicted_next_reward = torch.max(predicted_next_rewards, dim=-1)\n",
    "            target_reward = reward + GAMMA * max_predicted_next_reward\n",
    "            loss = loss_func(predicted_rewards[action], target_reward)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            observation = new_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dd3321c-a4c0-4ed1-812a-3b2ea5d0e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edadbf2b-4e56-41b4-8101-902de98113cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4899e79d-95d3-4ff5-9262-517d0517b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        self.w_k = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        self.w_v = nn.Linear(d_model, self.d_head * n_heads)\n",
    "        \n",
    "        self.linear = nn.Linear(self.d_head * n_heads, d_model)\n",
    "    \n",
    "    def split(self, x):\n",
    "        batch_size, n_words, d_model = x.size()\n",
    "        return x.view(batch_size, self.n_heads, n_words, self.d_head)\n",
    "    \n",
    "    def concat(self, x):\n",
    "        batch_size, n_heads, n_words, d_heads = x.size()\n",
    "        return x.view(batch_size, n_words, n_heads * d_heads)\n",
    "    \n",
    "    def forward(self, pre_q, pre_k, pre_v):\n",
    "        q, k, v = self.w_q(pre_q), self.w_k(pre_k), self.w_v(pre_v)\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        \n",
    "        output, attention_weights = self.attention(q, k, v)\n",
    "        \n",
    "        output = self.concat(output)\n",
    "        projection = self.linear(output)\n",
    "        \n",
    "        return projection, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b21f9846-5b8b-4032-967a-62851ff1b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb5c29be-bd7f-4659-8bdf-deac9d17e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b4b52-b987-41a4-ac72-bbddc2d2e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_heads):\n",
    "        super().__init__()\n",
    "        self.d_heads = d_heads\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        permuted_k = k.permute(3, 2)\n",
    "        scores = torch.matmul(q, k)\n",
    "        scaled_scores = scores / math.sqrt(self.d_head)\n",
    "        attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b5f0d-4fbf-420a-8698-ae7cb1c462de",
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_tensors = torch.stack([tensor(Image.open(o)) for o in sevens], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "872e5de9-fb95-4115-9e94-8f6a200062ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, targ):\n",
    "    return (pred - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f060ef5-c734-44c8-be08-f12b037e744f",
   "metadata": {},
   "source": [
    "### Fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8ab70ab-69e7-4a62-be1a-f8d2ef3365ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f61589-e4e6-4655-a5f2-787c5859173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L(nums).apply(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b8fd7-aefa-4faa-a5a3-f58dcb775d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "L(b).gotattr('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594b307-c55e-4aa2-b866-70d61dc8223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict actual + gamma next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256a6b0-e9a3-4f25-acab-aa099ce492a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, cache, cocurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd66839-a7b8-421f-b4a1-fa3c65f98387",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual + gamma * highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d0740-72d3-42f3-88cf-dfd3b339911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action reward, next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c66aa5-7378-4b84-9f5c-fe30967bbc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
