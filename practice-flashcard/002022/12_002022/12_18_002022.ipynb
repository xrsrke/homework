{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2838e4-48e7-4ece-bee7-27d3d4a48a96",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3bf77-3427-4868-a23d-6cf25dc61159",
   "metadata": {},
   "source": [
    "SELECT users.name, likes.like FROM users OUTER JOIN likes ON users.id == likes.user_id "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc4878-9948-46d5-9122-c8ede18e4332",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2ee278-18df-40e6-971e-63a37632b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0c11d8-d11c-4adf-9c68-fed4dd86f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def book():\n",
    "    return Book(\"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ab0de1-0c2a-42be-a591-c140becf6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_book(book):\n",
    "    assert book.name == \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bae0940-d171-405f-a521-b5aca02dbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UppercaseTuple(tuple):\n",
    "    def __new__(self, items):\n",
    "        # uppercase\n",
    "        return super().__new__(items)\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.items = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06faadd-b95f-42f9-a609-c9f4d596f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\n",
    "    'test_input, output',\n",
    "    (\n",
    "        [1, 1],\n",
    "        [2, 4]\n",
    "    )\n",
    ")\n",
    "def test_square(test_input, output):\n",
    "    assert square(test_input) == output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006e989a-3fa3-4f7e-ba95-df6e037ca283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27c822a-31e1-478b-8fd8-e0ec2bc5bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85ad5d-bc56-418c-86f0-61942bbcf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(multiple, numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30bd0-4609-4166-9db8-bf618b0c2509",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b411ad37-0124-4b83-8657-ad74336d8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d6b688-0ae0-42c7-b7aa-82f296c01fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ffab6b-3153-4f38-8fe0-f5f1c9c4406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_model,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embed(x)\n",
    "        return embeddings * math.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a524c1-5b9e-49b2-a607-f2ff26333851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        k_permuted = k.permute(3, 2)\n",
    "        qk_matmul = torch.matmul(q, k_permuted)\n",
    "        scores = qk_matmul / math.sqrt(self.d_head)\n",
    "        attention_weights = F.softmax(scaled_qk_matmul, dim=-1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_weights.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470b711e-9906-4aa7-9a09-132723fe76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(self.dropout(x) + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5d9230-2d0e-4856-85bd-15613fa81784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.text_embedding = TextEmbedding(\n",
    "            vocab_size=10000, d_model=d_model,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model\n",
    "        )\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model, n_heads, d_ff, dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.text_embedding(x)\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        \n",
    "        output = embeddings\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            output, attention_weights = encoder_layers(output)\n",
    "    \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167df1fe-a02a-4281-b8aa-fba8a4ac405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(d_model, n_heads)\n",
    "        self.norm_1 = ResidualLayerNorm(d_model, dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, dropout)\n",
    "        self.norm_2 = ResidualLayerNorm(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, attention_weights = self.mha(x, x, x)\n",
    "        norm_1 = self.norm_1(output, x)\n",
    "        feed_forward = self.feed_forward(norm_1)\n",
    "        norm_2 = self.norm_2(feed_forward, norm_1)\n",
    "        return norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e24c46-98ef-4359-a5b3-2e5783ccf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f1d70-674e-4f82-be48-8255a113d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ef535-d6fd-4356-b04d-a14aeb083acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell state, input, hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc80271-4bc8-4e39-83cd-ed74b351b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clamp_min(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acd199-b3ee-48e1-bd38-37ba7aac406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward + gamma * highest next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b956d5-56eb-4770-ba51-91a260056374",
   "metadata": {},
   "outputs": [],
   "source": [
    "current state, action, next state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "190e2213-e77d-48bf-ba59-3c38dff59ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, discount_factor):\n",
    "    time_step = torch.arange(len(rewards))\n",
    "    discount_factors = discount_factor ** time_step\n",
    "    \n",
    "    discounted_reward = discount_factors * rewards\n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93640ad4-4b88-485e-ab3a-cee04235d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2a53357-0244-46c0-9b99-37880f6fd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = discount_reward(rewards, torch.tensor(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66964220-ebdb-4b05-b477-ac7693b302be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.9800, 2.9403, 3.8812])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "201b3ac6-2711-417b-b2f8-8b6fa35f769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transitions(model, env):\n",
    "    transitions = []\n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    \n",
    "    while True:\n",
    "        predicted_action = model(state)\n",
    "        action = torch.argmax(predicted_action, dim=-1)\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        transitions.append(\n",
    "            state, action, next_state, reward\n",
    "        )\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "        state = torch.from_numpy(next_state)\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5ea23-e94c-4670-a26b-669ff88dcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(preds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e1a83-47af-416e-bf10-8e81cb73e601",
   "metadata": {},
   "source": [
    "minimize the difference between the probability distribution of target rewards and the prob distribution of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc90785-cf7a-4b8c-8359-958376183980",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference on gpu, quantization, share gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be19419-1bae-478e-a30b-fe144b554836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        k_permuted = k.permute(3, 2)\n",
    "        qk_matmul = torch.matmul(q, k_permuted)\n",
    "        scaled_qk_matmul = qk_matmul / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk_matmul.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scaled_qk_matmul, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8720c5-0c4d-4f86-a6d1-413e19beace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_activation(loss, actionvation, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028a230-4f43-494c-89a4-c8a360712623",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual, norm, conv, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdc6250a-9261-473c-9477-262c32646ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(rewards, discount_factor):\n",
    "    total_return = torch.zeros(1)\n",
    "    \n",
    "    for i in len(rewards):\n",
    "        total_return += reward[i] * discount_factor ** i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d89f06-27c2-465a-8213-8bcfacc75521",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Flatten(start_dim=1, end_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900949b5-eca9-4fd5-b6f3-3dbb5d651914",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(3, 3, latent_height, latent_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8a772-b089-45f9-b446-f88deb045374",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c91ffa-10e1-4880-acdf-e360f20304a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[0].requires_grad = True\n",
    "model.classifier[3].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c7f01-c65e-442a-90e8-0bb3b49573ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = nn.Linear(4096, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
