{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc406ee9-9fd0-47d1-858e-a4da3d5f86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f0947-2744-4c8c-b655-9c3ea597007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83b8f3-a668-4eaa-854e-8cd92c694321",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36448af-4f78-4b4f-9fbd-59c47b251d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a2d26-5c17-4dd6-8bd5-1540dd1fee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_trainer_workers': 0,\n",
       " 'num_gpus_per_trainer_worker': 0,\n",
       " 'num_cpus_per_trainer_worker': 1,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'placement_strategy': 'PACK',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'disable_env_checking': False,\n",
       " 'is_atari': None,\n",
       " 'auto_wrap_old_gym_envs': True,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'sample_async': False,\n",
       " 'enable_connectors': True,\n",
       " 'rollout_fragment_length': 'auto',\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'validate_workers_after_construction': True,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'restart_failed_sub_environments': False,\n",
       " 'num_consecutive_worker_failures_tolerance': 100,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'compress_observations': False,\n",
       " 'enable_tf1_exec_eagerly': False,\n",
       " 'sampler_perf_stats_ema_coef': None,\n",
       " 'worker_health_probe_timeout_s': 60,\n",
       " 'worker_restore_timeout_s': 1800,\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1,\n",
       "  '_use_default_native_models': -1},\n",
       " 'optimizer': {},\n",
       " 'max_requests_in_flight_per_sampler_worker': 2,\n",
       " 'rl_trainer_class': None,\n",
       " '_enable_rl_trainer_api': False,\n",
       " '_rl_trainer_hps': RLTrainerHPs(),\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec>},\n",
       " 'policy_states_are_swappable': False,\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'offline_sampling': False,\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_sample_timeout_s': 180.0,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'evaluation_config': None,\n",
       " 'off_policy_estimation_methods': {},\n",
       " 'ope_split_batch_by_episode': True,\n",
       " 'evaluation_num_workers': 0,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'enable_async_evaluation': False,\n",
       " 'in_evaluation': False,\n",
       " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'metrics_episode_collection_timeout_s': 60.0,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_iteration': None,\n",
       " 'min_train_timesteps_per_iteration': 0,\n",
       " 'min_sample_timesteps_per_iteration': 0,\n",
       " 'export_native_model_files': False,\n",
       " 'checkpoint_trainable_policies_only': False,\n",
       " 'logger_creator': None,\n",
       " 'logger_config': None,\n",
       " 'log_level': 'WARN',\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'seed': None,\n",
       " 'worker_cls': None,\n",
       " 'rl_module_class': None,\n",
       " '_enable_rl_module_api': False,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'simple_optimizer': -1,\n",
       " 'replay_sequence_length': None,\n",
       " 'horizon': -1,\n",
       " 'soft_horizon': -1,\n",
       " 'no_done_at_end': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1,\n",
       " 'lambda': 1.0,\n",
       " 'input': 'sampler',\n",
       " 'multiagent': {'policies': {'default_policy': (None, None, None, None)},\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.__init__.<locals>.<lambda>(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': -1,\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'observation_fn': None},\n",
       " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       " 'create_env_on_driver': False,\n",
       " 'custom_eval_function': None,\n",
       " 'framework': 'tf',\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'num_workers': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe36c2e-75e7-4d7e-9438-c4038283d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"env\"] = \"CartPole-v1\"\n",
    "config[\"framework\"] = \"pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e216c9-0b90-4795-8944-444f70baa1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid_search': [32, 64, 128]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.grid_search([32, 64, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458cbd6-d1f3-4ba6-8966-37623bc2f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275fe22-fb15-42c7-a6e4-5b8e6736dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:32:39,926\tERROR services.py:1169 -- Failed to start the dashboard , return code 1\n",
      "2023-03-21 11:32:39,928\tERROR services.py:1194 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2023-03-21 11:32:39,929\tERROR services.py:1204 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/tmp/ray/session_2023-03-21_11-32-38_722895_14466/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.\n",
      "2023-03-21 11:32:39,935\tERROR services.py:1238 -- \n",
      "The last 20 lines of /tmp/ray/session_2023-03-21_11-32-38_722895_14466/logs/dashboard.err (it contains the error message from the dashboard): \n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/ray/__init__.py\", line 63, in _configure_system\n",
      "    import grpc  # noqa: F401\n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/grpc/__init__.py\", line 22, in <module>\n",
      "    from grpc import _compression\n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/grpc/_compression.py\", line 15, in <module>\n",
      "    from grpc._cython import cygrpc\n",
      "ImportError: dlopen(/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/grpc/_cython/cygrpc.cpython-38-darwin.so, 0x0002): symbol not found in flat namespace (_CFRelease)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/ray/dashboard/dashboard.py\", line 10, in <module>\n",
      "    import ray._private.ray_constants as ray_constants\n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/ray/__init__.py\", line 101, in <module>\n",
      "    _configure_system()\n",
      "  File \"/Users/education/miniforge3/envs/gym/lib/python3.8/site-packages/ray/__init__.py\", line 65, in _configure_system\n",
      "    raise ImportError(\n",
      "ImportError: Failed to import grpc on Apple Silicon. On Apple Silicon machines, try `pip uninstall grpcio; conda install grpcio`. Check out https://docs.ray.io/en/master/ray-overview/installation.html#m1-mac-apple-silicon-support for more details.\n",
      "\n",
      "2023-03-21 11:32:41,027\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop,\n",
    "    # checkpoint_at_end=True,\n",
    "    # checkpoint_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504b716-cbb4-4fe6-98bb-0024d3ac10bd",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69639560-772e-41fe-b410-9f2b83c7e189",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa016e35-d21e-467f-97fb-7faaceb380f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738672d9-1469-4d0d-a806-7299e0834232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3552a2d3-2a54-409f-8a77-77f42768cf04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea60037f-27c9-4262-8bc4-3ed2cfb2b8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:47:19,728\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=15443)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15444)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15443)\u001b[0m 2023-03-21 11:47:25,694\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=15444)\u001b[0m 2023-03-21 11:47:25,695\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2023-03-21 11:47:26,105\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m 2023-03-21 11:47:26,117\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m AttributeError: '_TFStub' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     base.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/policy/eager_tf_policy_v2.py\", line 77, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     \"GPU\" if get_gpu_devices() else \"CPU\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/tf_utils.py\", line 166, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15443)\u001b[0m AttributeError: '_TFStub' object has no attribute 'config'\n",
      "2023-03-21 11:47:26,180\tERROR actor_manager.py:496 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\n",
      "AttributeError: '_TFStub' object has no attribute 'config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in __init__\n",
      "    self._build_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1984, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 139, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "    base.__init__(\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/policy/eager_tf_policy_v2.py\", line 77, in __init__\n",
      "    \"GPU\" if get_gpu_devices() else \"CPU\"\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/tf_utils.py\", line 166, in get_gpu_devices\n",
      "    devices = tf.config.experimental.list_physical_devices()\n",
      "AttributeError: '_TFStub' object has no attribute 'config'\n",
      "2023-03-21 11:47:26,182\tERROR actor_manager.py:496 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15444, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa531f7f1f0>)\n",
      "AttributeError: '_TFStub' object has no attribute 'config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15444, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa531f7f1f0>)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in __init__\n",
      "    self._build_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1984, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 139, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "    base.__init__(\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/policy/eager_tf_policy_v2.py\", line 77, in __init__\n",
      "    \"GPU\" if get_gpu_devices() else \"CPU\"\n",
      "  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/tf_utils.py\", line 166, in get_gpu_devices\n",
      "    devices = tf.config.experimental.list_physical_devices()\n",
      "AttributeError: '_TFStub' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m 2023-03-21 11:47:26,166\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15444, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa531f7f1f0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m AttributeError: '_TFStub' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15444, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa531f7f1f0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     self._build_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1984, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 139, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     base.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/policy/eager_tf_policy_v2.py\", line 77, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     \"GPU\" if get_gpu_devices() else \"CPU\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m   File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/tf_utils.py\", line 166, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15444)\u001b[0m AttributeError: '_TFStub' object has no attribute 'config'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_TFStub' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:240\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_workers_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:614\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py:477\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     remote_results\u001b[38;5;241m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[38;5;241m=\u001b[39mresult))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/_private/worker.py:2382\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2382\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\nAttributeError: '_TFStub' object has no attribute 'config'\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=15443, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9ac997f1c0>)\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in __init__\n    self._build_policy_map(policy_dict=self.policy_dict)\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1984, in _build_policy_map\n    new_policy = create_policy_for_framework(\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 139, in create_policy_for_framework\n    return policy_class(observation_space, action_space, merged_config)\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n    base.__init__(\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/policy/eager_tf_policy_v2.py\", line 77, in __init__\n    \"GPU\" if get_gpu_devices() else \"CPU\"\n  File \"/Users/education/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/utils/tf_utils.py\", line 166, in get_gpu_devices\n    devices = tf.config.experimental.list_physical_devices()\nAttributeError: '_TFStub' object has no attribute 'config'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm_config.py:926\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    924\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:445\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     }\n\u001b[1;32m    443\u001b[0m }\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:571\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rlexp/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:192\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_TFStub' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "algo = PPOConfig().environment(env_name).framework(\"pytorch\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fde5c-350b-4b38-8b55-f6bd150a8580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "episode_reward = 0\n",
    "terminated = truncated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7797eb0-baec-4c2e-aabe-ddbb2b73dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
