{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3db20c-85eb-4810-a48e-51e3dbbaec9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22e66d-4ead-4ea8-9976-a285223924bf",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1e8a2-d0c5-41c1-b4e3-8e65ef65620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 4)\n",
    "weight = torch.randn(2, 4)\n",
    "bias = torch.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de02251-dc45-48c9-9b8f-1f1b6c571089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_model_parallel_group(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129452ce-fa08-4e00-9e15-087ce3bc23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b39c2-b210-4571-a299-e8cda5eae0f3",
   "metadata": {},
   "source": [
    "get_tensor_model_parallel_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b3ad8-bb4a-4c1e-8aed-2d45c5303742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([2, 4]), torch.Size([2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, weight.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62b32a-7be1-4626-a98c-e0d955bb74ef",
   "metadata": {},
   "source": [
    "Let `L` be the loss function. We want to compute the gradient of `L` with respect to the input, which we denote as `∂L/∂input`. Using the chain rule, we can write this gradient as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b68837-da53-402d-925b-3f6e89e4fa79",
   "metadata": {},
   "source": [
    "**Hints**: \n",
    "- `output = input @ weight.t() + bias`\n",
    "- `∂L/∂input = (∂L/∂output) * (∂output/∂input) = grad_output * weight.t()`\n",
    "- `∂L/∂weight = (∂L/∂output) * (∂output/∂weight) = grad_output.t() @ input`\n",
    "- `∂L/∂bias = grad_output.sum(dim=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1d9c1-14d7-4284-862d-c936bbf36d9b",
   "metadata": {},
   "source": [
    "Explain the distributed part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a1412-9a03-4fed-9056-ac22f9149f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinearWithcAllreduce(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        output = torch.matmul(input, weight.T) + bias\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight = ctx.saved_tensors\n",
    "        grad_input = torch.matmul(grad_output, weight)\n",
    "        \n",
    "        handle = torch.distributed.all_reduce(\n",
    "            grad_input, group=get_tensor_model_parallel_group(), async_op=True\n",
    "        )\n",
    "        \n",
    "        # ignored: deplay for 3us, to have all-reduce\n",
    "        # scheduled first and have GPU resources allocated\n",
    "        \n",
    "        grad_weight = torch.matmul(grad_output.T, input)\n",
    "        grad_bias = grad_output.sum(dim=0)\n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7705cb-68c6-450d-9ae2-de76d1189266",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_input, grad_weight, grad_bias = ColumnParallelLinearWithAsyncAllreduce.apply(input, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75facfb4-1419-4af3-bf10-34721b9cd6be",
   "metadata": {},
   "source": [
    "### `ColumnParallelLinear`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699adea5-f4a3-4d3d-bbc9-099db97d2ce9",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3927daf-6291-498f-888a-b39e56dc3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8d05b-2858-4653-a90b-e6806336b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3f05c-84a1-4706-a161-9e9c95b3aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [torch.tensor(0.), torch.tensor(1.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fb35c-bdf7-4550-bebe-a88a0815d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc4870-6ead-447c-8dd5-a8c95913d15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5d2d2-3200-413a-a2f4-46734a850299",
   "metadata": {},
   "source": [
    "Create a function that will gather all tensors in a distributed group. This script will be distributed across all processes. And explain each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b0d82-1055-4ede-ad4c-eb583015450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e44b32-5a7d-401f-9614-2ccc680edfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_tensors(x):\n",
    "    world_size = dist.get_world_size()\n",
    "    xs = [torch.empty_like(x) for _ in range(world_size)]\n",
    "    \n",
    "    dist.all_gather(xs, x)\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    print(f\"Rank {rank}: xs = {xs}\")\n",
    "    \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d312adf-4f38-4de7-af73-7737901fb03c",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "- `world_size = dist.get_world_size()`: This line retrieves the number of processes involved in the distributed computation, using the `get_world_size` function from the PyTorch `dist` module. This is necessary because the `all_gather` function will collect tensors from all processes, and we need to know how many tensors to expect.\n",
    "\n",
    "- `xs = [torch.empty_like(x) for _ in range(world_size)]`: This line creates a list xs of `world_size` empty tensors, each with the same shape and data type as the input tensor `x`. These tensors will be used to store the gathered data from all processes.\n",
    "\n",
    "- `dist.all_gather(xs, x)`: This line uses the `all_gather` function from the PyTorch dist module to collect data from all processes and store it in the `xs` list. Specifically, it collects the data in tensor `x` from each process and stores the resulting tensors in the corresponding positions in `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b4e8a-5c37-409b-8d50-1dba20c4b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = gather_tensors(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3073070-0a39-4b4b-805f-5fdbe8571a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.), tensor(1.)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af839937-bb43-46f4-9018-856980a13903",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844c534-b81f-49ff-9f8f-3c4e4b799c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 4\n",
    "input_size = 16\n",
    "output_size = 12\n",
    "input_data = torch.randn(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca09ca5-62fc-43ca-946b-aba6febba521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16, 12)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_size, input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5f406-ae8d-43d0-be64-5ac3101ed56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c88465-1642-4301-8168-2ddca3ba10eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f4c14-ff69-4773-b1e7-a1a9b3a22e9d",
   "metadata": {},
   "source": [
    "Write **the forward pass** of `ColumnLinearParallel` in Megatron-LM. Explain\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Focus on how to do parallel computing and ignore details like initialization.\n",
    "- Do not initialize a master weight (the weight of a non-parallel linear layer) and scatter the corresponding part to each process.\n",
    "- The final output will be sent to all processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94adfe2-5f42-4ebb-96c8-85e35dba9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_partitions):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size_per_partition = output_size // num_partitions\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(self.output_size_per_partition, self.input_size))\n",
    "        self.bias = nn.Parameter(torch.empty(self.output_size_per_partition))\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_patrition = F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        outputs = [torch.empty_like(output_patrition) for _ in range(world_size)]\n",
    "        dist.all_gather(outputs, output_patrition)\n",
    "        \n",
    "        output = torch.cat(outputs, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd84ec-88ce-4598-bb9d-e6d63560e9ba",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "- `self.output_size_per_partition = output_size // num_partitions:` This line calculates the output size for each partition by dividing the total output size by the number of partitions. This is done because the output dimension of the linear layer is divided among multiple processes, and each process will handle its corresponding portion of the output dimension.\n",
    "\n",
    "\n",
    "- `self.weight = nn.Parameter(torch.empty(self.output_size_per_partition, self.input_size))`: This line initializes the weight parameter for the current process. Since each process is responsible for its own portion of the output dimension.\n",
    "\n",
    "- `output_partition = F.linear(input, self.weight, self.bias)`: The output partition corresponding to the current process.\n",
    "\n",
    "- `outputs = [torch.empty_like(output_partition) for _ in range(world_size)]`: This line creates an `outputs` list with empty tensors that have the same shape as `output_partition`. These tensors will be used to store the `output` of each process.\n",
    "\n",
    "- `dist.all_gather(outputs, output_partition)`: The `dist.all_gather` function is called to gather the `output_partition` from all processes in the distributed group and store them in the `outputs` list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d84e2b-c88f-4bc2-ac20-820cb39ccede",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229285cd-9fb8-4fe8-b297-6adcf8b5d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2c4ed-e916-4d32-aa8a-126807cd18ba",
   "metadata": {},
   "source": [
    "Write the forward pass and backward pass of `ColumnParallelLinear` in Megatron-LM\n",
    "\n",
    "**Hint**: In the backward pass: `Y = [Y1, Y2]` > ... > `X1 + X2 = X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67338a3e-fb4c-434f-a514-36b9d7d21722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, weight, bias)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class f(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        torch.distributed.all_reduce(grad_output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2085579-9577-4004-aef6-5be27868dee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mg\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m      4\u001b[0m         world_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_world_size()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class g(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        input_list = [torch.empty_like(input) for _ in range(world_size)]\n",
    "        dist.all_gather(input_list, input)\n",
    "        inputs = torch.cat(input_list, dim=-1)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        rank = torch.distributed.get_rank()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        dim_size = grad_output.shape[-1]\n",
    "        chunk_size = dim_size // world_size\n",
    "        grad_chunks = torch.split(grad_output, chunk_size, dim=-1)\n",
    "        return grad_chunks[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442553b-10c6-4a2d-a3ca-6bd36a646811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, num_partitions: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.output_size_per_partition = output_size // num_partitions\n",
    "\n",
    "        self.weight = Parameter(torch.empty(\n",
    "            self.output_size_per_partition,\n",
    "            self.input_size,\n",
    "            requires_grad=True\n",
    "        ))\n",
    "        self.bias = Parameter(torch.empty(\n",
    "            self.output_size_per_partition,\n",
    "            requires_grad=True\n",
    "        ))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_parallel = f.apply(input)\n",
    "        output_parallel = F.linear(input_parallel, self.weight, self.bias)\n",
    "        outputs = g.apply(output_parallel)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66c583-f1e0-4296-8369-67922c8f75ca",
   "metadata": {},
   "source": [
    "### `RowParallelLinear`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed85f9-b69a-4ea7-8021-6acf57ccf81e",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2236dbf-5eb6-4a8c-8db3-800292beb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52839c31-d533-4bd4-9657-60b652080f5c",
   "metadata": {},
   "source": [
    "Write the forward pass of `RowParallelLinear` in Megatron-LM. Ignore the details like initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80a74e-b54d-4aed-b6de-0ea497aed444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        self.input_size_per_patrition = input_size // world_size\n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.output_size,\n",
    "            self.input_size_per_patrition\n",
    "        ))\n",
    "        self.bias = nn.Parameter(torch.empty(\n",
    "            self.output_size\n",
    "        ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        rank = torch.distributed.get_rank()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        \n",
    "        last_dim_size = input.shape[-1]\n",
    "        n_chunks = last_dim_size // world_size\n",
    "        input_chunks = torch.split(input, n_chunks, dim=-1)\n",
    "        \n",
    "        input_parallel = input_chunks[rank]\n",
    "        output_parallel = F.linear(input_parallel, self.weight, self.bias)\n",
    "        \n",
    "        torch.distributed.all_reduce(output_parallel)\n",
    "        return output_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356181d8-6cf2-43fe-8e85-dc4a5296c0d4",
   "metadata": {},
   "source": [
    "### `VocabParallelEmbedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca9dfe-aa3f-4f3a-a5ee-fa0d8d2c84f7",
   "metadata": {},
   "source": [
    "##### Example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccc500-b15d-410d-866f-4f25552f7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range(self, num_embeddings, rank, world_size):\n",
    "    num_embeddings_per_patrition = num_embeddings // world_size\n",
    "    start_idx = rank * num_embeddings_per_patrition\n",
    "    end_idx = start_idx + num_embeddings_per_patrition\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b6e82-9c33-4102-85be-12934b5243e2",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33877f45-3056-4531-8797-d2be01047b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c36785-b14f-478b-95dc-9110491328cb",
   "metadata": {},
   "source": [
    "Implement `VocabParallelEmbedding` from Megatron-LM. Explain your code\n",
    "\n",
    "**Hints**\n",
    "- Ignore details like weight initialization. Just focus on the parallelization\n",
    "- Broadcast the final embedding to all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8a97d-8a6a-4663-9d49-6364cd4c6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabParallelEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        rank = torch.distributed.get_rank()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        self.vocab_start_idx, self.vocab_end_idx = self.extract_range(\n",
    "            self.num_embeddings, rank, world_size\n",
    "        )\n",
    "        self.num_embedding_per_patrition = self.vocab_end_idx - self.vocab_start_idx\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            self.num_embedding_per_patrition,\n",
    "            self.embedding_dim\n",
    "        ))\n",
    "\n",
    "    def extract_range(self, num_embeddings, rank, world_size):\n",
    "        per_patrition_vocab_size = num_embeddings // world_size\n",
    "        start_idx = rank * per_patrition_vocab_size\n",
    "        end_idx = start_idx + per_patrition_vocab_size\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_mask = (input < self.vocab_start_idx) | (input >= self.vocab_end_idx)\n",
    "        masked_input = input.clone() - self.vocab_start_idx\n",
    "        masked_input[input_mask] = 0\n",
    "\n",
    "        output_parallel = F.embedding(masked_input, self.weight)\n",
    "        masked_idxs = torch.where(input_mask == True)[1]\n",
    "        output_parallel[:, masked_idxs, :] = 0.\n",
    "\n",
    "        torch.distributed.all_reduce(\n",
    "            output_parallel,\n",
    "            op=torch.distributed.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        return output_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35676586-afc9-40dd-9d89-0e2428803f10",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "- Each process is responsible for a partition of the vocab embedding. To determine the corresponding partition, we calculate its `vocab_start_idx` and `vocab_end_idx`. The number of embedding tokens for the current process is calculated as `num_embedding_per_partition`. We then initialize the weight matrix for the current process's partition.\n",
    "\n",
    "- During the forward pass, we create an `input_mask` to identify tokens that are not covered by the current process's partition. We then adjust the input tensor by subtracting the `vocab_start_idx` to shift the token values to the corresponding indices in the embedding of the current process. And we set all masked input elements to `0`. This is because each process is only responsible for a portion of the final embedding, and tokens outside the current process's partition should not contribute to its output.\n",
    "\n",
    "- Next, we compute the partial embeddings for the masked input using the partition's weight matrix. We also find the indices of the masked elements in the input tensor and set the corresponding elements in the output tensor to `0`.\n",
    "\n",
    "- Finally, we perform an all-reduce operation to sum up the partial embeddings across all processes. This results in a complete embedding tensor, as each process contributes its portion of the embeddings. The complete embedding tensor is then broadcast to all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57e392-7031-4504-9c33-cee57e73721f",
   "metadata": {},
   "source": [
    "### `ParallelMLP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fff96-8b84-4aac-b2da-554df8d16150",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346d876-8e84-4e66-8871-421a9472cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5417c3b-5965-4f5a-a946-2b9670ecc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColumnParallelLinear(input_size, output_size): pass\n",
    "\n",
    "def RowParallelLinear(input_size, output_size): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30fc1b-be41-4781-a3e5-6d19f8fecda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c23d1-78d1-44e5-8e95-f9f6ed32f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "RowParallelLinear(input_size=1024, output_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb8059-c4d1-470c-9a8b-24fb9ad4fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnParallelLinear(input_size=100, output_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200c888-f9ba-47ad-914d-7871ebdf3566",
   "metadata": {},
   "source": [
    "Write the `ParallelMLP` in Megatron-LM with GELU activation, where the middle hidden state is four times the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba944906-6a95-4d7a-9605-788bef051893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelMLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dense_h_to_4h = ColumnParallelLinear(\n",
    "            input_size=hidden_size,\n",
    "            output_size=hidden_size*4\n",
    "        )\n",
    "        self.dense_4h_to_h = RowParallelLinear(\n",
    "            input_size=hidden_size*4,\n",
    "            output_size=hidden_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        intermediate_parallel, bias_parallel = self.dense_h_to_4h(hidden_states)\n",
    "        intermediate_parallel = self.gelu(intermediate_parallel)\n",
    "        output, output_bias = self.dense_4h_to_h(intermediate_parallel)\n",
    "        return output, output_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0861f-1566-4fdc-80c2-c214a3734951",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_mlp = ParallelMLP(hidden_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf855f1c-bd66-4f4f-a4e5-1f0390b68861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ParallelMLP"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parallel_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378abf1a-9f7a-4338-bd9e-3b6feaa32987",
   "metadata": {},
   "source": [
    "### `VocabParallelCrossEntropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee506f89-1abc-4c6c-9284-ac690b80eb56",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8f340-4ebd-46d8-b552-06b966d13611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabParallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
