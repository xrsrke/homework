{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e744374e-57bd-4635-bc95-b85ffbc71d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ec12d-3edd-4e3e-8837-b62cbb51a176",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9bcdb7-2cb2-4559-9e5c-841b00816eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Fork(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        phony = torch.empty(0, requires_grad=True)\n",
    "        return input, phony\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_phony):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d35b4b-08e2-42a3-9f2c-d9f37028db9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Checkpointing:\n",
    "    def __init__(self, function, input):\n",
    "        self.function = function\n",
    "        self.input = input\n",
    "            \n",
    "    def checkpoint(self):\n",
    "        output = Checkpoint.apply(self.function, self.input)\n",
    "        return output\n",
    "    \n",
    "    def recompute(self, output):\n",
    "        output, phony = Fork.apply(output)\n",
    "        phony = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f538a9f-6deb-442b-bc8a-78d7dfd494f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Checkpoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        recomputed,\n",
    "        function, input,\n",
    "    ):\n",
    "        ctx.function, ctx.input = function, input\n",
    "        ctx.recomputed = recomputed\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = function(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, input_leafs = ctx.recomputed.pop()\n",
    "        torch.autograd.backward(output, grad_output)\n",
    "        \n",
    "        grad_input = [None, None]\n",
    "        grad_input.extend(None)\n",
    "        return tuple(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06dd581-57e9-4145-898d-459595a7a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recompute(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, phony, recomputed, function, input):\n",
    "        ctx.recomputed = recomputed\n",
    "        ctx.function = function\n",
    "        ctx.input = input\n",
    "        return phony\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.input\n",
    "        function = ctx.function\n",
    "        input_leaf = tuple(x.detach().requires_grad(x.requires_grad) for x in input)\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            output = function(input_leaf)\n",
    "        \n",
    "        ctx.recomputed.append((output, input_leaf))\n",
    "        grad_input = [None, None, None, None]\n",
    "        grad_input.extend(None)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6754ad-8c64-41c2-9196-1367862d7de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(69., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709eee7d-1394-4127-ac2e-6e817f33e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f83e45e-f096-4e77-8c00-b25c6e39e114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckp = Checkpointing(function=f, input=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edab06-774c-4860-8482-e8fdaaf9f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e785f7b-3d0a-4b3f-877b-c547652cbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a30cf2-033e-4fe2-9baf-9a701fc75a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckp.recompute(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010f8ee-8b6c-4f97-856b-90ecba371159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0766fd4-d140-44f2-895e-a697a7e272fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Log(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, name, x):\n",
    "        ctx.name = name\n",
    "        timeline.append(f\"{name}:forward\")\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        name = ctx.name\n",
    "        timeline.append(f\"{name}:backward\")\n",
    "        return None, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec29ebbc-d363-42cf-9089-5164ff534b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(69., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c384149-96ea-4257-a88d-0428c7a7eaec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeline = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "705307d2-04a6-4d32-81f7-58ab4d430acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = a + 69 + 6.9 + 6.969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cce6ebb-1fb0-4be2-875a-5110d982f54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7ff888f-7e93-411f-8375-1759f0e22ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(function, input):\n",
    "    chk = Checkpointing(function, input)\n",
    "    # compute the forward pass\n",
    "    output = chk.checkpoint()\n",
    "    chk.recompute(output)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0634c81-bd9e-44ca-949d-a5b3b8b51db4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, input)\u001b[0m\n\u001b[1;32m      2\u001b[0m chk \u001b[38;5;241m=\u001b[39m Checkpointing(function, \u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# compute the forward pass\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mchk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m chk\u001b[38;5;241m.\u001b[39mrecompute(output)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mCheckpointing.checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mCheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "a = checkpoint(partial(Log.apply, \"a\"), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de6c5329-0019-4b06-a9d8-55cef7492ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, b \u001b[38;5;241m=\u001b[39m create_dependency(\u001b[43ma\u001b[49m, b)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a, b = create_dependency(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f83b2f-c2ae-48c7-8d5d-dd549cc550ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
