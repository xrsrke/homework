{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0089601-81e4-4cbf-a3c8-6d94434fc09c",
   "metadata": {},
   "source": [
    "Pipeline parallelism is a parallelization strategy that distributes the execution of a neural network's forward and backward passes across multiple devices. It does this by dividing the network into smaller subnetworks or partitions and assigning each partition to a different device. This way, the workload is evenly distributed and allows for improved training efficiency.\n",
    "\n",
    "In pipeline parallelism, the forward and backward passes are decomposed into smaller tasks based on the micro-batches and partitions of the network. Let's break down how the forward and backward passes are decomposed.\n",
    "\n",
    "Forward Pass Decomposition:\n",
    "\n",
    "Divide the input batch into micro-batches, e.g., $x_1, \\cdots, x_m$.\n",
    "Sequentially execute the partitions $f^j$ on each micro-batch $x_i$. This results in tasks $F_{i, j}$, where $x_i^0 = x_i$ and $x_i^j = f^j(x_i^{j-1})$ for $i = 1, \\cdots, m$ and $j = 1, \\cdots, n$.\n",
    "Compute the output $f(x)$ by aggregating the results from each device, $x_i^n = f(x_i)$.\n",
    "Backward Pass Decomposition:\n",
    "\n",
    "Compute the gradient of the loss with respect to each output, $dx_i^n$.\n",
    "Sequentially execute the backward pass through the partitions $f^j$ on each gradient $dx_i^j$. This results in tasks $B_{i, j}$, where $dx_i^{j-1} = \\partial_x f^j(dx_i^j)$ and $g_i^j = \\partial_{\\theta^j} f^j(dx_i^j)$ for $i = 1, \\cdots, m$ and $j = 1, \\cdots, n$.\n",
    "Compute the gradient of the loss with respect to the network parameters, $g^j = \\sum_{i=1}^m g_i^j$.\n",
    "Pipeline parallelism takes advantage of the sequential nature of the tasks in the forward and backward passes. By assigning tasks with different micro-batch indices to different devices, the network can be trained efficiently using data parallelism. Note that there are data dependencies between tasks, so they must be executed in a specific order to ensure that the required data is available when needed.\n",
    "\n",
    "In summary, pipeline parallelism decomposes the forward and backward passes into smaller tasks based on micro-batches and partitions, assigning each task to a different device for parallel execution. This enables efficient training of large neural networks across multiple devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36f923-63a0-447a-a240-9ca1787d63a5",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32b4bd-ac5f-4051-87ee-b643e8b348ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clock_cycle(m, n):\n",
    "    for k in range(m+n-1):\n",
    "        yield [(k-j, j) for j in range(max(1+k-m, 0), min(1+k), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fabf88-c0d4-443b-a45f-9160461a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99dc06e-ecf9-49e6-b16c-91f6541c9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1)]\n",
      "[(2, 1), (1, 2)]\n",
      "[(3, 1), (2, 2), (1, 3)]\n",
      "[(4, 1), (3, 2), (2, 3)]\n",
      "[(4, 2), (3, 3)]\n",
      "[(4, 3)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(m+n-1):\n",
    "    print( [(k - j + 1 , j +1 ) for j in range(max(1 + k - m, 0), min(1 + k, n))] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d27d0c-2839-41b6-a596-b9b9852afb9e",
   "metadata": {},
   "source": [
    "##### Draft 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec4089-3f4a-4f87-b680-ff06a1199d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a4da1-481f-4eec-a70c-ea4162c106f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
