{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901b4c69-df55-4674-b58f-bba4083ee83b",
   "metadata": {},
   "source": [
    "##### Draft 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34d41-a92a-4d82-b9ac-dbf64a3776ad",
   "metadata": {},
   "source": [
    "The passage handles backward dependency by introducing virtual dependencies and using Fork and Join primitive functions. These dependencies ensure that the autograd engine is aware of the correct execution order during the backward pass. The Fork function maps a tensor $x$ to a pair $(x, \\varnothing)$, where $\\varnothing$ is an empty tensor, while the Join function maps a pair $(x, \\varnothing)$ back to the tensor $x$. By utilizing these functions, the dependency of $F_{i+1, j}$ upon $F_{i, j}$ can be expressed, which translates to the dependency of $B_{i, j}$ upon $B_{i+1, j}$ in the backward computation graph. This approach maintains the correct timeline of the backward pass and ensures proper parallelization during training.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550281b-6854-4dff-974d-1702ae06f084",
   "metadata": {},
   "source": [
    "Here's a simple concrete example:\n",
    "Suppose we have a neural network with 2 partitions ($f^1$ and $f^2$) and 2 micro-batches ($x_1$ and $x_2$). The forward pass tasks are represented as $F_{1,1}, F_{1,2}, F_{2,1}$, and $F_{2,2}$, while the backward pass tasks are represented as $B_{1,1}, B_{1,2}, B_{2,1}$, and $B_{2,2}$.\n",
    "\n",
    "During the forward pass, after completing $F_{1,1}$, we apply the Fork function, creating a checkpoint with the data and an empty tensor. When starting the next task, $F_{2,1}$, we use the Join function, which waits for both the data and the \"sign-off\" from $F_{1,1}$. Once it receives both, the Join function combines them and proceeds with $F_{2,1}$.\n",
    "\n",
    "This process ensures that, during the backward pass, the autograd engine knows that $B_{2,1}$ must be executed before $B_{1,1}$, maintaining the correct order and timeline of the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31497d9-591a-4682-bb88-6cefad9918e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd19d53b-61e0-46de-bd6d-26cb98279533",
   "metadata": {},
   "source": [
    "### Fork and Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c686d-d60b-4eef-abf9-4da2623717fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f04663-acad-4110-bcd2-cbf42aeaf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_phonies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b5ead-2b4e-418a-b7e7-13964a31fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phony(device: torch.device, requires_grad: bool) -> torch.Tensor:\n",
    "    key = (device, requires_grad)\n",
    "    \n",
    "    try:\n",
    "        phony = _phonies[key]\n",
    "    except KeyError:\n",
    "        # stream = torch.cuda.default_stream()\n",
    "        # with torch.cuda.stream(stream):\n",
    "        phony = torch.empty(0, device=device, requires_grad=requires_grad)\n",
    "        \n",
    "        _phonies[key] = phony\n",
    "    \n",
    "    return phony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b164b-2ea3-4cc4-9482-447361524193",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49646ad8-8e18-41f7-a697-32b6a304da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fork(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        phony = get_phony(input.device, requires_grad=False)\n",
    "        return input.detach(), phony.detach()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input, grad_grad):\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676daabd-b076-46f1-9af4-72b535b934d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Join(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, phony):\n",
    "        return input.detach()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_input):\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df7469-23c3-4ccf-8877-ddd926535b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fork(input):\n",
    "    if torch.is_grad_enabled() and input.requires_grad:\n",
    "        input, phony = Fork.apply(input)\n",
    "    else:\n",
    "        phony = get_phony(input.device, requires_grad=False)\n",
    "        \n",
    "    return input, phony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cc402-f5e7-4a68-a672-7accd96a6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(input, phony):\n",
    "    if torch.is_grad_enabled() and (input.requires_grad or phony.requires_grad):\n",
    "        input = Join.apply(input, phony)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cfcb5-5a20-44e7-96c2-603cf814a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depend(fork_from: \"Batch\", join_to: \"Batch\"):\n",
    "    fork_from, phony = fork(fork_from)\n",
    "    join_to = join(join_to, phony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d5167-5626-4ac7-8139-1fac1e64ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd169dc-70e1-4dbd-8e2d-2a61503eaa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(device(type='cpu'), False): tensor([])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_phonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395ea4-7c3a-4e3e-9b17-fe5c0407a557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f0275-fd95-49e1-80b7-69a3cf7cc552",
   "metadata": {},
   "source": [
    "$f^j$: The $j$th partition of the neural network.\n",
    "$\\theta^j$: The parameters associated with the $j$th partition.\n",
    "$x_i$: The $i$th micro-batch of training data.\n",
    "$F_{i,j}$: The forward pass task for the $i$th micro-batch on the $j$th partition.\n",
    "$B_{i,j}$: The backward pass task for the $i$th micro-batch on the $j$th partition.\n",
    "$x_i^j$: The intermediate output of the $i$th micro-batch after passing through the $j$th partition.\n",
    "$d x_i^j$: The gradient of the loss with respect to $x_i^j$.\n",
    "$g_i^j$: The gradient of the loss with respect to the $j$th partition's parameters ($\\theta^j$).\n",
    "$F_{i, j}^{\\prime}$: The recomputation of the forward pass task $F_{i, j}$ to save memory during the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e14450-9810-4248-9618-551e351426a4",
   "metadata": {},
   "source": [
    "Now, let's consider a simple example with 2 partitions ($f^1$ and $f^2$) and 2 micro-batches ($x_1$ and $x_2$). The forward pass tasks are represented as $F_{1,1}, F_{1,2}, F_{2,1}$, and $F_{2,2}$, while the backward pass tasks are represented as $B_{1,1}, B_{1,2}, B_{2,1}$, and $B_{2,2}$.\n",
    "\n",
    "The dependency graph in this example would have two floors (partitions) and two rooms on each floor (micro-batches). The elevators (data dependencies) connect the rooms as follows:\n",
    "\n",
    "$F_{1,1}$ must be completed before $F_{1,2}$ and $F_{2,1}$.\n",
    "$F_{1,2}$ must be completed before $F_{2,2}$.\n",
    "$F_{2,1}$ must be completed before $B_{2,1}$.\n",
    "$F_{2,2}$ must be completed before $B_{2,2}$.\n",
    "$B_{2,1}$ must be completed before $B_{1,1}$.\n",
    "$B_{2,2}$ must be completed before $B_{1,2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1e92b-5583-4c8f-aa13-75ec1a96b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = torch.tensor(1), torch.tensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1155c-a7f6-42c5-b1c6-e7c487389cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "phony1, phony2 = torch.randn(1), torch.randn(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1269844-c424-4c11-9306-63a52d071819",
   "metadata": {},
   "source": [
    "### Build Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5e404-7cd7-4f97-87d2-771ef159f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with spawn_workers(devices) as (in_queues, out_queues):\n",
    "    for schedule in clock_cycles(m, n):\n",
    "        self.fence(schedule, skip_trackers)\n",
    "        self.compute(schedule, skip_trackers, in_queues, out_queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72512c3e-86c4-4942-bafc-51d17950b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clock_cycles(n_microbatches: int, n_patritions: int):\n",
    "    \"\"\"Generates schedules for each clock cycle.\"\"\"\n",
    "    # n_microbatches: number of micro-batches\n",
    "    # n_patritions: number of partitions\n",
    "    # i: index of micro-batch\n",
    "    # j: index of partition\n",
    "    # k: clock number\n",
    "    #\n",
    "    # k (i,j) (i,j) (i,j)\n",
    "    # - ----- ----- -----\n",
    "    # 0 (0,0)\n",
    "    # 1 (1,0) (0,1)\n",
    "    # 2 (2,0) (1,1) (0,2)\n",
    "    # 3       (2,1) (1,2)\n",
    "    # 4             (2,2)\n",
    "    for k in range(m+n-1):\n",
    "        yield [(k-j, j) for j in range(max(1+k-m, 0), min(1+k, n))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8d9d0-16c3-4da0-aa92-f4c570515a83",
   "metadata": {},
   "source": [
    "Because at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c2644-620c-423d-a545-c8c164192ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(n_microbatches+n_patritions-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d48c5-b942-41e5-9959-42df8b528dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_microbatches = 10\n",
    "n_patritions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f9a82-3fee-4d94-9bcb-48b137df497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = list(clock_cycles(n_microbatches, n_patritions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d1dc2-4e2e-4e13-a622-1067bff65f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clock cycle 0: [(0, 0)]\n",
      "Clock cycle 1: [(1, 0), (0, 1)]\n",
      "Clock cycle 2: [(2, 0), (1, 1), (0, 2)]\n",
      "Clock cycle 3: [(3, 0), (2, 1), (1, 2), (0, 3)]\n",
      "Clock cycle 4: [(4, 0), (3, 1), (2, 2), (1, 3)]\n",
      "Clock cycle 5: [(5, 0), (4, 1), (3, 2), (2, 3)]\n",
      "Clock cycle 6: [(6, 0), (5, 1), (4, 2), (3, 3)]\n",
      "Clock cycle 7: [(7, 0), (6, 1), (5, 2), (4, 3)]\n",
      "Clock cycle 8: [(8, 0), (7, 1), (6, 2), (5, 3)]\n",
      "Clock cycle 9: [(9, 0), (8, 1), (7, 2), (6, 3)]\n",
      "Clock cycle 10: [(9, 1), (8, 2), (7, 3)]\n",
      "Clock cycle 11: [(9, 2), (8, 3)]\n",
      "Clock cycle 12: [(9, 3)]\n"
     ]
    }
   ],
   "source": [
    "for i, schedule in enumerate(schedules):\n",
    "    print(f\"Clock cycle {i}: {schedule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe83738-c66b-4f6c-978e-0c442336cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_microbatches+n_patritions-1):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f223a0-05c5-45c6-9aa7-274d879370ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83552b67-a6a0-4910-8db2-072bf4e8f401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
