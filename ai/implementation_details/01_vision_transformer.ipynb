{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345842c-c9a1-44b0-83a4-f29794b19ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6537c-9a72-47c4-bac4-393741eaffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e187f-dd18-41b7-b96c-c2b7e0c1b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_height, patch_width = 16, 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abbc2a-8cc4-406b-8359-190ec89dda39",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18090906-6307-4967-a25f-c075f232518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497cd4a3-0c96-45af-9dc4-24f17daee90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455fd46-abef-4c25-bee6-5fc89c95c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_height, patch_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c4940-9ca3-40a1-88ea-d1b399cf56f4",
   "metadata": {},
   "source": [
    "Turn a batch of images into patches, explain the code and the final shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d2466-a6de-4023-830d-88ad7cbebc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rearrange(\n",
    "    images,\n",
    "    'b c (n_height_patches patch_height) (n_width_patches patch_width)\\\n",
    "     -> b (n_height_patches n_width_patches) (patch_height patch_width c)',\n",
    "    patch_height=patch_height, patch_width=patch_width\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b156f6c-7f6c-4944-9641-ca9ec85870f5",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "- The batch of images has shape `bach_size, n_channels, height, width`.\n",
    "- `n_height_patches` and `n_width_patches` represent the number of non-overlapping patches along the `height` and `width` of the image, respectively.\n",
    "\n",
    "**The original shape**: `b c (n_height_patches patch_height) (n_width_patches patch_width)`\n",
    "- Why the height of the image is `(n_height_patches patch_height)`?\n",
    "    - Because we divide the height of each image into `n_height_patches` non-overlapping patches, so `height = n_height_patches * patch_height`.\n",
    "    - Final shape: `(height / patch_height) * (width / patch_width) = (224 / 16) * (224 / 16) = 196`\n",
    "- Vice versa for `(n_width_patches patch_width)`\n",
    "\n",
    "**The final shape**: `b (n_height_patches n_width_patches) (patch_height patch_width c)`\n",
    "- `(n_height_patches n_width_patches)`\n",
    "    - This represents the total number of patches per image, obtained by multiplying the number of height patches by the number of width patches.\n",
    "    - We want to concatenate all the patches from the image into a single sequence so that the Vision Transformer can process them.\n",
    "\n",
    "- `(patch_height patch_width c)`\n",
    "    - This part flattens each patch, including all the color channels. \n",
    "    - The purpose of this is to convert the patch into a single flat vector.\n",
    "    - Final shape: `patch_height * patch_width * n_channels = 16 * 16 * 3 = 768`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd09559-27e6-4f0d-8a16-398511630c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 196, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19cfb7-6180-4c42-80fe-1a17a1ed4662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be052ae-e534-42d5-9907-564978f5daf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
