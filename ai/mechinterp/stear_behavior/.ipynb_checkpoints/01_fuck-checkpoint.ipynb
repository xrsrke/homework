{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22c87a9d-8344-48ab-8a83-231840298c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_kwargs = {'temperature': 1.0, 'top_p': 0.3, 'freq_penalty': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1ea196e-d2ec-42cb-87a9-d029170262c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 1.0, 'top_p': 0.3, 'freq_penalty': 1.0}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9948b-b057-4567-b4dc-2eb01b0e1c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e551cf4-54d5-468b-a74f-45bff5d0c5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a46de-c091-4b26-bb3c-08e475250f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6cf9d1-1ff7-4851-bb1d-8e35fd2f2968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d471e3-0684-405e-be79-325c5321ff76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0a87f80-a2fa-458d-8813-7c357a51b661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I hate you because \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e81f494c-3635-468a-b6de-d04adb9b9c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steer_positive_prompt = \"Bread\"\n",
    "steer_negative_prompt = \"  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16417836-12ac-4a8f-bac0-09f088a7abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompt)\n",
    "steer_positive_tokens = model.to_tokens(steer_positive_prompt)\n",
    "steer_negative_tokens = model.to_tokens(steer_negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f157cde-4729-4f37-935c-22555fc53a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,    33,   961]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_positive_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6a9259a-e007-4cd7-9ae6-8a4bcc47d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,   220,   220]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_negative_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b97094-1f31-4e02-ab93-f41f399690c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hook_name = \"blocks.6.hook_resid_pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "515c242a-ebf5-4863-9c4b-c9d8fc669fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, positive_activations = model.run_with_cache(steer_positive_tokens)\n",
    "_, negative_activations = model.run_with_cache(steer_negative_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39759c1d-3c94-498b-a59f-f196e9099dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_positive_activations = positive_activations[hook_name] * 5\n",
    "target_negative_activations = negative_activations[hook_name] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4f6364c-44aa-4639-8dd3-b457466b630d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_positive_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f582991f-97ea-4992-ac38-3ef9d2c1d8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_negative_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0961bd4-fe66-431d-a35b-2ab20d1d69f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activations_seq_len = target_positive_activations.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e98505a-d55b-40f3-967d-b97d28d94124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_slice = slice(0, activations_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6388bdd7-b6da-4cf1-a6a3-69b4692d8c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_stream_slice = slice(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c216c02-4d07-4d05-9a67-ceb287dd71ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_positive_tokens = steer_positive_tokens.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "161c33cb-74b5-48e3-b995-6cbd4f5eb456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95285813-a2e5-4f5d-86b0-0a118a63135d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_resid_pre_activations(activations, hook):\n",
    "    # set_trace()\n",
    "    activations[:, :n_positive_tokens, :] += target_positive_activations * 3\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f3b106e-1204-4570-a32a-767f26c9093c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0faa1727-e48f-4eed-a314-f671a86bcdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add_hook(hook_name, add_resid_pre_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab24a043-9183-40e0-949e-ece0a7c5b034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a187fef601c84981b15156c7d113201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 1, 768] doesn't match the broadcast shape [1, 3, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/transformer_lens/HookedTransformer.py:1474\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, num_return_sequences, use_past_kv_cache, prepend_bos, return_type, verbose)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_past_kv_cache:\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;66;03m# We just take the final tokens, as a [batch, 1] tensor\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1474\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   1481\u001b[0m             tokens, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m, past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache\n\u001b[1;32m   1482\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_resid_pre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_pre\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[38;5;241m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m, in \u001b[0;36madd_resid_pre_activations\u001b[0;34m(activations, hook)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_resid_pre_activations\u001b[39m(activations, hook):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# set_trace()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     activations[:, :n_positive_tokens, :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target_positive_activations \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 1, 768] doesn't match the broadcast shape [1, 3, 768]"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.generate(tokens, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd53c-c8e1-4787-80be-ffc99f3b1164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
