{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1899af-7594-4503-9ab3-b71245534b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb988a78850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba5572b-daa9-47f0-9370-15a9fbd1e083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41359b1-2980-4c0d-9f99-a2fc67bc72b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55b4b0-646b-4a33-ad55-e7de2bbab992",
   "metadata": {},
   "source": [
    "##### Example 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73fd59ce-6e03-489e-9037-40f32f680740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Persistence is all you need.\"\n",
    "model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "605014b5-29df-4117-933f-830602c0132b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4570b74-aada-4e19-a165-0adab4031acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2Model"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10d0fdc2-cd5d-4d77-abdf-841670a852b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30946, 13274,   318,   477,   345,   761,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa8c4c-a701-45f3-b984-18725134cec0",
   "metadata": {},
   "source": [
    "Print the shape of the input of the 2nd block in model given `prompt`\n",
    "\n",
    "**Hint**: `input[0].shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5d78b1f-fa63-4dab-88f8-bdd29fb1244f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_the_shape(module, input):\n",
    "    print(input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28777247-ce0e-40f8-937f-ae8d9937e195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fb989c20580>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[1].register_forward_pre_hook(extract_the_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63925496-78fb-4b36-9ab4-7822d0eaf9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "_ = model(**input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbd518-f8cc-40ac-95e6-71631f1fbe52",
   "metadata": {},
   "source": [
    "##### Example 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441d18e2-acb9-4a4e-868e-7731fc03c699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Persistence is all you need.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb686ce5-0bbd-44f9-9225-d6cbdb5dadfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b262ebb-bbf6-4a9b-88c8-2210f0d5ef51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hook1(module, input):\n",
    "    print('Hook 1')\n",
    "\n",
    "def hook2(module, input):\n",
    "    print('Hook 2')\n",
    "\n",
    "def hook3(module, input):\n",
    "    print('Hook 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac82042b-9f3e-4bba-b788-9950d1bf6611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.hook1(module, input)>,\n",
       " <function __main__.hook2(module, input)>,\n",
       " <function __main__.hook3(module, input)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook1, hook2, hook3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cfe0841-3650-4f7c-baf8-11e39b344cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hooks = [hook1, hook2, hook3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a903929-c4bd-4c3d-b9ec-82fa8042939c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30946, 13274,   318,   477,   345,   761,    13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75ce9ce-9273-471b-95a0-06bcf0497da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = input_ids[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a791de-d9e3-4ea0-989d-9051b0210efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.hook1(module, input)>,\n",
       " <function __main__.hook2(module, input)>,\n",
       " <function __main__.hook3(module, input)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a28ae7-c4ee-46f7-ab01-594ac4e305ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook 1\n",
      "Hook 2\n",
      "Hook 3\n"
     ]
    }
   ],
   "source": [
    "_ = [hook(None, None) for hook in hooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae393324-d807-4f38-93ad-cd2f5a2cb2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2Model"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966a9768-accd-4fb6-a5dd-b3bf754e9be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30946, 13274,   318,   477,   345,   761,    13]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f45d4-9208-4546-800f-9d163fe465c6",
   "metadata": {},
   "source": [
    "Before the forward pass, add `hook_1`, `hook_2`, and `hook_3` to the layer norm of `model`. Afterward, remove the second hook\n",
    "\n",
    "**Hint**: `model.ln_f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ac5e12-44d1-446f-97df-853a40e5091f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "933eeb10-f43b-4754-b538-c55638b82578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for hook_func in hooks:\n",
    "    handle = model.ln_f.register_forward_pre_hook(hook_func)\n",
    "    handles.append(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e99292-b832-440e-b7ef-cc4ca319e16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handles[1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c237758-6492-400a-86b1-180e95bab701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook 1\n",
      "Hook 3\n"
     ]
    }
   ],
   "source": [
    "_ = model(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5683a5-6f86-4aaf-be4f-873d223e6ae3",
   "metadata": {},
   "source": [
    "##### Example 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb478f99-e519-4f6b-b6e6-87a5d7a9f582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625d40ae-b1d8-4f12-be19-49e170c1d033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Persistence is all you need.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae75750-b995-49b2-9861-1713fc5a3bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ae3c6e-f14c-4bb5-af90-593bf45fb353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f466ad4b-424b-4b1f-865a-99716c09f309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = input_ids[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feea8446-97f3-4307-80e1-4eecfe59758e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hook1(module, input):\n",
    "    print('Hook 1')\n",
    "\n",
    "def hook2(module, input):\n",
    "    print('Hook 2')\n",
    "\n",
    "def hook3(module, input):\n",
    "    print('Hook 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b7fb30-f0bc-46e0-8049-24ab4529624a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.hook1(module, input)>,\n",
       " <function __main__.hook2(module, input)>,\n",
       " <function __main__.hook3(module, input)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook1, hook2, hook3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8351451e-7002-408d-9452-3f963b52a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hooks = [hook1, hook2, hook3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca834822-5398-436b-a602-9182598db522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30946, 13274,   318,   477,   345,   761,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f8c61a-58b0-431d-a140-2c2801a049cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b15ef2f-1d18-4cb3-9ec0-ab4619917712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.hook1(module, input)>,\n",
       " <function __main__.hook2(module, input)>,\n",
       " <function __main__.hook3(module, input)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9f805-2cdf-4506-b3e6-12dae49ae72e",
   "metadata": {},
   "source": [
    "First, add all the hooks in `hooks` to the second block in the `model`. Then, generate text based on `input_ids` with `max_new_tokens=1`. Finally, automatically remove all the hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3ddc91-8dc4-4ff3-b18f-971a16854708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e517a63d-e882-4c75-aa26-b91e7eec3bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def use_hooks(model, hooks):\n",
    "    try:\n",
    "        last_block = model.transformer.h[1]\n",
    "        handles = [last_block.register_forward_pre_hook(hook) for hook in hooks]\n",
    "        yield handles\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2e85ac-5ffa-4d72-b6bb-504d7d033c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook 1\n",
      "Hook 2\n",
      "Hook 3\n"
     ]
    }
   ],
   "source": [
    "with use_hooks(model, hooks):\n",
    "    generated_tokens = model.generate(**input_ids, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b12796-9993-416f-98ec-dc238e909c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[1]._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbe9ec-2e88-4730-9115-a280a77f1606",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277b6e1f-0f46-4410-9307-1be7e7109cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25189a9-ec49-480c-ab00-30217cde809b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steer_positive_prompt = \"Love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e614dd73-5521-4673-a702-b32cdf521c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steer_negative_prompt = \"Hate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba733c43-d769-4bf9-ba8c-fb6b59cb843f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [steer_positive_prompt, steer_negative_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cd8be-bff7-4df5-85e3-c9d835e269b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [tokenizer.bos_token + p for p in prompts]\n",
    "tokenized_prompts = tokenizer(prompts, return_tensors='pt', padding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
